{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4dd4555-473d-48cd-bc86-60a4c41da0a8",
   "metadata": {},
   "source": [
    "## load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "611f29b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b2c708e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yc/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "with open(\"hf.token\", \"r\") as f:\n",
    "    hftoken = f.read().strip()  \n",
    "\n",
    "import os\n",
    "cache_dir = \"/mnt/c/Users/yc/.cache/huggingface\"\n",
    "os.environ['HF_HOME'] = cache_dir\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(token=hftoken)  # Move token to environment variable\n",
    "\n",
    "from ult import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d74a59c-b625-4944-8a79-aea69bc6cd8a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "device_map = {\"\": 0}\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=device_map,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e1c388-57f9-41b0-87d1-25a9ad884430",
   "metadata": {},
   "source": [
    "## data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6281f4-7ebb-48fd-a08b-4271349ededf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coral_idx</th>\n",
       "      <th>Sex</th>\n",
       "      <th>UCSFDerivedRaceEthnicity_X</th>\n",
       "      <th>BirthDate</th>\n",
       "      <th>note_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>184</td>\n",
       "      <td>Female</td>\n",
       "      <td>Latinx</td>\n",
       "      <td>1983-10-04</td>\n",
       "      <td>Medical Oncology Consult Note  Video Consult  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>185</td>\n",
       "      <td>Female</td>\n",
       "      <td>Multi-Race/Ethnicity</td>\n",
       "      <td>1973-03-08</td>\n",
       "      <td>This is a shared service.  Physician Statement...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>193</td>\n",
       "      <td>Female</td>\n",
       "      <td>Multi-Race/Ethnicity</td>\n",
       "      <td>1979-06-20</td>\n",
       "      <td>ID: ***** ***** is a 39 y.o. premenopausal pat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>210</td>\n",
       "      <td>Female</td>\n",
       "      <td>Southwest Asian and North African</td>\n",
       "      <td>1974-04-05</td>\n",
       "      <td>Patient Name: ***** *****  ***** *****: 08/22/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>223</td>\n",
       "      <td>Female</td>\n",
       "      <td>Unknown/Declined</td>\n",
       "      <td>1960-10-12</td>\n",
       "      <td>We performed this consultation using real-time...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    coral_idx     Sex         UCSFDerivedRaceEthnicity_X   BirthDate  \\\n",
       "44        184  Female                             Latinx  1983-10-04   \n",
       "45        185  Female               Multi-Race/Ethnicity  1973-03-08   \n",
       "53        193  Female               Multi-Race/Ethnicity  1979-06-20   \n",
       "70        210  Female  Southwest Asian and North African  1974-04-05   \n",
       "83        223  Female                   Unknown/Declined  1960-10-12   \n",
       "\n",
       "                                            note_text  \n",
       "44  Medical Oncology Consult Note  Video Consult  ...  \n",
       "45  This is a shared service.  Physician Statement...  \n",
       "53  ID: ***** ***** is a 39 y.o. premenopausal pat...  \n",
       "70  Patient Name: ***** *****  ***** *****: 08/22/...  \n",
       "83  We performed this consultation using real-time...  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_file_path = 'data/formaldef.txt'\n",
    "formaldic = txt_to_dict(txt_file_path)\n",
    "len(formaldic)\n",
    "\n",
    "meddict={}\n",
    "for k,v in formaldic.items():\n",
    "    meddict[k.split('Listen to pronunciation')[0].split('(')[0]]=v\n",
    "filename= 'data/filtered_medical_dictionary.csv'\n",
    "eighth_grade_words=set()\n",
    "with open(filename, 'r', encoding='utf-8') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    next(reader)  # Skip the header row\n",
    "    for row in reader:\n",
    "        if row:  # Make sure row is not empty\n",
    "            eighth_grade_words.add(row[0])  # Add the word (first column)\n",
    "filtered_meddict = {word: explanation for word, explanation in meddict.items() \n",
    "                   if word in eighth_grade_words}\n",
    "meddict=filtered_meddict\n",
    "# load data\n",
    "df = pd.read_csv('data/CORAL/coral-expert-curated-medical-oncology-reports-to-advance-language-model-inference-1.0/coral/unannotated/data/breastca_unannotated.csv')\n",
    "# df = df.sample(1, random_state=42)\n",
    "# df=df.iloc[[44,45,53,70,83, 0,1,2,3,4]] \n",
    "# df=df.iloc[[44,45,53,70,83]] \n",
    "test_note=df.iloc[0]['note_text']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfa3a9d",
   "metadata": {},
   "source": [
    "# keypoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c3bce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction_prompts = {\n",
    "\n",
    "\"Medication_Plan\": \"\"\"\n",
    "TASK: Extract the 'Medication Plan'. Find the 'Assessment/Plan' section of the note, usually the final section.\n",
    "Include all current and future medication plans for both cancer therapy and supportive treatment. Cancer treatment could be one or many in chemotherapy, hormone therapy, bone therapy, radiotherapy (eg. rad onc, xrt). Supportive treatment could be one or many in bowel regimen, pain medication, psychiatry medication, neuropathy or any blood transfusion plan. \n",
    "\n",
    "Include whether a medication is being started now (e.g,“will start”, “Rx sent”, “starting today” ), plan or discuss in the future after certain condition (e.g., “plan to start after radiation”, “discussed addition of…”), continue or maintained(“continue”), stop or change.\n",
    "\n",
    "Do NOT include procedures, labs, imaging, or genetic testing.\n",
    "For your information, a completed, finished, status post medication, s/p, means past treatments.\n",
    "Include an alterative, second-line or clinical trials options if dicussed.\n",
    "\n",
    "Respond *only* with a JSON object using this exact schema:\n",
    "{\n",
    "    \"The medication/treatment, one of chemotherapy, hormone therapy, bone therapy, radiotherapy\": \n",
    "        {\n",
    "        \"summary\":\"the summary of this type of medication, including start/stop/cotinue if applicable\",\n",
    "        \"Short term side_effects_discussed\": \"short term Side effects of this particular medications.\",\n",
    "        \"Long term side_effects_discussed\": \"long term Side effects of this particular medications.\"\n",
    "        },\n",
    "}\n",
    "\"\"\",\n",
    "\n",
    "\"Medication_Plan old\": \"\"\"\n",
    "TASK: Extract the 'Medication Plan'. Find the 'Assessment/Plan' section of the note, usually the final section.\n",
    "Include future medication plans, changes to current meds (start, stop, continue), supportive meds, bowel regimen, and any blood transfusion plan.\n",
    "Respond *only* with a JSON object using this exact schema:\n",
    "{\n",
    "    \"medication_plan\": \"A summary of the complete medication plan.\"\n",
    "}\n",
    "\"\"\",\n",
    "\n",
    "\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fe5d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "res=[]\n",
    "\n",
    "for row in df.itertuples():\n",
    "    test_note=row.note_text\n",
    "    gen_config = {\n",
    "    \"max_new_tokens\": 512,  # <-- Increased from 150 to 512\n",
    "    \"eos_token_id\": tokenizer.eos_token_id,\n",
    "    \"do_sample\": False\n",
    "    }\n",
    "\n",
    "    # myprint(\"\\n--- 1. Creating Base KV Cache from long text... ---\")\n",
    "\n",
    "    base_prompt = (\n",
    "        f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n\"\n",
    "        f\"You are a medical data extraction expert. You will be given a long medical note. \"\n",
    "        f\"Your task is to answer a series of questions about it, one by one. \"\n",
    "        # FIX 2: Stricter instruction in system prompt\n",
    "        f\"Respond *only* with the valid JSON object requested. Do not add markdown backticks or any other text.\"\n",
    "        f\"<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "        f\"Here is the medical note:\\n\\n\"\n",
    "        f\"--- BEGIN NOTE ---\\n{test_note}\\n--- END NOTE ---\"\n",
    "        f\"\\n\\nI will now ask you to extract specific sections. \"\n",
    "        f\"Please wait for my first extraction task.\"\n",
    "        f\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "        f\"{{\\\"status\\\": \\\"Understood. I have read the note and am ready.\\\"}}\"\n",
    "    )\n",
    "\n",
    "    # myprint(\"  Tokenizing and running forward pass to get base cache...\")\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(base_prompt, return_tensors=\"pt\").to(model.device)\n",
    "        \n",
    "        # This logic to create the base_cache is correct\n",
    "        outputs = model(\n",
    "            input_ids=inputs[\"input_ids\"], \n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            use_cache=True\n",
    "        )\n",
    "        \n",
    "        base_cache = outputs.past_key_values\n",
    "        \n",
    "        # Clean up\n",
    "        del inputs, outputs\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    # myprint(\"Base KV Cache created. Ready for 'branching' extractions.\")\n",
    "\n",
    "    # --- FIX 3: Define STRICT SCHEMAS for each extraction task ---\n",
    "    # This is the most important \"prompt engineering\" fix.\n",
    "    # We are telling the model *exactly* what keys to use.\n",
    "\n",
    "\n",
    "    extracted_data = {}\n",
    "\n",
    "    # myprint(\"\\n--- 2. Running EFFICIENT 'Branching' Extractions ---\")\n",
    "\n",
    "    # (Assuming `run_model_with_cache_manual` is your function `run_model_with_cache`)\n",
    "    # If not, please rename this call to `run_model_with_cache`\n",
    "    run_model_function = run_model_with_cache_manual \n",
    "\n",
    "    for key, task in extraction_prompts.items():\n",
    "        # myprint(f\"\\nExtracting: {key}...\")\n",
    "        \n",
    "        # --- FIX 4: Remove the BOS token from the loop ---\n",
    "        # The base_cache *already* contains the BOS token.\n",
    "        # Adding it again can cause errors.\n",
    "        task_prompt = (\n",
    "            f\"<|start_header_id|>user<|end_header_id|>\\n\\n\" # <-- Removed <|begin_of_text|>\n",
    "            f\"{task}\" # <-- Use the new, detailed prompt\n",
    "            f\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "        )\n",
    "\n",
    "        # Use the SAME base_cache for each extraction\n",
    "        answer, returned_cache = run_model_function(\n",
    "            task_prompt, \n",
    "            model, \n",
    "            tokenizer, \n",
    "            gen_config, \n",
    "            kv_cache=base_cache\n",
    "        )\n",
    "        \n",
    "        # Your memory management here is good.\n",
    "        del returned_cache\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        # myprint(f\"  Raw Output: {answer}\")\n",
    "        \n",
    "        try:\n",
    "            # We are now *only* expecting a JSON object\n",
    "            clean_answer = answer.strip().strip(\"```json\").strip(\"```\").strip()\n",
    "            extracted_data[key] = json.loads(clean_answer)\n",
    "        except json.JSONDecodeError:\n",
    "            extracted_data[key] = {\"error\": \"Failed to parse JSON\", \"raw\": answer}\n",
    "\n",
    "    # myprint(\"\\n--- 3. All extractions complete. ---\")\n",
    "    # myprint(\"\\n--- FINAL EXTRACTED DATA ---\")\n",
    "    res.append(json.dumps(extracted_data, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c87f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "resdf=[json.loads(r) for r in res]\n",
    "for i in range(1):\n",
    "    test_note=df.iloc[i]['note_text']\n",
    "    resdf[i]['note']=test_note\n",
    "    resdf[i]['coral_idx']=df.iloc[i].coral_idx\n",
    "resdf=pd.DataFrame(resdf)\n",
    "resdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
