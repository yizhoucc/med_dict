{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "503c81c2-012a-4e15-a255-a4a3e4c576d7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install sentencepiece protobuf datasets transformers trl textstat peft bitsandbytes nltk --quiet\n",
    "!pip install -U bitsandbytes accelerate --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b2c708e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"hf.token\", \"r\") as f:\n",
    "    hftoken = f.read().strip()  \n",
    "\n",
    "import os\n",
    "cache_dir = \"/mnt/c/Users/yc/.cache/huggingface\"\n",
    "os.environ['HF_HOME'] = cache_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43c967d-ea35-4d4e-8ddc-f39328032b1a",
   "metadata": {},
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e3b7919-277b-47b9-95b9-eab518373862",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yc/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import csv\n",
    "import re\n",
    "import torch\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, words\n",
    "from collections import Counter\n",
    "import textwrap\n",
    "\n",
    "\n",
    "LINE_WIDTH = 140\n",
    "# Third-party data and ML libraries\n",
    "import pandas as pd\n",
    "# Hugging Face ecosystem\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "\n",
    "# Optional: Uncomment if needed\n",
    "from huggingface_hub import login\n",
    "login(token=hftoken)  # Move token to environment variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dd4555-473d-48cd-bc86-60a4c41da0a8",
   "metadata": {},
   "source": [
    "## load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d74a59c-b625-4944-8a79-aea69bc6cd8a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:32<00:00,  8.23s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "device_map = {\"\": 0}\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name,\n",
    "                                        #   cache_dir=cache_dir\n",
    "                                          )\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=device_map,\n",
    "    load_in_8bit=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e1c388-57f9-41b0-87d1-25a9ad884430",
   "metadata": {},
   "source": [
    "## data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf6281f4-7ebb-48fd-a08b-4271349ededf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def txt_to_dict(file_path):\n",
    "    data_dict = {}\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        for i in range(0, len(lines) - 1, 2):\n",
    "            key = lines[i].strip()    # Odd line are key\n",
    "            value = lines[i + 1].strip()  # Even line are value\n",
    "            data_dict[key] = value\n",
    "\n",
    "    return data_dict\n",
    "\n",
    "txt_file_path = 'data/formaldef.txt'\n",
    "formaldic = txt_to_dict(txt_file_path)\n",
    "len(formaldic)\n",
    "\n",
    "meddict={}\n",
    "for k,v in formaldic.items():\n",
    "    meddict[k.split('Listen to pronunciation')[0].split('(')[0]]=v\n",
    "filename= 'data/filtered_medical_dictionary.csv'\n",
    "eighth_grade_words=set()\n",
    "with open(filename, 'r', encoding='utf-8') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    next(reader)  # Skip the header row\n",
    "    for row in reader:\n",
    "        if row:  # Make sure row is not empty\n",
    "            eighth_grade_words.add(row[0])  # Add the word (first column)\n",
    "filtered_meddict = {word: explanation for word, explanation in meddict.items() \n",
    "                   if word in eighth_grade_words}\n",
    "meddict=filtered_meddict\n",
    "# load data\n",
    "df = pd.read_csv('/mnt/c/Users/yc/Downloads/coral/unannotated/data/breastca_unannotated.csv')\n",
    "df = df.sample(2, random_state=42) \n",
    "text=df.iloc[0]['note_text']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8239be",
   "metadata": {},
   "source": [
    "audience--> audiense str\n",
    "key details, summary --> a summary\n",
    "extraction terms --> a dict?\n",
    "main prompt, takes in 123, out a str\n",
    "check with key details and main Gen, takes \n",
    "final clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a445b4-0a2b-45d1-9892-42f775ef20b8",
   "metadata": {},
   "source": [
    "# prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "516413a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_model(prompt: str, model, tokenizer, generation_config: Dict) -> str:\n",
    "\n",
    "    try:\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(model.device)\n",
    "        input_length = inputs[\"input_ids\"].shape[1]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                **generation_config\n",
    "            )\n",
    "        \n",
    "        response_tokens = outputs[0][input_length:]\n",
    "        # raw_output = tokenizer.decode(response_tokens, skip_special_tokens=False)\n",
    "        # return raw_output\n",
    "        raw_output = tokenizer.decode(response_tokens, skip_special_tokens=True)\n",
    "        return raw_output.strip()\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error generating response: {str(e)}\"\n",
    "\n",
    "def extract_medical_terms(text: str, meddict: Dict[str, str]) -> Dict[str, str]:\n",
    "\n",
    "    found_terms = {}\n",
    "    \n",
    "    # Strategy 1: Single words\n",
    "    words = re.findall(r'\\b[A-Za-z]+(?:[-\\'][A-Za-z]+)*\\b', text)\n",
    "    for word in words:\n",
    "        definition = find_term_in_dict(word, meddict)\n",
    "        if definition:\n",
    "            found_terms[word] = definition\n",
    "    \n",
    "    # Strategy 2: Multi-word terms\n",
    "    for n in range(2, 6):\n",
    "        n_grams = get_n_grams(text, n)\n",
    "        for phrase in n_grams:\n",
    "            definition = find_term_in_dict(phrase, meddict)\n",
    "            if definition:\n",
    "                found_terms[phrase] = definition\n",
    "    \n",
    "    # Strategy 3: Medical abbreviations\n",
    "    abbreviations = re.findall(r'\\b[A-Z]{2,8}\\b', text)\n",
    "    for abbrev in abbreviations:\n",
    "        definition = find_term_in_dict(abbrev, meddict)\n",
    "        if definition:\n",
    "            found_terms[abbrev] = definition\n",
    "    \n",
    "    # Strategy 4: Medical procedures and conditions with specific patterns\n",
    "    medical_patterns = [\n",
    "        r'\\b\\w+oscopy\\b',          # bronchoscopy, endoscopy, etc.\n",
    "        r'\\b\\w+ectomy\\b',          # appendectomy, etc.\n",
    "        r'\\b\\w+itis\\b',            # bronchitis, arthritis, etc.\n",
    "        r'\\b\\w+osis\\b',            # fibrosis, stenosis, etc.\n",
    "        r'\\b\\w+emia\\b',            # anemia, septicemia, etc.\n",
    "        r'\\b\\w+pathy\\b',           # myopathy, neuropathy, etc.\n",
    "        r'\\b\\w+malacia\\b',         # tracheomalacia, etc.\n",
    "    ]\n",
    "    \n",
    "    for pattern in medical_patterns:\n",
    "        matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "        for match in matches:\n",
    "            definition = find_term_in_dict(match, meddict)\n",
    "            if definition:\n",
    "                found_terms[match] = definition\n",
    "    \n",
    "    # Strategy 5: Medication names\n",
    "    medication_patterns = [\n",
    "        r'\\b\\w+cillin\\b',          # penicillin, amoxicillin, etc.\n",
    "        r'\\b\\w+mycin\\b',           # streptomycin, etc.\n",
    "        r'\\b\\w+floxacin\\b',        # levofloxacin, ciprofloxacin, etc.\n",
    "        r'\\b\\w+sone\\b',            # prednisone, cortisone, etc.\n",
    "        r'\\b\\w+pam\\b',             # lorazepam, etc.\n",
    "    ]\n",
    "    \n",
    "    for pattern in medication_patterns:\n",
    "        matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "        for match in matches:\n",
    "            definition = find_term_in_dict(match, meddict)\n",
    "            if definition:\n",
    "                found_terms[match] = definition\n",
    "    \n",
    "    return found_terms\n",
    "\n",
    "\n",
    "def get_n_grams(text: str, n: int) -> List[str]:\n",
    "    \"\"\"Generate n-grams from text.\"\"\"\n",
    "    words = re.findall(r'\\b[A-Za-z]+\\b', text.lower())\n",
    "    n_grams = []\n",
    "    for i in range(len(words) - n + 1):\n",
    "        phrase = ' '.join(words[i:i+n])\n",
    "        n_grams.append(phrase)\n",
    "    return n_grams\n",
    "\n",
    "\n",
    "def find_term_in_dict(term: str, meddict: Dict[str, str]) -> Optional[str]:\n",
    "    \"\"\"Find term in medical dictionary.\"\"\"\n",
    "    search_formats = [\n",
    "        term, term.lower(), term.upper(), term.title(), term.capitalize()\n",
    "    ]\n",
    "    \n",
    "    for search_term in search_formats:\n",
    "        if search_term in meddict:\n",
    "            return meddict[search_term]\n",
    "    \n",
    "    # Partial matching\n",
    "    for key in meddict.keys():\n",
    "        if key.lower() == term.lower():\n",
    "            return meddict[key]\n",
    "    \n",
    "    return None\n",
    "\n",
    "def create_annotated_text(text: str, meddict: Dict[str, str]) -> str:\n",
    "\n",
    "    # 1. Use the existing function to find all unique terms and their definitions.\n",
    "    found_terms = extract_medical_terms(text, meddict)\n",
    "    \n",
    "    # 2. Sort terms by length in descending order to handle overlaps.\n",
    "    # This is critical for terms like \"cerebral palsy\" and \"palsy\".\n",
    "    sorted_terms = sorted(found_terms.keys(), key=len, reverse=True)\n",
    "    \n",
    "    annotated_text = text\n",
    "    \n",
    "    # 3. Iterate and replace.\n",
    "    for term in sorted_terms:\n",
    "        definition = found_terms[term]\n",
    "        annotation = f\"{term} [DEFINITION: {definition}]\"\n",
    "        pattern = r'\\b' + re.escape(term) + r'\\b'\n",
    "        annotated_text = re.sub(pattern, annotation, annotated_text, count=1, flags=re.IGNORECASE)\n",
    "        \n",
    "    return annotated_text\n",
    "# =============================================================================\n",
    "# PROMPT CREATION FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def create_key_summary_prompt(original_text: str) -> str:\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "[INST] You are an expert medical information extractor with exceptional attention to detail. Your task is to carefully read the medical text below and extract ONLY the key factual details that are explicitly mentioned. You must be extremely precise and never infer, assume, or add any information not directly stated in the text.\n",
    "\n",
    "**Medical Text:**\n",
    "\"{original_text}\"\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "Carefully analyze the text and extract information for each category below. For each category, provide ONLY what is explicitly mentioned in the original text. If a category is not mentioned or unclear, write \"Not mentioned\" for that section.\n",
    "\n",
    "**CRITICAL RULES:**\n",
    "1. Extract ONLY facts explicitly stated in the text\n",
    "2. Use the EXACT wording from the original text when possible\n",
    "3. Do not interpret, infer, or elaborate beyond what is written\n",
    "4. If multiple items exist in a category, separate them with semicolons\n",
    "5. Keep each section concise but complete\n",
    "\n",
    "**Format your response EXACTLY as follows:**\n",
    "\n",
    "**TREATMENTS RECEIVED:**\n",
    "[List only treatments, procedures, medications, surgeries, or therapeutic interventions explicitly mentioned as having been completed, given, or performed]\n",
    "\n",
    "**MEDICAL CONDITIONS:**\n",
    "[List only diagnoses, medical conditions, diseases, or pathological findings explicitly stated]\n",
    "\n",
    "**CANCER STAGE:**\n",
    "[Only if cancer staging information is explicitly mentioned - include exact stage notation like \"Stage IV\", \"T2N1M0\", etc.]\n",
    "\n",
    "**REFERRALS:**\n",
    "[Only if referrals to specialists, departments, other physicians, or healthcare facilities are explicitly mentioned]\n",
    "\n",
    "**CURRENT STATUS:**\n",
    "[Patient's current medical condition, discharge status, vital status, or clinical state as explicitly stated]\n",
    "\n",
    "**NEXT STEPS/PLAN:**\n",
    "[Only future medical plans, follow-up appointments, scheduled procedures, or treatment recommendations explicitly mentioned]\n",
    "\n",
    "Extract the key information now:\n",
    "[/INST]\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_audience_determination_prompt(original_text: str) -> str:\n",
    "\n",
    "    base_prompt = f\"\"\"\n",
    "[INST] You are an expert medical text classifier. Read the following medical text and determine the appropriate audience for a summary letter.\n",
    "\n",
    "**Medical Text:**\n",
    "\"{original_text}\"\n",
    "\n",
    "**Instructions:**\n",
    "Based on the text, who is the audience for the explanation letter?\n",
    "- If the text describes a patient recovering, Discharge Condition says much improved, or having a positive or follow ongoing treatment plan, the audience is the **patient**.\n",
    "- If the text mentions \"died\", \"passed away,\" \"deceased,\" or describes a fatal outcome such \"comfort care\" \"hospice care\" \"pallliative care\", \"palliative extubate\", the audience is the **patient's family**.\n",
    "\n",
    "Respond with a single word ONLY: **patient** or **family**.\n",
    "[/INST]\n",
    "\"\"\"\n",
    "    return base_prompt.format(original_text=original_text)\n",
    "\n",
    "\n",
    "def create_explanation_prompt(annotated_text: str, audience: str, keysummary: str) -> str:\n",
    "\n",
    "    \n",
    "    if audience == 'family':\n",
    "        audience_instruction = \"The determined audience for this letter is the **patient's family**. You must address them directly as 'you' and refer to the patient in the third person (e.g., 'your loved one,' 'he/she').\"\n",
    "    else:  # patient\n",
    "        audience_instruction = \"The determined audience for this letter is the **patient**. You must address them directly as 'you' throughout the entire letter.\"\n",
    "\n",
    "    base_prompt = f\"\"\"\n",
    "<s>[INST] \n",
    "### Persona\n",
    "You are an experienced and compassionate Oncologist (cancer specialist) and a skilled medical educator. Your primary role is to translate complex medical information into clear, understandable, and supportive explanations for patients and their families. Your tone should always be professional, empathetic, and honest, balancing realism with hope.\n",
    "\n",
    "### Golden Rule: Radical Simplicity - Translate, Don't Transfer\n",
    "Your single most important task is to convert medical terminology into simple, 8th-grade level English. Do not just define a medical term; replace it entirely with an easy-to-understand explanation.\n",
    "\n",
    "**Examples of what you MUST do:**\n",
    "* **INSTEAD OF:** \"multifocal stage IIA breast cancer\"\n",
    "    * **WRITE:** \"a type of breast cancer that was in an early stage and was found in more than one spot in the breast.\"\n",
    "* **INSTEAD OF:** \"a biopsy of the mass in your right axilla\"\n",
    "    * **WRITE:** \"we will take a small sample of the lump in your right armpit to test it.\"\n",
    "* **INSTEAD OF:** \"mastectomy with sentinel node and implant reconstruction\"\n",
    "    * **WRITE:** \"surgery to remove the breast, check the nearby glands to see if the cancer had spread, and rebuild the breast shape with an implant.\"\n",
    "\n",
    "### Understanding the Medical Note's Structure\n",
    "Before you write, you must understand how the original medical text is organized. This structure is key to creating a logical explanation.\n",
    "* **Chief Complaint (CC):** This is the main reason for the visit in one short sentence. It answers the question, \"Why are you here today?\"\n",
    "* **History of Present Illness (HPI):** This is the detailed story of the Chief Complaint. It explains the symptoms using a framework often remembered by the acronym \"OLDCARTS\":\n",
    "    * **O**nset: When did the problem begin?\n",
    "    * **L**ocation: Where is the symptom located?\n",
    "    * **D**uration: How long has it been going on?\n",
    "    * **C**haracterization: What does the symptom feel like (e.g., sharp, dull)?\n",
    "    * **A**lleviating/Aggravating factors: What makes it better or worse?\n",
    "    * **R**adiation: Does the sensation move anywhere else?\n",
    "    * **T**emporal factor: Is it worse at a certain time of day?\n",
    "    * **S**everity: How bad is it on a scale of 1 to 10?\n",
    "* **History:** This section provides background context, including Oncology History (past cancer diagnoses/treatments), Past Medical History, Surgical History, Family History, and Social History.\n",
    "* **Assessment and Plan (A&P):** This is the doctor’s summary and conclusion. The Assessment is the diagnosis, and the Plan outlines the next steps (tests, treatments, etc.).\n",
    "\n",
    "### Original Medical Text:\n",
    "\"{annotated_text}\"\n",
    "\n",
    "### Internal Fact-Checking Reference:\n",
    "This technical summary is for your internal use ONLY to ensure your response is factually accurate.\n",
    "**STRICT INSTRUCTION:** You must treat this summary as a list of facts to be **translated** into simple language. DO NOT copy the medical terminology from this summary directly into the patient letter. You must translate these facts into the simple, empathetic language required by your persona.\n",
    "\"{keysummary}\"\n",
    "\n",
    "### {audience_instruction}\n",
    "\n",
    "### Your Task:\n",
    "Your goal is to write a single, complete, and polished letter that explains the information from the medical text above, following the logical flow of a clinical visit. Imagine you are sitting with the recipient and explaining this to them in person, then putting it in writing.\n",
    "STRICT NEGATIVE CONSTRAINT: Under NO circumstances should you say something not exist in the note.\n",
    "\n",
    "**1. Letter Structure and Flow:** Organize your letter logically using the following question-based headers to guide the reader.\n",
    "    * **Empathetic Opening:** Start with a short, warm, and supportive salutation. (e.g., \"Dear [Patient Name],\") Acknowledge the reason for their recent visit.\n",
    "    * **Why did you come to the clinic?**\n",
    "        * Answer this using information from the **Chief Complaint (CC)** section. State the main symptom or reason for the visit clearly and simply.\n",
    "    * **What was discussed?**\n",
    "        * Answer this by translating the story from the **History of Present Illness (HPI)**. Describe the symptoms in plain language (when they started, what they feel like, what makes them better/worse, etc.), following the OLDCARTS framework.\n",
    "        * Also, briefly recap relevant information from the **History** section (e.g., \"As a reminder, your initial diagnosis was...\").\n",
    "        * **Crucial rule:** When discussing past treatment decisions, use neutral language. Instead of \"you refused,\" say \"At that time, the decision was made not to proceed with...\"\n",
    "    * **What did we find? (Assessment)**\n",
    "        * Clearly and gently explain the main conclusion from the **Assessment** part of the note. Explain what terms like \"metastatic\" or \"recurrence\" mean. Use an analogy if helpful.\n",
    "        * **STRICT NEGATIVE CONSTRAINT:** If the cancer has spread (metastasis), absolutely DO NOT list the specific organs affected. Instead, just say \"the cancer has spread to other parts of your body.\"\n",
    "    * **What is the plan? (Plan)**\n",
    "        * Detail the next steps from the **Plan** section. For each step (biopsy, MRI, new medication), explain **WHAT** it is and, more importantly, **WHY** we are doing it.\n",
    "        * If the note mentions palliative care, explain it as an active treatment focused on controlling cancer, managing symptoms, and maximizing quality of life. The focus must be on living well.\n",
    "    * **Closing with Support:** End the letter by reinforcing that your team is there to support them.\n",
    "\n",
    "**2. Language and Tone Directives:**\n",
    "    * **Maintain Your Persona:** Use \"we\" for the medical team. Write with empathy and clarity.\n",
    "    * **STRICT NEGATIVE CONSTRAINTS:**\n",
    "        * Do not use fatalistic language. Avoid phrases like \"until the end of your life\" or \"preparing for the end.\" The focus MUST be on quality and extension of LIFE.\n",
    "        * Avoid lengthy, overly sympathetic sentences. Be concise and focused on the key information.\n",
    "\n",
    "### Strict Output Formatting:\n",
    "Provide ONLY the letter. Your output must start directly with the salutation and end with the signature. Use the question-based headers exactly as specified in the structure above.\n",
    "\n",
    "Dear [Patient Name],\n",
    "\n",
    "I am writing to summarize our discussion from your recent visit. I know that receiving and processing this information can be overwhelming, and I hope this written summary is helpful.\n",
    "\n",
    "**Why did you come to the clinic?**\n",
    "[Your simplified explanation of the Chief Complaint]\n",
    "\n",
    "**What was discussed?**\n",
    "[Your simplified explanation of the HPI and relevant History]\n",
    "\n",
    "**What did we find? (Assessment)**\n",
    "[Your simplified explanation of the diagnosis]\n",
    "\n",
    "**What is the plan?**\n",
    "[Your simplified explanation of the next steps, tests, and treatment goals]\n",
    "\n",
    "We are here to support you every step of the way. Please do not hesitate to contact our office with any questions you may have.\n",
    "\n",
    "Sincerely,\n",
    "Your Care Team at [Institution]\n",
    "\n",
    "Please provide the patient-focused explanation now. Your output must start directly with the salutation.\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "    \n",
    "    return base_prompt.format(\n",
    "        annotated_text=annotated_text,\n",
    "        keysummary=keysummary,\n",
    "        audience_instruction=audience_instruction\n",
    "    )\n",
    "\n",
    "def create_cleaning_prompt(raw_response: str, audience: str) -> str:\n",
    "\n",
    "    base_prompt = f\"\"\"\n",
    "[INST]\n",
    "### Persona\n",
    "You are an expert medical writer and editor. Your unique skill is communicating complex clinical information with absolute precision and clarity, and you are adept at tailoring your language for different audiences, from senior physicians to concerned patients. Your primary directive is to preserve the original meaning without fail.\n",
    "\n",
    "### Target Audience:\n",
    "{audience}\n",
    "\n",
    "### Your Task\n",
    "Your primary task is to revise the provided medical text. Based on the specified **Target Audience**, you will improve its quality in the following areas:\n",
    "\n",
    "1.  **Clean Up Language:** Improve sentence structure and use professional language appropriate for the target audience.\n",
    "2.  **Reduce Repetition:** Eliminate redundant words and phrases without losing critical information.\n",
    "3.  **Improve Flow:** Enhance the logical flow and transitions to make the narrative easier to follow.\n",
    "4.  **Define Medical Terms (Conditional Task):**\n",
    "    * **IF the Target Audience is 'Patient/Layperson'**, you MUST perform this task: For any medical term or jargon a non-medical person would not understand, provide a simple, brief explanation in parentheses immediately after its first appearance.\n",
    "    * **Example:** \"The patient presented with tachycardia (a heart rate over 100 beats per minute) and pedal edema (swelling in the feet).\"\n",
    "    * **IF the Target Audience is 'Clinical Professional'**, you MUST NOT perform this task. Do not define standard medical terms.\n",
    "\n",
    "### The Golden Rule: Preserve Clinical Meaning at All Costs\n",
    "This is the most important rule. The revised text MUST be semantically and factually identical to the original.\n",
    "\n",
    "**STRICT PROHIBITIONS:**\n",
    "* **DO NOT** alter, add, or remove any clinical facts, diagnoses, measurements, dosages, or timelines.\n",
    "* **DO NOT** change the certainty of a statement. A possibility (\"suggests,\" \"possible\") must remain a possibility. A certainty (\"diagnosed with,\" \"confirmed\") must remain a certainty.\n",
    "* **DO NOT** reorder information in a way that changes the chronological or logical sequence of events.\n",
    "\n",
    "---\n",
    "### Instructions for Output\n",
    "\n",
    "Please provide your response in two parts:\n",
    "1.  **Revised Medical Text:** The complete, revised version of the text, tailored for the specified audience.\n",
    "2.  **Summary of Changes:** A brief, bulleted list explaining the key changes you made.\n",
    "\n",
    "### Medical Text to Revise:\n",
    "\n",
    "{raw_response}\n",
    "\n",
    "Now, please proceed with the revision based on the specified Target Audience.\n",
    "[/INST]\n",
    "\"\"\"\n",
    "    return base_prompt.format(raw_response=raw_response)\n",
    "\n",
    "\n",
    "def get_default_generation_config(tokenizer) -> Dict:\n",
    "    \"\"\"Get default generation configuration.\"\"\"\n",
    "    return {\n",
    "        \"max_new_tokens\": 999,\n",
    "        \"temperature\": 0.3,\n",
    "        \"top_p\": 0.85,\n",
    "        \"repetition_penalty\": 1.15,\n",
    "        \"do_sample\": True,\n",
    "        \"pad_token_id\": tokenizer.eos_token_id,\n",
    "        \"eos_token_id\": tokenizer.eos_token_id,\n",
    "        \"early_stopping\": True,\n",
    "    }\n",
    "\n",
    "\n",
    "def get_audience_generation_config(tokenizer) -> Dict:\n",
    "    \"\"\"Get generation configuration for audience determination.\"\"\"\n",
    "    return {\n",
    "        \"max_new_tokens\": 5,\n",
    "        \"temperature\": 0.01,\n",
    "        \"do_sample\": False,\n",
    "        \"pad_token_id\": tokenizer.eos_token_id\n",
    "    }\n",
    "\n",
    "\n",
    "def get_cleaning_generation_config(tokenizer) -> Dict:\n",
    "    \"\"\"Get generation configuration for response cleaning.\"\"\"\n",
    "    return {\n",
    "        \"max_new_tokens\": 600,\n",
    "        \"temperature\": 0.1,\n",
    "        \"top_p\": 0.9,\n",
    "        \"repetition_penalty\": 1.1,\n",
    "        \"do_sample\": True,\n",
    "        \"pad_token_id\": tokenizer.eos_token_id,\n",
    "        \"eos_token_id\": tokenizer.eos_token_id,\n",
    "        \"early_stopping\": True,\n",
    "    }\n",
    "\n",
    "\n",
    "def parse_audience_response(raw_response: str) -> str:\n",
    "    \"\"\"Parse and validate audience determination response.\"\"\"\n",
    "    cleaned_response = raw_response.lower().strip()\n",
    "    if \"family\" in cleaned_response:\n",
    "        return \"family\"\n",
    "    else:\n",
    "        return \"patient\"  # Default to patient\n",
    "\n",
    "def determine_audience(original_text: str, model, tokenizer) -> str:\n",
    "    \"\"\"\n",
    "    Determine the target audience for the medical explanation.\n",
    "    \n",
    "    Args:\n",
    "        original_text: The original medical text\n",
    "        model: The loaded model instance\n",
    "        tokenizer: The tokenizer instance\n",
    "    \n",
    "    Returns:\n",
    "        Audience type ('patient' or 'family')\n",
    "    \"\"\"\n",
    "    try:\n",
    "        prompt = create_audience_determination_prompt(original_text)\n",
    "        config = get_audience_generation_config(tokenizer)\n",
    "        raw_output = run_model(prompt, model, tokenizer, config)\n",
    "        audience = parse_audience_response(raw_output)\n",
    "        return audience\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Audience determination failed: {e}. Defaulting to 'patient'.\")\n",
    "        return \"patient\"\n",
    "\n",
    "\n",
    "\n",
    "def parse_key_summary_response(original_text: str, model, tokenizer) -> str:\n",
    "\n",
    "    prompt = create_key_summary_prompt(original_text)\n",
    "    config = {\n",
    "        \"max_new_tokens\": 500,\n",
    "        \"temperature\": 0.1,  # Very low temperature for factual accuracy\n",
    "        \"top_p\": 0.9,\n",
    "        \"repetition_penalty\": 1.1,\n",
    "        \"do_sample\": True,\n",
    "        \"pad_token_id\": tokenizer.eos_token_id,\n",
    "        \"eos_token_id\": tokenizer.eos_token_id,\n",
    "        \"early_stopping\": True,\n",
    "    }\n",
    "    keysummary = run_model(prompt, model, tokenizer, config)\n",
    "    return keysummary\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate_explanation(annotated_text: str, \n",
    "                        audience: str, keysummary: str, model, tokenizer) -> str:\n",
    "    prompt = create_explanation_prompt(annotated_text, audience,keysummary)\n",
    "    config = get_default_generation_config(tokenizer)\n",
    "    return run_model(prompt, model, tokenizer, config)\n",
    "\n",
    "\n",
    "\n",
    "def clean_response(raw_response: str, audience: str, model, tokenizer) -> str:\n",
    "    prompt = create_cleaning_prompt(raw_response, audience)\n",
    "    config = get_cleaning_generation_config(tokenizer)\n",
    "    cleaned = run_model(prompt, model, tokenizer, config)\n",
    "    return cleaned\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95303d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "KEY_SUMMARY_CONFIG = {\n",
    "    \"max_new_tokens\": 500,\n",
    "    \"temperature\": 0.1,  # Low temperature for factual, precise extraction\n",
    "    \"top_p\": 0.9,\n",
    "    \"repetition_penalty\": 1.1,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "    \"eos_token_id\": tokenizer.eos_token_id,\n",
    "    \"early_stopping\": True,\n",
    "}\n",
    "\n",
    "AUDIENCE_DETERMINATION_CONFIG = {\n",
    "    \"max_new_tokens\": 5,\n",
    "    \"temperature\": 0.01, # Almost zero temperature for deterministic classification\n",
    "    \"do_sample\": False,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "}\n",
    "\n",
    "EXPLANATION_GENERATION_CONFIG = {\n",
    "    \"max_new_tokens\": 999,\n",
    "    \"temperature\": 0.3, # Creative but controlled temperature for generation\n",
    "    \"top_p\": 0.85,\n",
    "    \"repetition_penalty\": 1.15,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "    \"eos_token_id\": tokenizer.eos_token_id,\n",
    "    \"early_stopping\": True,\n",
    "}\n",
    "\n",
    "CLEANING_CONFIG = {\n",
    "    \"max_new_tokens\": 600,\n",
    "    \"temperature\": 0.1, # Low temperature for precise editing\n",
    "    \"top_p\": 0.9,\n",
    "    \"repetition_penalty\": 1.1,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "    \"eos_token_id\": tokenizer.eos_token_id,\n",
    "    \"early_stopping\": True,\n",
    "}\n",
    "\n",
    "def run_model(prompt: str, model, tokenizer, generation_config: Dict) -> str:\n",
    "    \"\"\"The single, core function to run model generation.\"\"\"\n",
    "    try:\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(model.device)\n",
    "        input_length = inputs[\"input_ids\"].shape[1]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                **generation_config\n",
    "            )\n",
    "        \n",
    "        response_tokens = outputs[0][input_length:]\n",
    "        raw_output = tokenizer.decode(response_tokens, skip_special_tokens=True)\n",
    "        return raw_output.strip()\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error generating response: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28f2dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing row 84/2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yc/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.01` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/yc/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/yc/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:638: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def run_model(prompt: str, model, tokenizer, generation_config: Dict) -> str:\n",
    "    \"\"\"The single, core function to run model generation.\"\"\"\n",
    "    try:\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(model.device)\n",
    "        input_length = inputs[\"input_ids\"].shape[1]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                **generation_config\n",
    "            )\n",
    "        \n",
    "        response_tokens = outputs[0][input_length:]\n",
    "        raw_output = tokenizer.decode(response_tokens, skip_special_tokens=True)\n",
    "        return raw_output.strip()\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error generating response: {str(e)}\"\n",
    "\n",
    "# The text processing, annotation, and prompt creation functions remain the same.\n",
    "# They are well-defined and serve distinct purposes.\n",
    "# --- extract_medical_terms(text, meddict) -> Dict\n",
    "# --- get_n_grams(text, n) -> List\n",
    "# --- find_term_in_dict(term, meddict) -> Optional[str]\n",
    "# --- create_annotated_text(text, meddict) -> str\n",
    "# --- create_key_summary_prompt(original_text) -> str\n",
    "# --- create_audience_determination_prompt(original_text) -> str\n",
    "# --- create_explanation_prompt(annotated_text, audience, keysummary) -> str\n",
    "# --- create_cleaning_prompt(raw_response, audience) -> str\n",
    "\n",
    "def parse_audience_response(raw_response: str) -> str:\n",
    "    \"\"\"\n",
    "    This helper is kept because it performs a specific parsing task,\n",
    "    not just a model call.\n",
    "    \"\"\"\n",
    "    cleaned_response = raw_response.lower().strip()\n",
    "    return \"family\" if \"family\" in cleaned_response else \"patient\"\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# REFACTORED MAIN PROCESSING LOOP\n",
    "# =============================================================================\n",
    "\n",
    "all_results = []\n",
    "LINE_WIDTH = 100 # Example line width\n",
    "\n",
    "# Assume 'df' is your DataFrame and 'meddict', 'model', 'tokenizer' are loaded.\n",
    "for index, row in df.iterrows():\n",
    "    text = row['note_text'] \n",
    "    \n",
    "    print(f\"\\nProcessing row {index + 1}/{len(df)}...\")\n",
    "    \n",
    "    # --- Step 1: Determine Audience ---\n",
    "    audience_prompt = create_audience_determination_prompt(text)\n",
    "    raw_audience_output = run_model(audience_prompt, model, tokenizer, AUDIENCE_DETERMINATION_CONFIG)\n",
    "    audience = parse_audience_response(raw_audience_output)\n",
    "    \n",
    "    # --- Step 2: Annotate Text (Non-LLM step) ---\n",
    "    annotated_text = create_annotated_text(text, meddict)\n",
    "    \n",
    "    # --- Step 3: Extract Key Summary ---\n",
    "    summary_prompt = create_key_summary_prompt(text)\n",
    "    keysummary = run_model(summary_prompt, model, tokenizer, KEY_SUMMARY_CONFIG)\n",
    "    \n",
    "    # --- Step 4: Generate Initial Explanation ---\n",
    "    explanation_prompt = create_explanation_prompt(annotated_text, audience, keysummary)\n",
    "    explanation = run_model(explanation_prompt, model, tokenizer, EXPLANATION_GENERATION_CONFIG)\n",
    "\n",
    "    # --- Step 5: Clean and Finalize the Explanation ---\n",
    "    cleaning_prompt = create_cleaning_prompt(explanation, audience)\n",
    "    final_result = run_model(cleaning_prompt, model, tokenizer, CLEANING_CONFIG)\n",
    "    \n",
    "    # --- Store and Print Results ---\n",
    "    row_result = {\n",
    "        'original_text': text,\n",
    "        'determined_audience': audience,\n",
    "        'annotated_text': annotated_text,\n",
    "        'keysummary': keysummary,\n",
    "        'raw_explanation': explanation,\n",
    "        'final_letter': final_result\n",
    "    }\n",
    "    \n",
    "    all_results.append(row_result)\n",
    "\n",
    "    # Print the result immediately after generation\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"RESULTS FOR ROW {index + 1}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    for col in ['keysummary', 'raw_explanation', 'final_letter']:\n",
    "        print(f\"\\n--- Column: {col} ---\")\n",
    "        original_text = row_result[col]\n",
    "        wrapped_text = textwrap.fill(original_text, width=LINE_WIDTH)\n",
    "        print(wrapped_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c1b0c9",
   "metadata": {},
   "source": [
    "# generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf5b8b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved to 'output_40.csv'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Convert to DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Save to CSV\n",
    "base_filename = 'output'\n",
    "extension = '.csv'\n",
    "output_filename = base_filename + extension\n",
    "counter = 1\n",
    "\n",
    "while os.path.exists(output_filename):\n",
    "    output_filename = f\"{base_filename}_{counter}{extension}\"\n",
    "    counter += 1\n",
    "\n",
    "results_df.to_csv(output_filename, index=False, sep='||')\n",
    "print(f\"DataFrame saved to '{output_filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f02b539",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
