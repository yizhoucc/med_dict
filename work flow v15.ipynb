{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4dd4555-473d-48cd-bc86-60a4c41da0a8",
   "metadata": {},
   "source": [
    "## load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611f29b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b2c708e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yc/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "with open(\"hf.token\", \"r\") as f:\n",
    "    hftoken = f.read().strip()  \n",
    "\n",
    "import os\n",
    "cache_dir = \"/mnt/c/Users/yc/.cache/huggingface\"\n",
    "os.environ['HF_HOME'] = cache_dir\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(token=hftoken)  # Move token to environment variable\n",
    "\n",
    "from ult import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d74a59c-b625-4944-8a79-aea69bc6cd8a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [01:46<00:00, 26.69s/it]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "device_map = {\"\": 0}\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=device_map,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e1c388-57f9-41b0-87d1-25a9ad884430",
   "metadata": {},
   "source": [
    "## data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6281f4-7ebb-48fd-a08b-4271349ededf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coral_idx</th>\n",
       "      <th>Sex</th>\n",
       "      <th>UCSFDerivedRaceEthnicity_X</th>\n",
       "      <th>BirthDate</th>\n",
       "      <th>note_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>184</td>\n",
       "      <td>Female</td>\n",
       "      <td>Latinx</td>\n",
       "      <td>1983-10-04</td>\n",
       "      <td>Medical Oncology Consult Note  Video Consult  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>185</td>\n",
       "      <td>Female</td>\n",
       "      <td>Multi-Race/Ethnicity</td>\n",
       "      <td>1973-03-08</td>\n",
       "      <td>This is a shared service.  Physician Statement...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>193</td>\n",
       "      <td>Female</td>\n",
       "      <td>Multi-Race/Ethnicity</td>\n",
       "      <td>1979-06-20</td>\n",
       "      <td>ID: ***** ***** is a 39 y.o. premenopausal pat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>210</td>\n",
       "      <td>Female</td>\n",
       "      <td>Southwest Asian and North African</td>\n",
       "      <td>1974-04-05</td>\n",
       "      <td>Patient Name: ***** *****  ***** *****: 08/22/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>223</td>\n",
       "      <td>Female</td>\n",
       "      <td>Unknown/Declined</td>\n",
       "      <td>1960-10-12</td>\n",
       "      <td>We performed this consultation using real-time...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    coral_idx     Sex         UCSFDerivedRaceEthnicity_X   BirthDate  \\\n",
       "44        184  Female                             Latinx  1983-10-04   \n",
       "45        185  Female               Multi-Race/Ethnicity  1973-03-08   \n",
       "53        193  Female               Multi-Race/Ethnicity  1979-06-20   \n",
       "70        210  Female  Southwest Asian and North African  1974-04-05   \n",
       "83        223  Female                   Unknown/Declined  1960-10-12   \n",
       "\n",
       "                                            note_text  \n",
       "44  Medical Oncology Consult Note  Video Consult  ...  \n",
       "45  This is a shared service.  Physician Statement...  \n",
       "53  ID: ***** ***** is a 39 y.o. premenopausal pat...  \n",
       "70  Patient Name: ***** *****  ***** *****: 08/22/...  \n",
       "83  We performed this consultation using real-time...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_file_path = 'data/formaldef.txt'\n",
    "formaldic = txt_to_dict(txt_file_path)\n",
    "len(formaldic)\n",
    "\n",
    "meddict={}\n",
    "for k,v in formaldic.items():\n",
    "    meddict[k.split('Listen to pronunciation')[0].split('(')[0]]=v\n",
    "filename= 'data/filtered_medical_dictionary.csv'\n",
    "eighth_grade_words=set()\n",
    "with open(filename, 'r', encoding='utf-8') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    next(reader)  # Skip the header row\n",
    "    for row in reader:\n",
    "        if row:  # Make sure row is not empty\n",
    "            eighth_grade_words.add(row[0])  # Add the word (first column)\n",
    "filtered_meddict = {word: explanation for word, explanation in meddict.items() \n",
    "                   if word in eighth_grade_words}\n",
    "meddict=filtered_meddict\n",
    "# load data\n",
    "df = pd.read_csv('data/CORAL/coral-expert-curated-medical-oncology-reports-to-advance-language-model-inference-1.0/coral/unannotated/data/breastca_unannotated.csv')\n",
    "# df = df.sample(1, random_state=42)\n",
    "df=df.iloc[[44,45,53,70,83]] \n",
    "test_note=df.iloc[0]['note_text']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfa3a9d",
   "metadata": {},
   "source": [
    "# save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c3bce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction_prompts = {\n",
    "    \"Reason_for_Visit\": \"\"\"\n",
    "TASK: Extract 'Reason for Visit'.\n",
    "Respond *only* with a JSON object using this exact schema:\n",
    "{\n",
    "\"visit_type\": \"e.g., New patient, Surveillance, Side effect management\",\n",
    "\"summary\": \"A brief summary of the reason for visit.\"\n",
    "}\n",
    "\"\"\",\n",
    "\n",
    "\n",
    "    \"What_We_Found\": \"\"\"\n",
    "TASK: Extract 'What We Found'.\n",
    "Respond *only* with a JSON object using this exact schema:\n",
    "{\n",
    "\"imaging_summary\": \"Summary of most recent imaging.\",\n",
    "\"lab_summary\": \"Summary of key lab results.\",\n",
    "\"performance_status\": \"e.g., ECOG 0, Stable\",\n",
    "\"findings\": \"Summary of new findings or disease status.\"\n",
    "}\n",
    "\"\"\",\n",
    "\n",
    "\n",
    "    \"Treatment_Summary\": \"\"\"\n",
    "TASK: Extract 'Treatment Summary'.\n",
    "Respond *only* with a JSON object using this exact schema:\n",
    "{\n",
    "\"current_meds\": \"List of current oncologic medications or regimens.\",\n",
    "\"recent_changes\": \"Any holds, dose reductions, or switches.\",\n",
    "\"supportive_meds\": \"List of supportive medications.\"\n",
    "}\n",
    "\"\"\",\n",
    "\n",
    "    \"What_We_Discussed\": \"\"\"\n",
    "TASK: Extract 'What We Discussed / Decided'.\n",
    "Respond *only* with a JSON object using this exact schema:\n",
    "{\n",
    "\"goals_of_treatment\": \"e.g., Palliative, Surveillance\",\n",
    "\"response_assessment\": \"How the cancer is responding.\",\n",
    "\"side_effects_discussed\": \"Side effects discussed and match them to the corresponding medications.\",\n",
    "\"other_discussions\": \"e.g., Clinical trials, palliative care\"\n",
    "}\n",
    "\"\"\",\n",
    "\n",
    "    \"Plan_Going_Forward\": \"\"\"\n",
    "TASK: Extract 'Plan Going Forward'.\n",
    "Respond *only* with a JSON object using this exact schema:\n",
    "{\n",
    "\"medication_plan\": \"e.g., Continue, Start, Stop\",\n",
    "\"imaging_plan\": \"Which scans and when.\",\n",
    "\"lab_plan\": \"Which labs and when.\",\n",
    "\"referrals\": \"List of any new referrals.\",\n",
    "\"follow_up\": \"When the next visit is.\"\n",
    "}\n",
    "\"\"\",\n",
    "\n",
    "\"Medication_Plan\": \"\"\"\n",
    "TASK: Extract the 'Medication Plan'.\n",
    "Include future medication plans, changes to current meds (start, stop, continue), supportive meds, bowel regimen, and any blood transfusion plan.\n",
    "Respond *only* with a JSON object using this exact schema:\n",
    "{\n",
    "    \"medication_plan\": \"A summary of the complete medication plan.\"\n",
    "}\n",
    "\"\"\",\n",
    "\n",
    "\"Imaging and Lab Plan\": \"\"\"\n",
    "TASK: Extract the 'Imaging Plan'.\n",
    "Include all future imaging like CT, MRI, PET, ultrasound,  DEXA scans, including timing if mentioned.\n",
    "Include future labs like CBC, CMP, tumor markers, coagulation profile, etc. Specify frequency and rationale if mentioned.\n",
    "Respond *only* with a JSON object using this exact schema:\n",
    "{\n",
    "    \"imaging_lab_plan\": \"A summary of the future imaging plan (e.g., 'CT chest in 3 months').\"\n",
    "}\n",
    "\"\"\",\n",
    "\n",
    "\"Genetic_Testing_Plan\": \"\"\"\n",
    "TASK: Extract the 'Genetic Testing Plan'.\n",
    "Include any molecular or genetic testing, such as tumor sequencing, germline panels, or liquid biopsy, do not include testings that are already done.\n",
    "Respond *only* with a JSON object using this exact schema:\n",
    "{\n",
    "    \"genetic_testing_plan\": \"A summary of any future planned genetic or molecular tests.\"\n",
    "}\n",
    "\"\"\",\n",
    "\n",
    "\"Procedure_Plan\": \"\"\"\n",
    "TASK: Extract the 'Procedure Plan'.\n",
    "Include future procedures like surgery, radiation therapy, or interventional procedures.\n",
    "Respond *only* with a JSON object using this exact schema:\n",
    "{\n",
    "    \"procedure_plan\": \"A summary of any planned procedures.\"\n",
    "}\n",
    "\"\"\",\n",
    "\n",
    "\"Referral\": \"\"\"\n",
    "TASK: Extract 'Referral'.\n",
    "Respond *only* with a JSON object using this exact schema:\n",
    "{\n",
    "\"Nutrition\": \"any nutration referrals such as diet optimization, appetite, weight maintenance), \"Genetics\": \"eg, germline testing, family counseling\",\n",
    "\"other\": \"eg, Palliative care (symptom or pain management, goals of care), Radiation oncology / surgical oncology, Physical or occupational therapy, Social work, financial counseling, Psychology / psychiatry for coping and mood support\"\n",
    "}\n",
    "\"\"\",\n",
    "\n",
    "\"follow up/next visit\": \"\"\"\n",
    "TASK: Extract the 'follow up/next visit'.\n",
    "Respond *only* with a JSON object using this exact schema:\n",
    "{\n",
    "    \"Next clinic visit\": \"(in-person or telehealth): timing and purpose\"\n",
    "}\n",
    "\"\"\",\n",
    "\n",
    "\"Advance care planning\": \"\"\"\n",
    "TASK: Extract the 'Advance care planning'.\n",
    "Include Advance directives, health-care proxy, code status (if appropriate)、\n",
    "Respond *only* with a JSON object using this exact schema:\n",
    "{\n",
    "    \"Advance care\": \"A summary of any planned Advance care.\"\n",
    "}\n",
    "\"\"\",\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "15fe5d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "res=[]\n",
    "\n",
    "for row in df.itertuples():\n",
    "    test_note=row.note_text\n",
    "    gen_config = {\n",
    "    \"max_new_tokens\": 512,  # <-- Increased from 150 to 512\n",
    "    \"eos_token_id\": tokenizer.eos_token_id,\n",
    "    \"do_sample\": False\n",
    "    }\n",
    "\n",
    "    # myprint(\"\\n--- 1. Creating Base KV Cache from long text... ---\")\n",
    "\n",
    "    base_prompt = (\n",
    "        f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n\"\n",
    "        f\"You are a medical data extraction expert. You will be given a long medical note. \"\n",
    "        f\"Your task is to answer a series of questions about it, one by one. \"\n",
    "        # FIX 2: Stricter instruction in system prompt\n",
    "        f\"Respond *only* with the valid JSON object requested. Do not add markdown backticks or any other text.\"\n",
    "        f\"<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "        f\"Here is the medical note:\\n\\n\"\n",
    "        f\"--- BEGIN NOTE ---\\n{test_note}\\n--- END NOTE ---\"\n",
    "        f\"\\n\\nI will now ask you to extract specific sections. \"\n",
    "        f\"Please wait for my first extraction task.\"\n",
    "        f\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "        f\"{{\\\"status\\\": \\\"Understood. I have read the note and am ready.\\\"}}\"\n",
    "    )\n",
    "\n",
    "    # myprint(\"  Tokenizing and running forward pass to get base cache...\")\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(base_prompt, return_tensors=\"pt\").to(model.device)\n",
    "        \n",
    "        # This logic to create the base_cache is correct\n",
    "        outputs = model(\n",
    "            input_ids=inputs[\"input_ids\"], \n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            use_cache=True\n",
    "        )\n",
    "        \n",
    "        base_cache = outputs.past_key_values\n",
    "        \n",
    "        # Clean up\n",
    "        del inputs, outputs\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    # myprint(\"Base KV Cache created. Ready for 'branching' extractions.\")\n",
    "\n",
    "    # --- FIX 3: Define STRICT SCHEMAS for each extraction task ---\n",
    "    # This is the most important \"prompt engineering\" fix.\n",
    "    # We are telling the model *exactly* what keys to use.\n",
    "\n",
    "\n",
    "    extracted_data = {}\n",
    "\n",
    "    # myprint(\"\\n--- 2. Running EFFICIENT 'Branching' Extractions ---\")\n",
    "\n",
    "    # (Assuming `run_model_with_cache_manual` is your function `run_model_with_cache`)\n",
    "    # If not, please rename this call to `run_model_with_cache`\n",
    "    run_model_function = run_model_with_cache_manual \n",
    "\n",
    "    for key, task in extraction_prompts.items():\n",
    "        # myprint(f\"\\nExtracting: {key}...\")\n",
    "        \n",
    "        # --- FIX 4: Remove the BOS token from the loop ---\n",
    "        # The base_cache *already* contains the BOS token.\n",
    "        # Adding it again can cause errors.\n",
    "        task_prompt = (\n",
    "            f\"<|start_header_id|>user<|end_header_id|>\\n\\n\" # <-- Removed <|begin_of_text|>\n",
    "            f\"{task}\" # <-- Use the new, detailed prompt\n",
    "            f\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "        )\n",
    "\n",
    "        # Use the SAME base_cache for each extraction\n",
    "        answer, returned_cache = run_model_function(\n",
    "            task_prompt, \n",
    "            model, \n",
    "            tokenizer, \n",
    "            gen_config, \n",
    "            kv_cache=base_cache\n",
    "        )\n",
    "        \n",
    "        # Your memory management here is good.\n",
    "        del returned_cache\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        # myprint(f\"  Raw Output: {answer}\")\n",
    "        \n",
    "        try:\n",
    "            # We are now *only* expecting a JSON object\n",
    "            clean_answer = answer.strip().strip(\"```json\").strip(\"```\").strip()\n",
    "            extracted_data[key] = json.loads(clean_answer)\n",
    "        except json.JSONDecodeError:\n",
    "            extracted_data[key] = {\"error\": \"Failed to parse JSON\", \"raw\": answer}\n",
    "\n",
    "    # myprint(\"\\n--- 3. All extractions complete. ---\")\n",
    "    # myprint(\"\\n--- FINAL EXTRACTED DATA ---\")\n",
    "    res.append(json.dumps(extracted_data, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c87f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "resdf=[json.loads(r) for r in res]\n",
    "for i in range(5):\n",
    "    test_note=df.iloc[i]['note_text']\n",
    "    resdf[i]['note']=test_note\n",
    "    resdf[i]['coral_idx']=df.iloc[i].coral_idx\n",
    "resdf=pd.DataFrame(resdf)\n",
    "mysave(resdf, base_filename='output/keysummary', extension='.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
