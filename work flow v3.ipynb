{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "503c81c2-012a-4e15-a255-a4a3e4c576d7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install sentencepiece protobuf datasets transformers trl textstat peft bitsandbytes nltk --quiet\n",
    "!pip install -U bitsandbytes accelerate --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b2c708e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"hf.token\", \"r\") as f:\n",
    "    hftoken = f.read().strip()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43c967d-ea35-4d4e-8ddc-f39328032b1a",
   "metadata": {},
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e3b7919-277b-47b9-95b9-eab518373862",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yc/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "\n",
    "# Third-party data and ML libraries\n",
    "import pandas as pd\n",
    "import torch\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Hugging Face ecosystem\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "\n",
    "# Fine-tuning and optimization\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# Text analysis and readability metrics\n",
    "from textstat import (\n",
    "    flesch_kincaid_grade, \n",
    "    flesch_reading_ease,\n",
    "    smog_index, \n",
    "    gunning_fog, \n",
    "    dale_chall_readability_score,\n",
    "    text_standard, \n",
    "    syllable_count\n",
    ")\n",
    "\n",
    "# Optional: Uncomment if needed\n",
    "from huggingface_hub import login\n",
    "login(token=hftoken)  # Move token to environment variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dd4555-473d-48cd-bc86-60a4c41da0a8",
   "metadata": {},
   "source": [
    "## load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d74a59c-b625-4944-8a79-aea69bc6cd8a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  3.42s/it]\n"
     ]
    }
   ],
   "source": [
    "# Model Configuration\n",
    "# model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "# Recommended upgrade - Llama 3.1 8B\n",
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "# # Or try Mistral 7B\n",
    "# model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "device_map = {\"\": 0}\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "# Load model with quantization\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     device_map=device_map,\n",
    "#     quantization_config=bnb_config\n",
    "# )\n",
    "\n",
    "# full precision\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=device_map,\n",
    "    torch_dtype=torch.float16  # Use half precision instead\n",
    ")\n",
    "\n",
    "# # 8 precision\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     device_map=device_map,\n",
    "#     load_in_8bit=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e1c388-57f9-41b0-87d1-25a9ad884430",
   "metadata": {},
   "source": [
    "## data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6281f4-7ebb-48fd-a08b-4271349ededf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def calculate_readability_scores(text, term=None):\n",
    "    \"\"\"Calculate readability scores for a piece of text, optionally removing a term\"\"\"\n",
    "    scoring_text = text\n",
    "    \n",
    "    # Remove the term and its variations before scoring if provided\n",
    "    if term:\n",
    "        # Create variations of the term to remove (capitalized, lowercase)\n",
    "        term_variations = [term, term.lower(), term.capitalize()]\n",
    "        \n",
    "        for variation in term_variations:\n",
    "            scoring_text = scoring_text.replace(variation, \"\")\n",
    "        \n",
    "        # Clean up any double spaces created\n",
    "        scoring_text = \" \".join(scoring_text.split())\n",
    "    \n",
    "    fk_grade = flesch_kincaid_grade(scoring_text)\n",
    "    readability = flesch_reading_ease(scoring_text)\n",
    "    return {\n",
    "        \"fk_grade\": fk_grade,\n",
    "        \"readability\": readability,\n",
    "    } \n",
    "def txt_to_dict(file_path):\n",
    "    data_dict = {}\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        for i in range(0, len(lines) - 1, 2):\n",
    "            key = lines[i].strip()    # Odd line are key\n",
    "            value = lines[i + 1].strip()  # Even line are value\n",
    "            data_dict[key] = value\n",
    "\n",
    "    return data_dict\n",
    "\n",
    "\n",
    "txt_file_path = 'formaldef.txt' \n",
    "formaldic = txt_to_dict(txt_file_path)\n",
    "len(formaldic)\n",
    "\n",
    "meddict={}\n",
    "for k,v in formaldic.items():\n",
    "    meddict[k.split('Listen to pronunciation')[0].split('(')[0]]=v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c6d4475-de2d-4b19-b300-0db9acd700c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "file_name = \"Discharge Sum_Sample_MIMIC.csv\"\n",
    "df = pd.read_csv(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34119805-0771-45a9-ac20-79e23affb9f9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# promt, single term\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8eefaba-320e-4774-afc9-48e7af43786c",
   "metadata": {},
   "source": [
    "chatgpt baseline.\n",
    "\n",
    "A33\n",
    "A33 is a protein found on the surface of certain cells in the intestines and some types of cancer. Doctors can target it to help find or treat these cancers. It acts like a flag that helps identify where the cancer cells are.\n",
    "\n",
    "A6\n",
    "A6 is a small part of a protein that can help stop cancer from spreading. It works by blocking signals that tell cancer cells to move. Scientists are studying it to see if it can be used as a treatment.\n",
    "\n",
    "AAP\n",
    "AAP stands for American Academy of Pediatrics. It’s a group of doctors who focus on keeping children healthy. They create guidelines to help parents and doctors care for kids.\n",
    "\n",
    "AAT deficiency\n",
    "AAT deficiency is a condition where the body doesn't make enough of a protein that protects the lungs. Without it, the lungs can get damaged more easily, especially by smoking or pollution. It can also affect the liver in some people.\n",
    "\n",
    "Abarelix\n",
    "Abarelix is a medicine that lowers certain hormones in the body. It is mainly used to treat prostate cancer by stopping the cancer from growing. It works by blocking signals from the brain that tell the body to make these hormones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a445b4-0a2b-45d1-9892-42f775ef20b8",
   "metadata": {},
   "source": [
    "# prompt, paragraph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5214f6-5ac9-4799-bc18-cfeb96d6deac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modular Medical Text Explanation System\n",
    "# Returns clean dict with configurable steps: extract, generate, clean\n",
    "\n",
    "import re\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "\n",
    "class MedicalTextExplainer:\n",
    "    def __init__(self, model, tokenizer, meddict):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.meddict = meddict\n",
    "        \n",
    "        # Improved generation config for better outputs\n",
    "        self.generation_config = {\n",
    "            \"max_new_tokens\": 999,              # Increased for fuller explanations\n",
    "            \"temperature\": 0.3,\n",
    "            \"top_p\": 0.85,\n",
    "            \"repetition_penalty\": 1.15,\n",
    "            \"do_sample\": True,\n",
    "            \"pad_token_id\": tokenizer.eos_token_id,\n",
    "            \"eos_token_id\": tokenizer.eos_token_id,  # Ensure proper stopping\n",
    "            \"early_stopping\": True,              # Stop when EOS token is generated\n",
    "        }\n",
    "        \n",
    "        # Define available steps\n",
    "        self.available_steps = ['determine_audience', 'extract', 'generate', 'clean'] # 增加新步骤\n",
    "        self.step_functions = {\n",
    "            'determine_audience': self._step_determine_audience, # 增加新映射\n",
    "            'extract': self._step_extract_terms,\n",
    "            'generate': self._step_generate_explanation,\n",
    "            'clean': self._step_clean_response\n",
    "        }\n",
    "    def create_audience_prompt(self, original_text):\n",
    "            \"\"\"\n",
    "            Creates a simple, direct prompt to classify the audience.\n",
    "            \"\"\"\n",
    "            prompt = f\"\"\"\n",
    "        [INST] You are an expert medical text classifier. Read the following medical text and determine the appropriate audience for a summary letter.\n",
    "\n",
    "        **Medical Text:**\n",
    "        \"{original_text}\"\n",
    "\n",
    "        **Instructions:**\n",
    "        Based on the text, who is the audience for the explanation letter?\n",
    "        - If the text describes a patient recovering, Discharge Condition says much improved, or having a positive or follow ongoing treatment plan, the audience is the **patient**.\n",
    "        - If the text mentions \"died\", \"passed away,\" \"deceased,\" or describes a fatal outcome such \"comfort care\" \"hospice care\" \"pallliative care\", \"palliative extubate\", the audience is the **patient's family**.\n",
    "\n",
    "        Respond with a single word ONLY: **patient** or **family**.\n",
    "        [/INST]\n",
    "        \"\"\"\n",
    "            return prompt\n",
    "    \n",
    "\n",
    "    def _step_determine_audience(self, result):\n",
    "        \"\"\"Step 0: Determine the correct audience (patient or family).\"\"\"\n",
    "        text = result['original_text']\n",
    "        \n",
    "        # 1. 创建分类任务的Prompt\n",
    "        audience_prompt = self.create_audience_prompt(text)\n",
    "        \n",
    "        # 2. 调用模型进行分类 (使用一个轻量的生成配置)\n",
    "        try:\n",
    "            inputs = self.tokenizer(audience_prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "            input_length = inputs[\"input_ids\"].shape[1]\n",
    "            \n",
    "            # 使用一个非常短的生成配置，因为我们只需要一个词\n",
    "            audience_config = {\n",
    "                \"max_new_tokens\": 5,\n",
    "                \"temperature\": 0.01, # 温度极低，追求最确定的答案\n",
    "                \"do_sample\": False, # 不进行采样\n",
    "                \"pad_token_id\": self.tokenizer.eos_token_id\n",
    "            }\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(inputs[\"input_ids\"], **audience_config)\n",
    "            \n",
    "            response_tokens = outputs[0][input_length:]\n",
    "            raw_label = self.tokenizer.decode(response_tokens, skip_special_tokens=True).lower().strip()\n",
    "            \n",
    "            # 3. 清理和验证标签\n",
    "            if \"family\" in raw_label:\n",
    "                audience = \"family\"\n",
    "            else:\n",
    "                audience = \"patient\" # 默认为 patient\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ⚠️ Audience determination failed: {e}. Defaulting to 'patient'.\")\n",
    "            audience = \"patient\"\n",
    "\n",
    "        result['determined_audience'] = audience\n",
    "        print(f\"   🎯 Determined audience: '{audience}'\")\n",
    "        return result\n",
    "\n",
    "\n",
    "    def explain_medical_text(self, text, steps=['extract', 'generate', 'clean']):\n",
    "        \"\"\"\n",
    "        Modular main function that runs configurable steps.\n",
    "        \n",
    "        Args:\n",
    "            text (str): Input medical text\n",
    "            steps (list): List of steps to run. Options: ['extract', 'generate', 'clean']\n",
    "                         - 'extract': Extract medical terms from text\n",
    "                         - 'generate': Generate explanation from model  \n",
    "                         - 'clean': Clean the raw output\n",
    "        \n",
    "        Returns:\n",
    "            dict: Contains results from each step that was run\n",
    "        \"\"\"\n",
    "        \n",
    "        # Validate steps\n",
    "        invalid_steps = [step for step in steps if step not in self.available_steps]\n",
    "        if invalid_steps:\n",
    "            raise ValueError(f\"Invalid steps: {invalid_steps}. Available steps: {self.available_steps}\")\n",
    "        \n",
    "        # Initialize result with original text\n",
    "        result = {\n",
    "            'original_text': text,\n",
    "            'steps_run': steps.copy(),\n",
    "            'available_steps': self.available_steps.copy()\n",
    "        }\n",
    "        \n",
    "        # Run each step in sequence, passing result between steps\n",
    "        for step in steps:\n",
    "            print(f\"🔄 Running step: {step}\")\n",
    "            result = self.step_functions[step](result)\n",
    "        \n",
    "        print(f\"✅ Completed {len(steps)} steps: {steps}\")\n",
    "        return result\n",
    "    \n",
    "    def _step_extract_terms(self, result):\n",
    "        \"\"\"Step 1: Extract medical terms from text.\"\"\"\n",
    "        text = result['original_text']\n",
    "        found_terms = self.extract_medical_terms(text)\n",
    "        \n",
    "        result.update({\n",
    "            'found_terms': found_terms,\n",
    "            'terms_count': len(found_terms)\n",
    "        })\n",
    "        \n",
    "        print(f\"   📋 Found {len(found_terms)} medical terms\")\n",
    "        return result\n",
    "    \n",
    "    \n",
    "    def _step_generate_explanation(self, result):\n",
    "        \"\"\"Step 2: Generate explanation using model.\"\"\"\n",
    "        text = result['original_text']\n",
    "        if 'determined_audience' not in result:\n",
    "            raise ValueError(\"Audience not determined. Please run 'determine_audience' step before 'generate'.\")\n",
    "        \n",
    "        audience = result['determined_audience']\n",
    "        # Use found terms if extract step was run, otherwise extract them now\n",
    "        if 'found_terms' in result:\n",
    "            found_terms = result['found_terms']\n",
    "        else:\n",
    "            print(\"   ⚠️  Terms not extracted yet, extracting now...\")\n",
    "            found_terms = self.extract_medical_terms(text)\n",
    "            result['found_terms'] = found_terms\n",
    "        \n",
    "        # Create prompt and generate\n",
    "        prompt = self.create_prompt(text, found_terms, audience)\n",
    "        raw_output = self._generate_raw_response(prompt)\n",
    "        \n",
    "        result.update({\n",
    "            'prompt': prompt,\n",
    "            'raw_output': raw_output,\n",
    "            'prompt_length': len(prompt),\n",
    "        })\n",
    "        \n",
    "        print(f\"   🤖 Generated {len(raw_output)} character response\")\n",
    "        return result\n",
    "    \n",
    "    \n",
    "    def _step_clean_response(self, result):\n",
    "        \"\"\"Step 3: Clean the raw model output.\"\"\"\n",
    "        \n",
    "        # Check if we have raw output to clean\n",
    "        if 'raw_output' not in result:\n",
    "            raise ValueError(\"No raw_output found. Must run 'generate' step before 'clean' step.\")\n",
    "        \n",
    "        raw_output = result['raw_output']\n",
    "        \n",
    "        cleaned_output = self.model_clean_response(raw_output, result['determined_audience'])\n",
    "        \n",
    "        result.update({\n",
    "            'cleaned_output': cleaned_output,\n",
    "            'cleaned_length': len(cleaned_output)\n",
    "        })\n",
    "        \n",
    "        print(f\"   🧹 Cleaned to {len(cleaned_output)} characters\")\n",
    "        return result\n",
    "    \n",
    "    def extract_medical_terms(self, text):\n",
    "        \"\"\"\n",
    "        Enhanced medical term extraction to catch more terms from complex texts.\n",
    "        \"\"\"\n",
    "        found_terms = {}\n",
    "        \n",
    "        # Strategy 1: Single words (improved regex for medical terms)\n",
    "        words = re.findall(r'\\b[A-Za-z]+(?:[-\\'][A-Za-z]+)*\\b', text)\n",
    "        for word in words:\n",
    "            definition = self.find_term_in_dict(word)\n",
    "            if definition:\n",
    "                found_terms[word] = definition\n",
    "        \n",
    "        # Strategy 2: Multi-word terms (expanded range for complex medical phrases)\n",
    "        for n in range(2, 6):  # Increased from 5 to 6 for longer medical phrases\n",
    "            n_grams = self.get_n_grams(text, n)\n",
    "            for phrase in n_grams:\n",
    "                definition = self.find_term_in_dict(phrase)\n",
    "                if definition:\n",
    "                    found_terms[phrase] = definition\n",
    "        \n",
    "        # Strategy 3: Medical abbreviations (enhanced pattern)\n",
    "        abbreviations = re.findall(r'\\b[A-Z]{2,8}\\b', text)  # Increased from 6 to 8\n",
    "        for abbrev in abbreviations:\n",
    "            definition = self.find_term_in_dict(abbrev)\n",
    "            if definition:\n",
    "                found_terms[abbrev] = definition\n",
    "        \n",
    "        # Strategy 4: Medical procedures and conditions with specific patterns\n",
    "        medical_patterns = [\n",
    "            r'\\b\\w+oscopy\\b',          # bronchoscopy, endoscopy, etc.\n",
    "            r'\\b\\w+ectomy\\b',          # appendectomy, etc.\n",
    "            r'\\b\\w+itis\\b',            # bronchitis, arthritis, etc.\n",
    "            r'\\b\\w+osis\\b',            # fibrosis, stenosis, etc.\n",
    "            r'\\b\\w+emia\\b',            # anemia, septicemia, etc.\n",
    "            r'\\b\\w+pathy\\b',           # myopathy, neuropathy, etc.\n",
    "            r'\\b\\w+malacia\\b',         # tracheomalacia, etc.\n",
    "        ]\n",
    "        \n",
    "        for pattern in medical_patterns:\n",
    "            matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "            for match in matches:\n",
    "                definition = self.find_term_in_dict(match)\n",
    "                if definition:\n",
    "                    found_terms[match] = definition\n",
    "        \n",
    "        # Strategy 5: Medication names (often end in specific suffixes)\n",
    "        medication_patterns = [\n",
    "            r'\\b\\w+cillin\\b',          # penicillin, amoxicillin, etc.\n",
    "            r'\\b\\w+mycin\\b',           # streptomycin, etc.\n",
    "            r'\\b\\w+floxacin\\b',        # levofloxacin, ciprofloxacin, etc.\n",
    "            r'\\b\\w+sone\\b',            # prednisone, cortisone, etc.\n",
    "            r'\\b\\w+pam\\b',             # lorazepam, etc.\n",
    "        ]\n",
    "        \n",
    "        for pattern in medication_patterns:\n",
    "            matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "            for match in matches:\n",
    "                definition = self.find_term_in_dict(match)\n",
    "                if definition:\n",
    "                    found_terms[match] = definition\n",
    "        \n",
    "        return found_terms\n",
    "    \n",
    "    \n",
    "    def get_n_grams(self, text, n):\n",
    "        \"\"\"Generate n-grams from text.\"\"\"\n",
    "        words = re.findall(r'\\b[A-Za-z]+\\b', text.lower())\n",
    "        n_grams = []\n",
    "        for i in range(len(words) - n + 1):\n",
    "            phrase = ' '.join(words[i:i+n])\n",
    "            n_grams.append(phrase)\n",
    "        return n_grams\n",
    "    \n",
    "    def find_term_in_dict(self, term):\n",
    "        \"\"\"Find term in medical dictionary.\"\"\"\n",
    "        search_formats = [\n",
    "            term, term.lower(), term.upper(), term.title(), term.capitalize()\n",
    "        ]\n",
    "        \n",
    "        for search_term in search_formats:\n",
    "            if search_term in self.meddict:\n",
    "                return self.meddict[search_term]\n",
    "        \n",
    "        # Partial matching\n",
    "        for key in self.meddict.keys():\n",
    "            if key.lower() == term.lower():\n",
    "                return self.meddict[key]\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def create_prompt(self, original_text, found_terms, determined_audience):\n",
    "        \"\"\"\n",
    "        Create the exact prompt that will be sent to the model.\n",
    "        Enhanced with better instructions for patient-focused explanations.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Format found terms for inclusion in prompt\n",
    "        terms_section = \"\"\n",
    "        if found_terms:\n",
    "            terms_section = \"Found medical terms and their definitions:\\n\"\n",
    "            for term, definition in found_terms.items():\n",
    "                terms_section += f\"- {term}: {definition}\\n\"\n",
    "        else:\n",
    "            terms_section = \"No medical terms found in dictionary.\\n\"\n",
    "        \n",
    "        if determined_audience == 'family':\n",
    "            audience_instruction = \"The determined audience for this letter is the **patient's family**. You must address them directly as 'you' and refer to the patient in the third person (e.g., 'your loved one,' 'he/she').\"\n",
    "        else: # patient\n",
    "            audience_instruction = \"The determined audience for this letter is the **patient**. You must address them directly as 'you' throughout the entire letter.\"\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "    <s>[INST] You are a medical doctor and educator who writes clear, caring, and situationally appropriate medical explanations. Your persona is consistently that of a medical professional.\n",
    "\n",
    "    Original medical text:\n",
    "    \"{original_text}\"\n",
    "\n",
    "    {terms_section}\n",
    "\n",
    "    ### Your Task:\n",
    "    Your primary goal is to generate a single, complete, and polished letter.\n",
    "\n",
    "    1.  **Audience Directive:** {audience_instruction} **This is a strict rule.** Never mix the audience.\n",
    "    2.  Explain all medical content in simple, everyday language, defining every term clearly.\n",
    "    3.  Organize the letter logically and strive for genuine, varied language. Avoid overusing clichés.\n",
    "    4.  Maintain your professional persona as a doctor. Use \"we\" for the medical team. Never use personal terms like \"my mother.\"\n",
    "\n",
    "    ### Strict Output Formatting:\n",
    "    * Produce ONLY the letter text.\n",
    "    * NO meta-commentary, notes, or alternative versions.\n",
    "    * NO technical markers (`<s>`, `</INST>`, etc.).\n",
    "    * Start with a natural, professional salutation.\n",
    "\n",
    "    Please provide the patient-focused explanation now:\n",
    "\n",
    "    [/INST]\n",
    "    \"\"\"\n",
    "        return prompt\n",
    "    \n",
    "    def _generate_raw_response(self, prompt):\n",
    "        \"\"\"\n",
    "        Generate raw response from model (separated from cleaning).\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Tokenize input\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(self.model.device)\n",
    "            input_length = inputs[\"input_ids\"].shape[1]\n",
    "            \n",
    "            # Generate response\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    inputs[\"input_ids\"],\n",
    "                    attention_mask=inputs[\"attention_mask\"],\n",
    "                    **self.generation_config\n",
    "                )\n",
    "            \n",
    "            # Extract only the NEW tokens (response), not the input prompt\n",
    "            response_tokens = outputs[0][input_length:]  # Skip input tokens\n",
    "            raw_output = self.tokenizer.decode(response_tokens, skip_special_tokens=False)\n",
    "            \n",
    "            return raw_output\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error generating response: {str(e)}\"\n",
    "    \n",
    "    def model_clean_response(self, raw_response, aduience):\n",
    "        \"\"\"\n",
    "        Enhanced cleaning with better instructions for patient-focused, structured output.\n",
    "        \"\"\"\n",
    "        \n",
    "        # If response is very short or error, return as-is\n",
    "        if len(raw_response.strip()) < 10 or raw_response.startswith(\"❌\"):\n",
    "            return raw_response\n",
    "        \n",
    "        cleaning_prompt = f\"\"\"\n",
    "\n",
    "        \n",
    "[INST] You are a compassionate and professional editor. Your task is to revise the following draft of a letter from a doctor to a patient's family.\n",
    "\n",
    "**Draft Letter:**\n",
    "\"{raw_response}\"\n",
    "\n",
    "**Your Instructions:**\n",
    "1.  **Fix Audience Inconsistency:** Ensure the entire letter is addressed consistently to a single audience. In this case, the audience is the {aduience}.\n",
    "2.  **Improve Tone and Word Choice:** Revise the text to make it sound more genuine and less formulaic. Replace clichés (like \"despite our best efforts,\" \"with heavy hearts\") with more heartfelt and personal language where appropriate.\n",
    "3.  **Ensure Professional Persona:** Correct any text that breaks the doctor's persona (e.g., remove any personal anecdotes or incorrect points of view).\n",
    "4.  **Final Output:** Provide only the final, polished letter. Do not include any notes or explanations about your edits.\n",
    "\n",
    "Please provide the revised letter now:\n",
    "[/INST]\n",
    "\"\"\"\n",
    "        \n",
    "        try:\n",
    "            inputs = self.tokenizer(cleaning_prompt, return_tensors=\"pt\", truncation=True).to(self.model.device)\n",
    "            input_length = inputs[\"input_ids\"].shape[1]\n",
    "            \n",
    "            # Use enhanced generation config for cleaning\n",
    "            cleaning_config = {\n",
    "                \"max_new_tokens\": 600,  # Increased for more complete cleaning\n",
    "                \"temperature\": 0.1,\n",
    "                \"top_p\": 0.9,\n",
    "                \"repetition_penalty\": 1.1,\n",
    "                \"do_sample\": True,\n",
    "                \"pad_token_id\": self.tokenizer.eos_token_id,\n",
    "                \"eos_token_id\": self.tokenizer.eos_token_id,\n",
    "                \"early_stopping\": True,\n",
    "            }\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    inputs[\"input_ids\"],\n",
    "                    attention_mask=inputs[\"attention_mask\"],\n",
    "                    **cleaning_config\n",
    "                )\n",
    "            \n",
    "            # Extract only the cleaned response\n",
    "            response_tokens = outputs[0][input_length:]\n",
    "            cleaned = self.tokenizer.decode(response_tokens, skip_special_tokens=True)\n",
    "            \n",
    "\n",
    "            \n",
    "            return cleaned\n",
    "            \n",
    "        except Exception as e:\n",
    "     \n",
    "            return raw_response\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "330cd4de-e256-45db-94ae-348effe10813",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = MedicalTextExplainer(model, tokenizer, meddict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2ebb4233-0672-4c02-8932-e9a0382e3647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Admission Date:  [**2119-5-4**]              Discharge Date:   [**2119-5-25**]\\n\\n\\nService: CARDIOTHORACIC\\n\\nAllergies:\\nAmlodipine\\n\\nAttending:[**Last Name (NamePattern1) 1561**]\\nChief Complaint:\\n81 yo F smoker w/ COPD, severe TBM, s/p tracheobronchoplasty [**5-5**]\\ns/p perc trach [**5-13**]\\n\\nMajor Surgical or Invasive Procedure:\\nbronchoscopy 3/31,4/2,3,[**6-12**], [**5-17**], [**5-19**]\\ns/p trachealplasty [**5-5**]\\npercutaneous tracheostomy [**5-13**] after failed extubation\\ndown size trach on [**5-25**] to size 6 cuffless\\n\\n\\nHistory of Present Illness:\\nThis 81 year old woman has a history of COPD. Over the past five\\n\\nyears she has had progressive difficulties with her breathing.\\nIn\\n[**2118-6-4**] she was admitted to [**Hospital1 18**] for respiratory failure\\ndue\\nto a COPD exacerbation. Due to persistent hypoxemia, she\\nrequired\\nintubation and a eventual bronchoscopy on [**2118-6-9**] revealed marked\\n\\nnarrowing of the airways on expiration consistent with\\ntracheomalacia.\\nShe subsequently underwent placement of two\\nsilicone stents, one in the left main stem and one in the\\ntrachea. During the admission the patient had complaints of\\nchest\\npain and ruled out for an MI. She was subsequently discharged to\\n\\n[**Hospital1 **] for physical and pulmonary rehab. Repeat bronchoscopy\\non\\n[**2118-8-1**] revealed granulation tissue at the distal right lateral\\nwall of the tracheal stent. There was significant malacia of the\\n\\nperipheral and central airways with complete collapse of the\\nairways on coughing and forced expiration. Small nodules were\\nalso noted on the vocal cords. She has noticed improvement in\\nher\\nrespiratory status, but most recently has been in discussion\\nwith Dr. [**First Name4 (NamePattern1) 951**] [**Last Name (NamePattern1) 952**] regarding possible tracheobronchial plasty\\n\\nwith mesh. Tracheal stents d/c [**2119-4-19**] in anticipation of\\nsurgery.\\nIn terms of symptoms, she describes many years of intermittent\\nchest pain that she describes as left sided and occurring at any\\n\\ntime. Currently, she notices it about three times a week, and\\nstates that it seems to resolve after three nitroglycerin.\\nShe currently is dependent on oxygen and wears 1.5-2 liters\\naround the clock. She has frequent coughing and brings up \"dark\\nsputum\".\\n\\n\\n\\nPast Medical History:\\nCOPD flare [**6-7**] s/p intubation, s/p distal tracheal to Left Main\\nStem stents placed [**2118-6-9**]. Stents d/c\\'d [**2119-4-19**], CAD w/ atypical\\nangina (LAD 30%, RCA 30%, EF 63%), ^chol, hypothyroidism, htn,\\nhiatal hernia, lacunar CVA, s/p ped struck -> head injury & rib\\nfx, depression\\nPMH:\\nCOPD, s/p admit [**6-7**] for exacerbation requiring intubation\\ntracheobronchomalacia, s/p bronchial stenting\\nLarge hiatal hernia\\nLacunar CVA\\nHypothyroidism by records in CCC, although patient denies and is\\n\\nnot taking any medication\\nDepression\\nMVA, s/p head injury approximately 10 years ago\\nHypertension\\nHysterectomy\\n\\n\\nSocial History:\\nSocial History: The patient is married and worked as a clinical\\npsychologist. Her husband is a pediatric neurologist at\\n[**Hospital3 **]. They have several children, one of which is\\n\\na nurse.\\n\\n\\nFamily History:\\nFamily History: (+) FHx CAD; Father with an MI in his 40\\'s, died\\n\\nof a CVA at age 59\\n\\n\\nPhysical Exam:\\nAdmit H+P\\nGeneral-lovely 81 yr old feamle in NAD.\\nNeuro- intermittently anxious, MAE, PERRLA, L eye ptosis,\\nsymetrical smile, gossly intact.\\nHEENT-PERRLA, sclera anicteric, pharynx- no exud or erythema\\nResp-clear upper, diffuse ronchi, intermit exp wheezes\\nCor- RRR, No M, R, G\\nAbd- soft, NT, ND, no masses. Slight protrusion at area of\\nhiatal hernia\\nExt- no edema or clubbing\\n\\nBrief Hospital Course:\\n82 y/o female admitted [**2119-5-4**] for consideration of\\ntracheoplasty.\\nBronchoscopy done [**5-4**] confirming severe TBM. Underwent\\ntracheoplasty [**5-5**], complicated by resp failure d/t mucous\\nplugging, hypoxia requiring re-intubation resulting in prolonged\\nICU and hospital course. Also developed right upper extrem DVT\\nfrom mid line.\\n\\nPain- Epidural accidently d/c\\'d POD#1, pt briefly used dilaudid\\nPCA intermittently w/ fair pain control. Pt required\\nre-intubation for resp failure d/t secretions and PCA d/c at\\nthat time. Propofol for sedation while intubated. Sedation d/c\\'d\\n[**5-12**] for weaning trial w/ ETT- failed trial. Trach [**5-13**]-weaning\\nefforts as below. Minimal c/o pain since [**5-13**]. Presently pain\\nfree.\\n\\nNeuro- Initially intact- post op aggitation, inhibiting weaning\\nefforts [**5-16**]. Psych eval [**5-18**]-Started on zyprexa and ativan w/\\nimprovement in anxiety. Presently A+Ox3- cooperative and lovely.\\n\\nResp- Extubated POD#2 then required re-intub  [**5-7**] for hypoxia\\nd/t poor cough and mucous plugging. SIMV/PS alt w/CMV at night\\nx4-5d, with CPAP attempts during day.\\nBronchoscopy qd [**Date range (1) 1813**] for secretion management. Bronch [**5-9**]\\nrevealed swollen epiglottis, bronch [**5-10**] - good leak w/ ETT cuff\\ndeflated. Bronch [**5-13**] for eval/trach placement. Last bronch [**5-19**]\\nw/ min secretions present, sputum sent.\\n[**5-13**] perc trach done(#8 Portex- cuffed low pressure maintained to\\npreserve tracheoplasty site). [**5-13**] CPAP15/peep5 initiated post\\ntrach placement. Weaning ongoing.  [**Date range (1) 1814**]- Aggressive weaning\\nw/ increasing episodes of CPAP, progressing to Trach Mask.\\n[**2033-5-20**]-Trach mask overnight w/ no episodes of SOB, or\\nhemodynamic instability. Trach changed to #6 portex- capped and\\n[**Last Name (un) 1815**] well x48hrs on 2LNP. productive cough. Aggressive PT as\\nwell w/ OOB > chair [**Hospital1 **]-tid to total 4-6hr qd. Ambulation\\n~100-120 ft [**5-22**] w/ PT assist.\\n\\nID- Vancomycin started post-op for graft prophylaxis. Fever\\nspike [**2119-5-8**] w/ BAl & sputum sent> + MRSA. Vanco cont to [**4-7**]\\nweeks post trachealplasty. Fever low grade [**5-12**], [**5-15**]> cultured-\\nno new results. [**5-19**]- WBC 20.8 .\\n\\nCardiac-Hypertension controlled w/ hydralazine IV, then d/c and\\ncont controlled. HR 65-75 NSR. Avoiding B Blockers. Lasix 20mg\\nIV qd.\\n[**5-15**]- RUE redness and swelling at site of midline, RUE DVT by\\nultrasound, midline d/c; heparin gtt started and therapeutic\\nrange monitored. [**5-22**]  changed to Lovenox sq [**Hospital1 **]. Coags in good\\ncontrol [**5-23**] (48.2/13.8/1.2)\\nAccess- R midline placed [**2119-5-9**] for access- clotted [**2119-5-15**] and\\nd/c\\'d.  RUE redness and swelling and DVT via ultrasound. [**5-15**]- L\\nbrachial PICC line placed- TPN resumed.\\n\\nGI-Large hiatal hernia- unable to place enteral feeding tube at\\nbedside or underfluoro. Re-attempt [**5-17**] by EGD doboff tube\\nplaced distal esophagus, dislodged in 12hours and removed.\\n\\nNutrition- PPN/TPN initiated [**2119-5-8**]- [**2119-5-25**]. PICC placed\\n[**2119-5-15**]. Speech and Swallow eval [**5-22**]- rec change trach form #8\\nto #6 Portex to allow improved epiglotis and oropharyngeal\\nmovement to assist w/ swallowing. Then re-eval.  Trach changed\\n[**5-23**] to #6 cuffless portex trach. Passed repeat swallow eval and\\n[**Last Name (un) 1815**] diet of regular solids w/ thin liquids- CHIN TUCK to\\nswallow thin liquids. Give meds whole w/ apple sauce. WOULD\\nRECOMMEND repeat video swallow eval in [**8-17**] days to possibly\\neliminate chin tuck- see page 3 referral.\\n\\nEndo- Hypothyroid, maintained on levoxyl.\\n\\nMuscu/Skel- OOB> chair 4-6hours/day, PT consulting.\\n\\n\\nMedications on Admission:\\nadvair 250/50\", atrovent, imdur 60\\', lasix 40\\', lexapro 20\\',\\nlipitor 10\\', prilosec 20\\', mucinex 600\", synthroid 75\\', detrol\\nLA 4\\', ambien 5\\', trazadone 75\\', melatonin prn\\n\\nDischarge Medications:\\n1. Albuterol Sulfate 0.083 % Solution Sig: One (1)  Inhalation\\nQ6H (every 6 hours) as needed for wheezing.\\n2. Ipratropium Bromide 0.02 % Solution Sig: One (1)  Inhalation\\nQ6H (every 6 hours) as needed for wheezing.\\n3. Fluticasone-Salmeterol 250-50 mcg/Dose Disk with Device Sig:\\nOne (1) Disk with Device Inhalation [**Hospital1 **] (2 times a day).\\n4. Albuterol 90 mcg/Actuation Aerosol Sig: 1-2 Puffs Inhalation\\nQ6H (every 6 hours) as needed.\\n5. Ipratropium Bromide 18 mcg/Actuation Aerosol Sig: Two (2)\\nPuff Inhalation QID (4 times a day).\\n6. Acetaminophen 325 mg Tablet Sig: 1-2 Tablets PO Q4-6H (every\\n4 to 6 hours) as needed.\\n7. Sodium Chloride 0.65 % Aerosol, Spray Sig: [**2-5**] Sprays Nasal\\nQID (4 times a day) as needed.\\n8. Camphor-Menthol 0.5-0.5 % Lotion Sig: One (1) Appl Topical\\nTID (3 times a day) as needed.\\n9. Enoxaparin Sodium 60 mg/0.6mL Syringe Sig: One (1)\\nSubcutaneous Q12H (every 12 hours).\\n10. Trazodone HCl 50 mg Tablet Sig: 1.5 Tablets PO HS (at\\nbedtime) as needed.\\n11. Escitalopram Oxalate 10 mg Tablet Sig: Two (2) Tablet PO\\nDAILY (Daily).\\n12. Nystatin 100,000 unit/g Cream Sig: One (1) Appl Topical  [**Hospital1 **]\\n(2 times a day).\\n13. Pantoprazole Sodium 40 mg Tablet, Delayed Release (E.C.)\\nSig: One (1) Tablet, Delayed Release (E.C.) PO Q24H (every 24\\nhours).\\n14. Tolterodine Tartrate 2 mg Tablet Sig: One (1) Tablet PO BID\\n(2 times a day).\\n15. Levothyroxine Sodium 75 mcg Tablet Sig: One (1) Tablet PO\\nDAILY (Daily).\\n16. Heparin Lock Flush (Porcine) 100 unit/mL Syringe Sig: One\\n(1) ML Intravenous  DAILY (Daily) as needed.\\n\\n\\nDischarge Disposition:\\nExtended Care\\n\\nFacility:\\n[**Hospital3 7**] & Rehab Center - [**Hospital1 8**]\\n\\nDischarge Diagnosis:\\nCOPD, Coronary Artery Disease/atypical angina (LAD 30%, RCA 30%,\\nEF 63%), hypercholesterolemia, hypothyroidism, Hypertension,\\nhiatal hernia, Cerebral Vascular Accident,s/p Motor Vehicle\\nColision-> head injury & rib fracture.\\nTBM- s/p tracheoplasty.\\n\\n\\nDischarge Condition:\\ngood\\n\\nDischarge Instructions:\\nplease update Dr.[**Name (NI) 1816**] [**Telephone/Fax (1) 170**] office for:  fever,\\nshortness of breath, chest pain , productive cough or if you\\nhave any questions or concerns.\\n\\n\\nCompleted by:[**2119-5-25**]'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[1].TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5bebc6fb-06a4-4ec8-b5ae-81f3690ed848",
   "metadata": {},
   "outputs": [],
   "source": [
    "allres=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1e226643-1e3e-4b3b-9694-f678d6534190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ROW_ID', 'SUBJECT_ID', 'HADM_ID', 'CHARTDATE', 'CATEGORY',\n",
       "       'DESCRIPTION', 'TEXT'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "55cb93ac-50ff-41b2-8495-cfede50d5853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Running step: determine_audience\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yc/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.01` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/yc/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   🎯 Determined audience: 'patient'\n",
      "🔄 Running step: extract\n",
      "   📋 Found 131 medical terms\n",
      "🔄 Running step: generate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yc/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:638: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   🤖 Generated 3069 character response\n",
      "🔄 Running step: clean\n",
      "   🧹 Cleaned to 3054 characters\n",
      "✅ Completed 4 steps: ['determine_audience', 'extract', 'generate', 'clean']\n",
      "🔄 Running step: determine_audience\n",
      "   🎯 Determined audience: 'patient'\n",
      "🔄 Running step: extract\n",
      "   📋 Found 111 medical terms\n",
      "🔄 Running step: generate\n",
      "   🤖 Generated 3566 character response\n",
      "🔄 Running step: clean\n",
      "   🧹 Cleaned to 2874 characters\n",
      "✅ Completed 4 steps: ['determine_audience', 'extract', 'generate', 'clean']\n",
      "🔄 Running step: determine_audience\n",
      "   🎯 Determined audience: 'patient'\n",
      "🔄 Running step: extract\n",
      "   📋 Found 153 medical terms\n",
      "🔄 Running step: generate\n",
      "   🤖 Generated 4641 character response\n",
      "🔄 Running step: clean\n",
      "   🧹 Cleaned to 3157 characters\n",
      "✅ Completed 4 steps: ['determine_audience', 'extract', 'generate', 'clean']\n",
      "🔄 Running step: determine_audience\n",
      "   🎯 Determined audience: 'family'\n",
      "🔄 Running step: extract\n",
      "   📋 Found 167 medical terms\n",
      "🔄 Running step: generate\n",
      "   🤖 Generated 2897 character response\n",
      "🔄 Running step: clean\n",
      "   🧹 Cleaned to 2338 characters\n",
      "✅ Completed 4 steps: ['determine_audience', 'extract', 'generate', 'clean']\n",
      "🔄 Running step: determine_audience\n",
      "   🎯 Determined audience: 'patient'\n",
      "🔄 Running step: extract\n",
      "   📋 Found 82 medical terms\n",
      "🔄 Running step: generate\n",
      "   🤖 Generated 2980 character response\n",
      "🔄 Running step: clean\n",
      "   🧹 Cleaned to 2775 characters\n",
      "✅ Completed 4 steps: ['determine_audience', 'extract', 'generate', 'clean']\n",
      "🔄 Running step: determine_audience\n",
      "   🎯 Determined audience: 'family'\n",
      "🔄 Running step: extract\n",
      "   📋 Found 64 medical terms\n",
      "🔄 Running step: generate\n",
      "   🤖 Generated 2407 character response\n",
      "🔄 Running step: clean\n",
      "   🧹 Cleaned to 2207 characters\n",
      "✅ Completed 4 steps: ['determine_audience', 'extract', 'generate', 'clean']\n",
      "🔄 Running step: determine_audience\n",
      "   🎯 Determined audience: 'patient'\n",
      "🔄 Running step: extract\n",
      "   📋 Found 102 medical terms\n",
      "🔄 Running step: generate\n",
      "   🤖 Generated 4060 character response\n",
      "🔄 Running step: clean\n",
      "   🧹 Cleaned to 3091 characters\n",
      "✅ Completed 4 steps: ['determine_audience', 'extract', 'generate', 'clean']\n",
      "🔄 Running step: determine_audience\n",
      "   🎯 Determined audience: 'patient'\n",
      "🔄 Running step: extract\n",
      "   📋 Found 72 medical terms\n",
      "🔄 Running step: generate\n",
      "   🤖 Generated 2374 character response\n",
      "🔄 Running step: clean\n",
      "   🧹 Cleaned to 2272 characters\n",
      "✅ Completed 4 steps: ['determine_audience', 'extract', 'generate', 'clean']\n",
      "🔄 Running step: determine_audience\n",
      "   🎯 Determined audience: 'patient'\n",
      "🔄 Running step: extract\n",
      "   📋 Found 86 medical terms\n",
      "🔄 Running step: generate\n",
      "   🤖 Generated 4236 character response\n",
      "🔄 Running step: clean\n",
      "   🧹 Cleaned to 3097 characters\n",
      "✅ Completed 4 steps: ['determine_audience', 'extract', 'generate', 'clean']\n"
     ]
    }
   ],
   "source": [
    "for index, row in df.iterrows():\n",
    "    text=row['TEXT'] \n",
    "    result = explainer.explain_medical_text(text, steps=['determine_audience','extract', 'generate','clean'])\n",
    "    allres.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3d1e92aa-12c6-4c24-82c1-2398837a9848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['original_text', 'steps_run', 'available_steps', 'determined_audience',\n",
       "       'found_terms', 'terms_count', 'prompt', 'raw_output', 'prompt_length',\n",
       "       'cleaned_output', 'cleaned_length'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.DataFrame(allres)\n",
    "results_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6d09b893-bbd1-4072-a282-06f7929ea49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved to 'output_4.csv'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "base_filename = 'output'\n",
    "extension = '.csv'\n",
    "output_filename = base_filename + extension\n",
    "counter = 1\n",
    "\n",
    "# Check if the file exists and find a new name if it does\n",
    "while os.path.exists(output_filename):\n",
    "    output_filename = f\"{base_filename}_{counter}{extension}\"\n",
    "    counter += 1\n",
    "\n",
    "# Save the DataFrame to the new, unique filename\n",
    "results_df.to_csv(output_filename, index=False)\n",
    "\n",
    "print(f\"DataFrame saved to '{output_filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fe4b3657-49c7-487b-a5e9-b855ea79f616",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('output.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
