{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4dd4555-473d-48cd-bc86-60a4c41da0a8",
   "metadata": {},
   "source": [
    "## load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "611f29b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b2c708e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yc/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "with open(\"hf.token\", \"r\") as f:\n",
    "    hftoken = f.read().strip()  \n",
    "\n",
    "import os\n",
    "cache_dir = \"/mnt/c/Users/yc/.cache/huggingface\"\n",
    "os.environ['HF_HOME'] = cache_dir\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(token=hftoken)  # Move token to environment variable\n",
    "\n",
    "from ult import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e1c388-57f9-41b0-87d1-25a9ad884430",
   "metadata": {},
   "source": [
    "## data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf6281f4-7ebb-48fd-a08b-4271349ededf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coral_idx</th>\n",
       "      <th>Sex</th>\n",
       "      <th>UCSFDerivedRaceEthnicity_X</th>\n",
       "      <th>BirthDate</th>\n",
       "      <th>note_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>140</td>\n",
       "      <td>Female</td>\n",
       "      <td>Native Hawaiian or Other Pacific Islander</td>\n",
       "      <td>1964-03-25</td>\n",
       "      <td>Medical Oncology Consult Note    Patient Name:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>141</td>\n",
       "      <td>Female</td>\n",
       "      <td>Native Hawaiian or Other Pacific Islander</td>\n",
       "      <td>1975-03-29</td>\n",
       "      <td>This is a shared visit for services provided b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>142</td>\n",
       "      <td>Female</td>\n",
       "      <td>Native Hawaiian or Other Pacific Islander</td>\n",
       "      <td>1967-10-06</td>\n",
       "      <td>Medical Oncology Consult Note  Video Consult  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>143</td>\n",
       "      <td>Female</td>\n",
       "      <td>Native Hawaiian or Other Pacific Islander</td>\n",
       "      <td>1943-12-23</td>\n",
       "      <td>This is an independent visit      SUBJECTIVE  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>144</td>\n",
       "      <td>Female</td>\n",
       "      <td>Native Hawaiian or Other Pacific Islander</td>\n",
       "      <td>1987-06-23</td>\n",
       "      <td>***** ***** Note  Patient Name: ***** *****   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>145</td>\n",
       "      <td>Female</td>\n",
       "      <td>Native American or Alaska Native</td>\n",
       "      <td>1985-02-26</td>\n",
       "      <td>HPI:  ***** ***** is a 34 y.o. female with E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>146</td>\n",
       "      <td>Female</td>\n",
       "      <td>Native Hawaiian or Other Pacific Islander</td>\n",
       "      <td>1964-03-15</td>\n",
       "      <td>*****  ***** with MBC ***** 2008    CC  2nd op...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>147</td>\n",
       "      <td>Female</td>\n",
       "      <td>Native Hawaiian or Other Pacific Islander</td>\n",
       "      <td>1990-03-26</td>\n",
       "      <td>ID: ***** ***** ***** is a 29 y.o. premenopaus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>148</td>\n",
       "      <td>Female</td>\n",
       "      <td>Native American or Alaska Native</td>\n",
       "      <td>1957-05-23</td>\n",
       "      <td>***** ***** Note  Patient Name: ***** ***** **...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>149</td>\n",
       "      <td>Female</td>\n",
       "      <td>Native American or Alaska Native</td>\n",
       "      <td>1954-08-11</td>\n",
       "      <td>***** ***** Note  Patient Name: ***** *****   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   coral_idx     Sex                 UCSFDerivedRaceEthnicity_X   BirthDate  \\\n",
       "0        140  Female  Native Hawaiian or Other Pacific Islander  1964-03-25   \n",
       "1        141  Female  Native Hawaiian or Other Pacific Islander  1975-03-29   \n",
       "2        142  Female  Native Hawaiian or Other Pacific Islander  1967-10-06   \n",
       "3        143  Female  Native Hawaiian or Other Pacific Islander  1943-12-23   \n",
       "4        144  Female  Native Hawaiian or Other Pacific Islander  1987-06-23   \n",
       "5        145  Female           Native American or Alaska Native  1985-02-26   \n",
       "6        146  Female  Native Hawaiian or Other Pacific Islander  1964-03-15   \n",
       "7        147  Female  Native Hawaiian or Other Pacific Islander  1990-03-26   \n",
       "8        148  Female           Native American or Alaska Native  1957-05-23   \n",
       "9        149  Female           Native American or Alaska Native  1954-08-11   \n",
       "\n",
       "                                           note_text  \n",
       "0  Medical Oncology Consult Note    Patient Name:...  \n",
       "1  This is a shared visit for services provided b...  \n",
       "2  Medical Oncology Consult Note  Video Consult  ...  \n",
       "3  This is an independent visit      SUBJECTIVE  ...  \n",
       "4  ***** ***** Note  Patient Name: ***** *****   ...  \n",
       "5    HPI:  ***** ***** is a 34 y.o. female with E...  \n",
       "6  *****  ***** with MBC ***** 2008    CC  2nd op...  \n",
       "7  ID: ***** ***** ***** is a 29 y.o. premenopaus...  \n",
       "8  ***** ***** Note  Patient Name: ***** ***** **...  \n",
       "9  ***** ***** Note  Patient Name: ***** *****   ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_file_path = 'data/formaldef.txt'\n",
    "formaldic = txt_to_dict(txt_file_path)\n",
    "len(formaldic)\n",
    "\n",
    "meddict={}\n",
    "for k,v in formaldic.items():\n",
    "    meddict[k.split('Listen to pronunciation')[0].split('(')[0]]=v\n",
    "filename= 'data/filtered_medical_dictionary.csv'\n",
    "eighth_grade_words=set()\n",
    "with open(filename, 'r', encoding='utf-8') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    next(reader)  # Skip the header row\n",
    "    for row in reader:\n",
    "        if row:  # Make sure row is not empty\n",
    "            eighth_grade_words.add(row[0])  # Add the word (first column)\n",
    "filtered_meddict = {word: explanation for word, explanation in meddict.items() \n",
    "                   if word in eighth_grade_words}\n",
    "meddict=filtered_meddict\n",
    "# load data\n",
    "df = pd.read_csv('data/CORAL/coral-expert-curated-medical-oncology-reports-to-advance-language-model-inference-1.0/coral/unannotated/data/breastca_unannotated.csv')\n",
    "df=df.iloc[list(range(0,10))] \n",
    "# df = df.sample(1, random_state=42)\n",
    "# df=df.iloc[[44,45,53,70,83]+list(range(0,10))] \n",
    "# df=df.iloc[[44,45,53,70,83]] \n",
    "# df=df.iloc[[70,]] \n",
    "# test_note=df.iloc[2]['note_text']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d74a59c-b625-4944-8a79-aea69bc6cd8a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [01:34<00:00, 23.56s/it]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "device_map = {\"\": 0}\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=device_map,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfa3a9d",
   "metadata": {},
   "source": [
    "# keypoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03c3bce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "extraction_prompts = {\n",
    "\n",
    "\"Reason_for_Visit\": \"\"\"\n",
    "TASK: Extract 'Reason for Visit'.\n",
    "Respond *only* with a JSON object using this exact schema:\n",
    "{\n",
    "\"Patient type\": \"either New patient or follow up\"\n",
    "\"second opinion\": \"whether the visit is consultation/second opinion or not\",\n",
    "\"in-person\":\" either Televisit or in-person. (note, video consult, televisit, telehealth are the same thing\",\n",
    "\"summary\": \"A brief summary of the reason for visit.\"\n",
    "}\n",
    "\"\"\",\n",
    "\n",
    "\n",
    "\"What_We_Found\": \"\"\"\n",
    "TASK: Extract 'What We Found'. \n",
    "Respond *only* with a JSON object using this exact schema:\n",
    "{\n",
    "\"Type_of_Cancer\": \"list the type of cancer\"\n",
    "\"Stage_of_Cancer\": \"list the stage if it is written in the note\",\n",
    "\"Distant Metastasis\": \"if there is distant metastasis (met).\" \"Yes, to where; No, local, not sure, need more evidence such as imaging.\",\n",
    "\"Metastasis\": \"if there is met, Yes (to where), No, or Not sure\",\n",
    "\"lab_summary\": \"Summary of key lab results.\",\n",
    "\"findings\": \"Summary of new findings or disease status.\"\n",
    "}\n",
    "\"\"\",\n",
    "\n",
    "\n",
    "\"Treatment_Summary\": \"\"\"\n",
    "TASK: Extract 'Treatment Summary'. \n",
    "Respond *only* with a JSON object using this exact schema:\n",
    "{\n",
    "\"current_meds\": \"List of current oncologic medications or regimens.\",\n",
    "\"recent_changes\": \"Any holds, dose reductions, or switches.\",\n",
    "\"supportive_meds\": \"List of supportive medications.\"\n",
    "}\n",
    "\"\"\",\n",
    "\n",
    "\n",
    "\"Goals of care\": \"\"\"\n",
    "TASK: Extract 'What We Discussed / Decided'.\n",
    "Respond *only* with a JSON object using this exact schema:\n",
    "{\n",
    "\"goals_of_treatment\": \"eg, cancer is not curable, but it's treatable and the goal is to extend the duration and maintain the quality of life\",\n",
    "\"response_assessment\": \"How the cancer is responding to the treatment, not responding or responding.\",\n",
    "}\n",
    "\"\"\",\n",
    "}\n",
    "\n",
    "plan_extraction_prompts = {\n",
    "\n",
    "\"Medication_Plan_chatgpt\": \"\"\"\n",
    "TASK: Extract the 'Medication Plan' from the given 'Assessment/Plan' section.\n",
    "Include all current and future medication plans for both cancer therapy and supportive treatment. Cancer treatment could be one or many in chemotherapy, hormonal therapy, bone therapy, radiotherapy (eg. rad onc, xrt). Supportive treatment could be one or many in bowel regimen, pain medication, psychiatry medication, neuropathy or any blood transfusion plan. \n",
    "\n",
    "Include whether a medication is being started now (e.g,“will start”, “Rx sent”, “starting today” ), plan or discuss in the future after certain condition (e.g., “plan to start after radiation”, “discussed addition of…”), continue or maintained(“continue”), stop or change.\n",
    "\n",
    "Include an alterative, second-line or clinical trials options if dicussed. \n",
    "Include an 'other treatment' section for any other medications that only briefly mentioned in the plan section.\n",
    "Respond *only* with a JSON object using this exact schema:\n",
    "{\n",
    "    \"The medication/treatment, one of chemotherapy, hormonal therapy, bone therapy, radiotherapy\": \n",
    "{\"summary\":\"the summary of this type of medication, including start/stop/cotinue if applicable\",\n",
    "\"Short term side_effects_discussed\": \"short term Side effects of this particular medications.\",\n",
    "\"Long term side_effects_discussed\": \"long term Side effects of this particular medications..\"}\n",
    "}\n",
    "\"\"\",\n",
    "\n",
    "\n",
    "\"Medication_Plan\": \"\"\"\n",
    "TASK: Extract the 'Medication Plan' from the given 'Assessment/Plan' section.\n",
    "Include future medication plans, changes to current meds (start, stop, continue), supportive meds, bowel regimen, and any blood transfusion plan. Do not include past medications.\n",
    "Respond *only* with a JSON object using this exact schema:\n",
    "{\n",
    "    \"medication_plan\": \"A summary of the complete medication plan.\"\n",
    "}\n",
    "\"\"\",\n",
    "\n",
    "\n",
    "\"Therapy plan\": \"\"\"\n",
    "TASK: Extract the 'therapy Plan' from the given 'Assessment/Plan' section.\n",
    "Include chemotherapy, radiotherapy, hormonal therapy, and bone-therapy plans. Include future plans, changes to current therapy (start, stop, continue). Do not include past therapies.\n",
    "Respond *only* with a JSON object using this exact schema:\n",
    "{\n",
    "    \"therapy_plan\": \"A summary of the complete therapy plan.\"\n",
    "}\n",
    "\"\"\",\n",
    "\n",
    "\"radiotherapy plan\": \"\"\"\n",
    "TASK: Extract the 'radiotherapy Plan' from the given 'Assessment/Plan' section.\n",
    "For radiotherapy, include ANY statement that indicates \n",
    "it is being considered, recommended, or may be used, even if no explicit\n",
    "start/continue/plan keywords are present.\n",
    "\"\"\",\n",
    "\n",
    "\n",
    "\"Procedure_Plan\": \"\"\"\n",
    "TASK: Extract the 'Procedure Plan' from the given 'Assessment/Plan' section.\n",
    "Include future procedures including surgery, radiation therapy, or interventional procedures such as biopsy, lumbar puncture, or Chemo Port Insertion. Do not include past procedures.\n",
    "Do not include imaging plan, lab plan, or medication plan.\n",
    "Respond *only* with a JSON object using this exact schema:\n",
    "{\n",
    "    \"procedure_plan\": \"A summary of any planned procedures.\"\n",
    "}\n",
    "\"\"\",\n",
    "\n",
    "\n",
    "\"Imaging Plan\": \"\"\"\n",
    "TASK: Extract the 'Imaging Plan' from the given 'Assessment/Plan' section.\n",
    "Include all future imaging like CT, MRI, PET/CT, ultrasound,  DEXA scans, including timing and rationale if mentioned.\n",
    "Do not include any procedure plan, lab plan, or medication plan. Do not include past Imaging.\n",
    "Respond *only* with a JSON object using this exact schema:\n",
    "{\n",
    "    \"imaging_plan\": \"A summary of any planned procedures.\"\n",
    "}\n",
    "\"\"\",\n",
    "\n",
    "\n",
    "\"Lab Plan\": \"\"\"\n",
    "TASK: Extract the 'lab Plan' from the given 'Assessment/Plan' section.\n",
    "Include future labs like CBC, CMP, tumor markers, coagulation profile. Specify frequency and rationale if mentioned.\n",
    "Do not include any procedure plan, medication plan, or imagining plan. Do not include past labs.\n",
    "Respond *only* with a JSON object using this exact schema:\n",
    "{\n",
    "    \"lab_plan\": \"A summary of the future lab plan.\"\n",
    "}\n",
    "\"\"\",\n",
    "\n",
    "\n",
    "\"Genetic_Testing_Plan\": \"\"\"\n",
    "TASK: Extract the 'Genetic Testing Plan' from the given 'Assessment/Plan' section.\n",
    "\n",
    "1. Scope of section:\n",
    "- First, find the 'Assessment/Plan' section of the note (usually the final section).\n",
    "- Only look for plans that are clearly about FUTURE genetic or molecular TESTING.\n",
    "\n",
    "2. What COUNTS as 'genetic or molecular testing':\n",
    "- Diagnostic, prognostic, or predictive laboratory assays, such as:\n",
    "  - Tumor genomic sequencing / NGS / panel testing\n",
    "  - Germline genetic panels (e.g., hereditary cancer panel, BRCA testing)\n",
    "  - Liquid biopsy / circulating tumor DNA (ctDNA) tests\n",
    "  - Specific biomarker tests (e.g., PD-L1 testing, MSI testing, EGFR mutation testing)\n",
    "  - Any plan to \"send\", \"order\", \"check\", or \"obtain\" a genetic, genomic, or molecular TEST\n",
    "\n",
    "3. What MUST be EXCLUDED:\n",
    "- DO NOT include any medications, systemic therapies, or treatment plans:\n",
    "  - Chemotherapy, immunotherapy, targeted therapies (e.g., FGFR inhibitor, PARP inhibitor, CDK4/6 inhibitor, TKIs)\n",
    "  - Hormonal therapy, radiation therapy, surgery\n",
    "  - Clinical trial options, even if the trial involves targeted drugs or inhibitors\n",
    "- DO NOT include tests that are already completed, historic, or only mentioned in past oncology history.\n",
    "- DO NOT include imaging (CT, PET, MRI, X-ray, ultrasound) or routine labs (CBC, CMP).\n",
    "\n",
    "4. If there is NO new genetic or molecular test planned in the Assessment/Plan:\n",
    "- Set the value to a clear negative statement, for example:\n",
    "  \"No new genetic or molecular tests were planned during this visit.\"\n",
    "\n",
    "5. Output format:\n",
    "Respond *only* with a JSON object using this exact schema:\n",
    "{\n",
    "    \"genetic_testing_plan\": \"A summary of any future planned genetic or molecular tests, or a clear statement that none are planned.\"\n",
    "}\n",
    "\"\"\",\n",
    "\n",
    "\n",
    "\"Referral\": \"\"\"\n",
    "TASK: Extract 'Referral' from the given 'Assessment/Plan' section.\n",
    "do not mistake the doctor's signature at the end of the note as a referral.\n",
    "Respond *only* with a JSON object using this exact schema:\n",
    "{\n",
    "\"Nutrition\": \"any nutration referrals such as diet optimization, appetite, weight maintenance), \"Genetics\": \"eg, germline testing, family counseling\",\n",
    "\"Specialty\": \"eg, Palliative care (symptom or pain management, goals of care), Radiation oncology, surgical oncology, Psychology, psychiatry for coping and mood support\",\n",
    "\"Others\": \"Physical or occupational therapy, Social work, financial counseling\",\n",
    "\"follow up\": \"follow up mentioned in the provided text. f/up[, f/p and fup all mean follow up.\"\n",
    "}\n",
    "\"\"\",\n",
    "\n",
    "\n",
    "\"follow up/next visit\": \"\"\"\n",
    "TASK: Extract the 'follow up/next visit' from the given 'Assessment/Plan' section.\n",
    "\n",
    "Respond *only* with a JSON object using this exact schema:\n",
    "{\n",
    "    \"Next clinic visit\": \"(in-person or telehealth): timing and purpose\"\n",
    "}\n",
    "\"\"\",\n",
    "\n",
    "\n",
    "\"Advance care planning\": \"\"\"\n",
    "TASK: Extract the 'Advance care planning' from the given 'Assessment/Plan' section.\n",
    "Include Advance directives, health-care proxy, code status (if appropriate). \n",
    "Respond *only* with a JSON object using this exact schema:\n",
    "{\n",
    "    \"Advance care\": \"A summary of any planned Advance care.\"\n",
    "}\n",
    "\"\"\",\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2442743",
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_prompts = {\n",
    "\n",
    "\"Why you came today\": \"\"\"\n",
    "TASK: Briefly state the purpose of today's visit using only information from the key points. \n",
    "\"\"\",\n",
    "\n",
    "\n",
    "\"What you told us\": \"\"\"\n",
    "TASK: List the patient concerns, symptoms, or questions mentioned in the NOTE section.\n",
    "\"\"\",\n",
    "\n",
    "\n",
    "\"What did we find\": \"\"\"\n",
    "TASK: Explain the result of today's visit (exam, blood work, scans) in plain language. You need to include all 'key points' from KEYPOINTS section.\n",
    "\n",
    "\"\"\",\n",
    "\n",
    "\n",
    "\"What is the plan\": \"\"\"\n",
    "TASK: List the next steps (treatment, monitoring, follow-up) in plain language. Include all related information from KEYPOINTS section if the following are mentioned, including:\n",
    "    1, Medication plan\n",
    "    2, Procedure plan\n",
    "    3, Imaging plan\n",
    "    4, Lab plan\n",
    "    5, Genetic Testing Plan\n",
    "    6, Referral\n",
    "\"\"\",\n",
    "\n",
    "\n",
    "\"Closing with Support\": \"\"\"\n",
    "TASK: write an ending sentence to show your support. Limit to one sentence maximum.\n",
    "\"\"\",\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63edc7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create save file and write prompts\n",
    "with open('results.txt', 'w') as f:\n",
    "    f.write(\"extraction_prompts\\n\")\n",
    "    original_text = (str(extraction_prompts)\n",
    "                     .replace('\\\\n', '\\n')\n",
    "                     .replace(\"\\\\'\", \"'\")\n",
    "                     .replace('\\\\\"', '\"'))\n",
    "    f.write(original_text + \"\\n\")\n",
    "    f.write(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "    f.write(\"plan_extraction_prompts\\n\")\n",
    "    original_text = (str(plan_extraction_prompts)\n",
    "                     .replace('\\\\n', '\\n')\n",
    "                     .replace(\"\\\\'\", \"'\")\n",
    "                     .replace('\\\\\"', '\"'))\n",
    "    f.write(original_text + \"\\n\")\n",
    "    f.write(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "    f.write(\"explain_prompts\\n\")\n",
    "    original_text = (str(explain_prompts)\n",
    "                     .replace('\\\\n', '\\n')\n",
    "                     .replace(\"\\\\'\", \"'\")\n",
    "                     .replace('\\\\\"', '\"'))\n",
    "    f.write(original_text + \"\\n\")\n",
    "    f.write(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "with open('results.txt', 'a') as f:\n",
    "    f.write(\"\\n\"*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a96ba026",
   "metadata": {},
   "outputs": [],
   "source": [
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "EXPLANATION_GENERATION_CONFIG = {\n",
    "    \"max_new_tokens\": 666,\n",
    "    \"temperature\": 0.3,\n",
    "    \"top_p\": 0.85,\n",
    "    \"repetition_penalty\": 1.2,     \n",
    "    \"no_repeat_ngram_size\": 3,     \n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "    \"eos_token_id\": terminators,\n",
    "    \"early_stopping\": True,\n",
    "}\n",
    "\n",
    "CLEANING_CONFIG = {\n",
    "    \"max_new_tokens\": 666,\n",
    "    \"temperature\": 0.1,\n",
    "    \"top_p\": 0.9,\n",
    "    \"repetition_penalty\": 1.2,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "    \"eos_token_id\": terminators, \n",
    "    \"early_stopping\": True,\n",
    "}\n",
    "\n",
    "KEYPOINT_CONFIG = {\n",
    "    \"max_new_tokens\": 512, \n",
    "    \"eos_token_id\": tokenizer.eos_token_id,\n",
    "    \"do_sample\": False\n",
    "    }\n",
    "\n",
    "assesment_and_plan_CONFIG= {\n",
    "    \"max_new_tokens\": 2048, \n",
    "    \"eos_token_id\": tokenizer.eos_token_id,\n",
    "    \"do_sample\": False\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01a7421",
   "metadata": {},
   "source": [
    "# run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096fe7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing row 0/10...\n",
      "Attempt 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1 failed sanity check (not verbatim). Retrying...\n",
      "Attempt 2...\n",
      "Attempt 2 failed sanity check (not verbatim). Retrying...\n",
      "Attempt 3...\n",
      "Attempt 3 failed sanity check (not verbatim). Retrying...\n",
      "All attempts failed. Marking as None for manual review.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keypoints from extraction_prompts\n",
      "keypoints from plan_extraction_prompts\n",
      "\n",
      "Processing row 1/10...\n",
      "Attempt 1...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 76\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# 2. Run the model\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempt \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mretry_count\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 76\u001b[0m assesment_and_plan, _ \u001b[38;5;241m=\u001b[39m \u001b[43mrun_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcurrent_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43massesment_and_plan_CONFIG\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# 3. Sanity Check (The \"Rewire\" Trigger)\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# We strip whitespace from edges to prevent false negatives due to spaces\u001b[39;00m\n\u001b[1;32m     85\u001b[0m clean_original \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(note_text\u001b[38;5;241m.\u001b[39msplit())\n",
      "File \u001b[0;32m~/repo/med_dict/ult.py:167\u001b[0m, in \u001b[0;36mrun_model\u001b[0;34m(prompt_text, model, tokenizer, generation_config, kv_cache)\u001b[0m\n\u001b[1;32m    161\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mones_like(inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# Run the model\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# - past_key_values=kv_cache tells the model to use the \"memory\"\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# - return_dict_in_generate=True gives us a structured output\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m#   that includes the new kv_cache\u001b[39;00m\n\u001b[0;32m--> 167\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkv_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# This is key!\u001b[39;49;00m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;66;03m# 4. Extract the new, updated KV cache\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;66;03m# This can be fed back into the function on the next call\u001b[39;00m\n\u001b[1;32m    177\u001b[0m new_kv_cache \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mpast_key_values\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/generation/utils.py:2215\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2207\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2208\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2209\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2210\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2211\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2212\u001b[0m     )\n\u001b[1;32m   2214\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2215\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2216\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2220\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2222\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2223\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2225\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2226\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2227\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2228\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2229\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2234\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2235\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/generation/utils.py:3195\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3192\u001b[0m unfinished_sequences \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(batch_size, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3193\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_initial_cache_position(input_ids, model_kwargs)\n\u001b[0;32m-> 3195\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_has_unfinished_sequences\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3196\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthis_peer_finished\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcur_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcur_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\n\u001b[1;32m   3197\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   3198\u001b[0m     \u001b[38;5;66;03m# prepare model inputs\u001b[39;00m\n\u001b[1;32m   3199\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   3201\u001b[0m     \u001b[38;5;66;03m# prepare variable output controls (note: some models won't accept all output controls)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/generation/utils.py:2413\u001b[0m, in \u001b[0;36mGenerationMixin._has_unfinished_sequences\u001b[0;34m(self, this_peer_finished, synced_gpus, device, cur_len, max_length)\u001b[0m\n\u001b[1;32m   2411\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m this_peer_finished_flag\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m   2412\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 2413\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2414\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   2415\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_results = {}\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    print(f\"\\nProcessing row {index}/{len(df)}...\")\n",
    "\n",
    "    # note_text = row['note_text']\n",
    "    # assesment_and_plan,_=run_model(\n",
    "    # 'here is a medical note \\n\\n'+note_text+'\\n\\n now, return me all the ORINGAL TEXT (do not do any modifications, do no rephrase and summarize) after the words like \"Assessment and Plan\" or \"Assessment/Plan\". ignore anything before that. ignore the line breaking characters.',\n",
    "    # model,\n",
    "    # tokenizer,\n",
    "    # assesment_and_plan_CONFIG)\n",
    "\n",
    "\n",
    "    # sanity_prompt = (\n",
    "    #     \"You are a text validator. Compare the following two texts.\\n\\n\"\n",
    "    #     \"--- ORIGINAL SOURCE START ---\\n\" + note_text + \"\\n--- ORIGINAL SOURCE END ---\\n\\n\"\n",
    "    #     \"--- EXTRACTED SEGMENT START ---\\n\" + assesment_and_plan + \"\\n--- EXTRACTED SEGMENT END ---\\n\\n\"\n",
    "    #     \"Task: Verify if the 'EXTRACTED SEGMENT' appears word-for-word inside the 'ORIGINAL SOURCE'.\\n\"\n",
    "    #     \"If it is a summary, rephrased, or contains text not in the original, return FAIL.\\n\"\n",
    "    #     \"If it is an exact substring match, return PASS.\\n\"\n",
    "    #     \"Answer with one word: PASS or FAIL.\"\n",
    "    # )\n",
    "\n",
    "    # sanity_result, _ = run_model(\n",
    "    #     sanity_prompt,\n",
    "    #     model,\n",
    "    #     tokenizer,\n",
    "    #     assesment_and_plan_CONFIG\n",
    "    # )\n",
    "\n",
    "    # if \"FAIL\" in sanity_result.upper():\n",
    "    #     print(f\"Sanity Check Failed for row. Model output was: {sanity_result}\")\n",
    "    #     # Optional: Retry logic, or flag for manual review\n",
    "    #     # assesment_and_plan = None \n",
    "    # else:\n",
    "    #     print(\"got assesment_and_plan (verified)\")\n",
    "\n",
    "\n",
    "\n",
    "    # print('got assesment_and_plan')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    MAX_RETRIES = 3\n",
    "    retry_count = 0\n",
    "    extraction_success = False\n",
    "    final_assessment_plan = \"\"\n",
    "\n",
    "    note_text = row['note_text']\n",
    "\n",
    "    while retry_count < MAX_RETRIES and not extraction_success:\n",
    "        \n",
    "        # 1. Dynamic Prompting: Make the prompt angrier/stricter on retries\n",
    "        if retry_count == 0:\n",
    "            # Standard Prompt\n",
    "            current_prompt = (\n",
    "                'here is a medical note \\n\\n' + note_text + '\\n\\n '\n",
    "                'now, return me all the ORINGAL TEXT (do not do any modifications, do no rephrase and summarize) '\n",
    "                'after the words like \"Assessment and Plan\" or \"Assessment/Plan\". '\n",
    "                'ignore anything before that. ignore the line breaking characters.'\n",
    "            )\n",
    "        else:\n",
    "            # \"Rewired\" Prompt: Explicitly forbid the previous error\n",
    "            current_prompt = (\n",
    "                'The previous extraction failed because it was a summary. '\n",
    "                'You must act as a \"Copy-Paste\" tool only.\\n\\n'\n",
    "                'SOURCE TEXT:\\n' + note_text + '\\n\\n'\n",
    "                'INSTRUCTION: Extract the \"Assessment and Plan\" section EXACTLY as written. '\n",
    "                'Do not change a single character. Do not fix grammar. Do not summarize.\\n'\n",
    "                'Start copying immediately after the header.'\n",
    "            )\n",
    "\n",
    "        # 2. Run the model\n",
    "        print(f\"Attempt {retry_count + 1}...\")\n",
    "        assesment_and_plan, _ = run_model(\n",
    "            current_prompt,\n",
    "            model,\n",
    "            tokenizer,\n",
    "            assesment_and_plan_CONFIG\n",
    "        )\n",
    "        \n",
    "        # 3. Sanity Check (The \"Rewire\" Trigger)\n",
    "        # We strip whitespace from edges to prevent false negatives due to spaces\n",
    "        clean_original = \" \".join(note_text.split())\n",
    "        clean_candidate = \" \".join(assesment_and_plan.split())\n",
    "\n",
    "        # Check if the candidate exists strictly inside the original text\n",
    "        if clean_candidate in clean_original and len(clean_candidate) > 10:\n",
    "            # Success!\n",
    "            final_assessment_plan = assesment_and_plan\n",
    "            extraction_success = True\n",
    "            print(f\"Success on attempt {retry_count + 1}\")\n",
    "        else:\n",
    "            # Failure! Loop will run again\n",
    "            print(f\"Attempt {retry_count + 1} failed sanity check (not verbatim). Retrying...\")\n",
    "            retry_count += 1\n",
    "\n",
    "    # 4. Final Fallback (if all retries fail)\n",
    "    if not extraction_success:\n",
    "        print(\"All attempts failed. Marking as None for manual review.\")\n",
    "        final_assessment_plan = note_text # or keep the last messy attempt: assesment_and_plan\n",
    "\n",
    "\n",
    "\n",
    "    # extract keypoints from extraction_prompts\n",
    "    keypoints_base_prompt = (\n",
    "        f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n\"\n",
    "        f\"You are a medical data extraction expert. You will be given a long medical note. \"\n",
    "        f\"Your task is to answer a series of questions about it, one by one. \"\n",
    "        f\"Respond *only* with the valid JSON object requested. Do not add markdown backticks or any other text.\"\n",
    "        f\"<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "        f\"Here is the medical note:\\n\\n\"\n",
    "        f\"--- BEGIN NOTE ---\\n{note_text}\\n--- END NOTE ---\"\n",
    "        f\"\\n\\nI will now ask you to extract specific sections. \"\n",
    "        f\"Please wait for my first extraction task.\"\n",
    "        f\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "        f\"{{\\\"status\\\": \\\"Understood. I have read the note and am ready.\\\"}}\"\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(keypoints_base_prompt, return_tensors=\"pt\").to(model.device)\n",
    "        outputs = model(\n",
    "            input_ids=inputs[\"input_ids\"], \n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            use_cache=True\n",
    "        )\n",
    "        base_cache = outputs.past_key_values\n",
    "        del inputs, outputs\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    keypoints = {}\n",
    "    run_model_function = run_model_with_cache_manual \n",
    "    for key, task in extraction_prompts.items():\n",
    "        task_prompt = (\n",
    "            f\"<|start_header_id|>user<|end_header_id|>\\n\\n\" \n",
    "            f\"{task}\"\n",
    "            f\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "        )\n",
    "        answer, returned_cache = run_model_function(\n",
    "            task_prompt, \n",
    "            model, \n",
    "            tokenizer, \n",
    "            KEYPOINT_CONFIG, \n",
    "            kv_cache=base_cache\n",
    "        )\n",
    "        del returned_cache\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        # try:\n",
    "        #     clean_answer = answer.strip().strip(\"```json\").strip(\"```\").strip()\n",
    "        #     keypoints[key] = json.loads(clean_answer)\n",
    "        # except json.JSONDecodeError:\n",
    "        #     keypoints[key] = answer\n",
    "\n",
    "        # check\n",
    "        verification_prompt = (\n",
    "            f\"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "            f\"CONTEXT: You previously extracted the following information (Initial Answer):\\n\"\n",
    "            f\"--- BEGIN INITIAL ANSWER ---\\n{answer}\\n--- END INITIAL ANSWER ---\\n\\n\"\n",
    "            f\"**CRITICAL TASK:** You must now act as a verifier. Review the Initial Answer against the original full medical note (which is stored in your memory/context).\\n\"\n",
    "            f\"1. **Faithfulness Check:** Check if every statement in the Initial Answer is strictly supported by the original medical note.\\n\"\n",
    "            f\"2. **Revision:** Generate a **Final Answer** by removing *any* part of the Initial Answer that is not supported by the original medical note. If the Initial Answer is fully supported, the Final Answer should be the same.\\n\"\n",
    "            f\"Return the result strictly as a JSON object with the key 'final_answer'.\\n\"\n",
    "            f\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "        )\n",
    "\n",
    "        final_answer_raw, _ = run_model_function(\n",
    "            verification_prompt, model, tokenizer, KEYPOINT_CONFIG, kv_cache=base_cache\n",
    "        )\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        try:\n",
    "            final_result = json.loads(final_answer_raw.strip().strip(\"```json\").strip(\"```\").strip())\n",
    "            keypoints[key] = final_result.get('final_answer', answer)\n",
    "        except json.JSONDecodeError:\n",
    "            keypoints[key] = answer\n",
    "    print('keypoints from extraction_prompts')\n",
    "\n",
    "\n",
    "    # extract keypoints from plan_extraction_prompts\n",
    "    keypoints_base_prompt = (\n",
    "    f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n\"\n",
    "    f\"You are a medical data extraction expert. You will be given a long medical note. \"\n",
    "    f\"Your task is to answer a series of questions about it, one by one. \"\n",
    "    f\"Respond *only* with the valid JSON object requested. Do not add markdown backticks or any other text.\"\n",
    "    f\"<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "    f\"Here is the medical note:\\n\\n\"\n",
    "    f\"--- BEGIN NOTE ---\\n{assesment_and_plan}\\n--- END NOTE ---\"\n",
    "    f\"\\n\\nI will now ask you to extract specific sections. \"\n",
    "    f\"Please wait for my first extraction task.\"\n",
    "    f\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "    f\"{{\\\"status\\\": \\\"Understood. I have read the note and am ready.\\\"}}\"\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(keypoints_base_prompt, return_tensors=\"pt\").to(model.device)\n",
    "        outputs = model(\n",
    "            input_ids=inputs[\"input_ids\"], \n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            use_cache=True\n",
    "        )\n",
    "        base_cache = outputs.past_key_values\n",
    "        del inputs, outputs\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    run_model_function = run_model_with_cache_manual \n",
    "    for key, task in plan_extraction_prompts.items():\n",
    "        task_prompt = (\n",
    "            f\"<|start_header_id|>user<|end_header_id|>\\n\\n\" \n",
    "            f\"{task}\"\n",
    "            f\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "        )\n",
    "        answer, returned_cache = run_model_function(\n",
    "            task_prompt, \n",
    "            model, \n",
    "            tokenizer, \n",
    "            KEYPOINT_CONFIG, \n",
    "            kv_cache=base_cache\n",
    "        )\n",
    "        del returned_cache\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        # try:\n",
    "        #     clean_answer = answer.strip().strip(\"```json\").strip(\"```\").strip()\n",
    "        #     keypoints[key] = json.loads(clean_answer)\n",
    "        # except json.JSONDecodeError:\n",
    "        #     keypoints[key] = answer\n",
    "                # check\n",
    "        verification_prompt = (\n",
    "            f\"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "            f\"CONTEXT: You previously extracted the following information (Initial Answer):\\n\"\n",
    "            f\"--- BEGIN INITIAL ANSWER ---\\n{answer}\\n--- END INITIAL ANSWER ---\\n\\n\"\n",
    "            f\"**CRITICAL TASK:** You must now act as a verifier. Review the Initial Answer against the original full medical note (which is stored in your memory/context).\\n\"\n",
    "            f\"1. **Faithfulness Check:** Check if every statement in the Initial Answer is strictly supported by the original medical note.\\n\"\n",
    "            f\"2. **Revision:** Generate a **Final Answer** by removing *any* part of the Initial Answer that is not supported by the original medical note. If the Initial Answer is fully supported, the Final Answer should be the same.\\n\"\n",
    "            f\"Return the result strictly as a JSON object with the key 'final_answer'.\\n\"\n",
    "            f\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "        )\n",
    "\n",
    "        final_answer_raw, _ = run_model_function(\n",
    "            verification_prompt, model, tokenizer, KEYPOINT_CONFIG, kv_cache=base_cache\n",
    "        )\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        try:\n",
    "            final_result = json.loads(final_answer_raw.strip().strip(\"```json\").strip(\"```\").strip())\n",
    "            keypoints[key] = final_result.get('final_answer', answer)\n",
    "        except json.JSONDecodeError:\n",
    "            keypoints[key] = answer\n",
    "\n",
    "    print('keypoints from plan_extraction_prompts')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # # explain\n",
    "    # explain_base_prompt = (\n",
    "    #     f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n\"\n",
    "    #     f\"You are an experienced and compassionate Oncologist and medical educator. Your primary role is to translate complex medical information into clear, 8th-grade level English. \"\n",
    "    #     f\"Your task is to answer a series of questions about it, one by one. \"\n",
    "    #     f'''### STRICT NEGATIVE CONSTRAINTS:\n",
    "    #     * Do not say anything not present in the medical note.\n",
    "    #     * If cancer has spread (metastasis), DO NOT list the specific organs affected. Say \"the cancer has spread to other parts of the body.\"\n",
    "    #     * Do not use fatalistic language. The focus MUST be on quality of life.\n",
    "    #     *NO ADDING SIDE EFFECTS, Unless they appear in the keypoints exactly.\n",
    "    #     '''\n",
    "    #     f\"<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "    #     f\"Here is the medical note:\\n\\n\"\n",
    "    #     f\"--- BEGIN NOTE ---\\n{note_text}\\n--- END NOTE ---\"\n",
    "    #     f\"Here is the keypoints extracted from the medical note:\\n\\n\"\n",
    "    #     f\"--- BEGIN KEYPOINTS ---\\n{keypoints}\\n--- END KEYPOINTS ---\"\n",
    "    #     f\"\\n\\nI will now ask you to extract specific sections. \"\n",
    "    #     f\"Please wait for my first extraction task.\"\n",
    "    #     f\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "    #     f\"{{\\\"status\\\": \\\"Understood. I have read the note and am ready.\\\"}}\"\n",
    "    # )\n",
    "\n",
    "    # with torch.no_grad():\n",
    "    #     inputs = tokenizer(explain_base_prompt, return_tensors=\"pt\").to(model.device)\n",
    "    #     outputs = model(\n",
    "    #         input_ids=inputs[\"input_ids\"], \n",
    "    #         attention_mask=inputs[\"attention_mask\"],\n",
    "    #         use_cache=True\n",
    "    #     )\n",
    "    #     base_cache = outputs.past_key_values\n",
    "    #     del inputs, outputs\n",
    "    #     torch.cuda.empty_cache()\n",
    "    #     gc.collect()\n",
    "\n",
    "    # explain = {}\n",
    "    # run_model_function = run_model_with_cache_manual \n",
    "    # for key, task in explain_prompts.items():\n",
    "    #     task_prompt = (\n",
    "    #         f\"<|start_header_id|>user<|end_header_id|>\\n\\n\" \n",
    "    #         f\"{task}\"\n",
    "    #         f\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "    #     )\n",
    "    #     answer, returned_cache = run_model_function(\n",
    "    #         task_prompt, \n",
    "    #         model, \n",
    "    #         tokenizer, \n",
    "    #         KEYPOINT_CONFIG, \n",
    "    #         kv_cache=base_cache\n",
    "    #     )\n",
    "    #     del returned_cache\n",
    "    #     torch.cuda.empty_cache()\n",
    "    #     gc.collect()\n",
    "        \n",
    "    #     try:\n",
    "    #         clean_answer = answer.strip().strip(\"```json\").strip(\"```\").strip()\n",
    "    #         explain[key] = json.loads(clean_answer)\n",
    "    #     except json.JSONDecodeError:\n",
    "    #         explain[key] = answer\n",
    "\n",
    "    # print('explain')\n",
    "\n",
    "    \n",
    "\n",
    "    # print('explain')\n",
    "    # explanation_prompt= create_explanation_prompt(note_text, keypoints)\n",
    "    # explanation,_  = run_model_function(explanation_prompt, model, tokenizer, EXPLANATION_GENERATION_CONFIG)\n",
    "\n",
    "    # # clean up\n",
    "    # print('clean')\n",
    "    # cleaning_prompt = create_cleaning_prompt(explanation)\n",
    "    # final_result,_  = run_model_function(cleaning_prompt, model, tokenizer, CLEANING_CONFIG)\n",
    "    \n",
    "    row_result = {\n",
    "        'coral_idx': row['coral_idx'],\n",
    "        'note_text': note_text,\n",
    "        'assesment_and_plan': assesment_and_plan,\n",
    "        'keypoints': keypoints,\n",
    "        # 'explain': explain,\n",
    "        # 'raw_result': explanation,\n",
    "        # 'clean_result': final_result\n",
    "    }\n",
    "    \n",
    "    all_results[index]=(row_result)\n",
    "\n",
    "    # # print here\n",
    "    # print('\\n'*5)\n",
    "    # print(f\"\\n{'='*60}\")\n",
    "    # print(f\"RESULTS FOR ROW {index + 1}\")\n",
    "    # print(f\"{'='*60}\")\n",
    "    \n",
    "    # for col in ['assesment_and_plan','keypoints', ]:\n",
    "    #     print(f\"\\n--- Column: {col} ---\")\n",
    "    #     original_text = row_result[col]\n",
    "    #     try:\n",
    "    #         (print_json((original_text)))\n",
    "    #     except:\n",
    "    #         print(original_text)\n",
    "\n",
    "\n",
    "    # write to txt file\n",
    "    with open('results.txt', 'a') as f:  # 'a' to append, 'w' to overwrite\n",
    "        f.write('\\n' * 5)\n",
    "        f.write(f\"\\n{'='*60}\\n\")\n",
    "        f.write(f\"RESULTS FOR ROW {index + 1}\\n\")\n",
    "        f.write(f\"{'='*60}\\n\")\n",
    "        \n",
    "        for col in row_result.keys():\n",
    "            f.write(f\"\\n--- Column: {col} ---\\n\")\n",
    "            original_text = row_result[col]\n",
    "            try:\n",
    "                import json\n",
    "                f.write(json.dumps(original_text, indent=2) + '\\n')\n",
    "            except:\n",
    "                f.write(str(original_text) + '\\n')\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
