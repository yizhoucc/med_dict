{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4dd4555-473d-48cd-bc86-60a4c41da0a8",
   "metadata": {},
   "source": [
    "## load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "611f29b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b2c708e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yc/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "with open(\"hf.token\", \"r\") as f:\n",
    "    hftoken = f.read().strip()  \n",
    "\n",
    "import os\n",
    "cache_dir = \"/mnt/c/Users/yc/.cache/huggingface\"\n",
    "os.environ['HF_HOME'] = cache_dir\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(token=hftoken)  # Move token to environment variable\n",
    "\n",
    "from ult import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d74a59c-b625-4944-8a79-aea69bc6cd8a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [01:33<00:00, 23.36s/it]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "device_map = {\"\": 0}\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=device_map,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e1c388-57f9-41b0-87d1-25a9ad884430",
   "metadata": {},
   "source": [
    "## data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf6281f4-7ebb-48fd-a08b-4271349ededf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coral_idx</th>\n",
       "      <th>Sex</th>\n",
       "      <th>UCSFDerivedRaceEthnicity_X</th>\n",
       "      <th>BirthDate</th>\n",
       "      <th>note_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>184</td>\n",
       "      <td>Female</td>\n",
       "      <td>Latinx</td>\n",
       "      <td>1983-10-04</td>\n",
       "      <td>Medical Oncology Consult Note  Video Consult  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>185</td>\n",
       "      <td>Female</td>\n",
       "      <td>Multi-Race/Ethnicity</td>\n",
       "      <td>1973-03-08</td>\n",
       "      <td>This is a shared service.  Physician Statement...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>193</td>\n",
       "      <td>Female</td>\n",
       "      <td>Multi-Race/Ethnicity</td>\n",
       "      <td>1979-06-20</td>\n",
       "      <td>ID: ***** ***** is a 39 y.o. premenopausal pat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>210</td>\n",
       "      <td>Female</td>\n",
       "      <td>Southwest Asian and North African</td>\n",
       "      <td>1974-04-05</td>\n",
       "      <td>Patient Name: ***** *****  ***** *****: 08/22/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>223</td>\n",
       "      <td>Female</td>\n",
       "      <td>Unknown/Declined</td>\n",
       "      <td>1960-10-12</td>\n",
       "      <td>We performed this consultation using real-time...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>140</td>\n",
       "      <td>Female</td>\n",
       "      <td>Native Hawaiian or Other Pacific Islander</td>\n",
       "      <td>1964-03-25</td>\n",
       "      <td>Medical Oncology Consult Note    Patient Name:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>141</td>\n",
       "      <td>Female</td>\n",
       "      <td>Native Hawaiian or Other Pacific Islander</td>\n",
       "      <td>1975-03-29</td>\n",
       "      <td>This is a shared visit for services provided b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>142</td>\n",
       "      <td>Female</td>\n",
       "      <td>Native Hawaiian or Other Pacific Islander</td>\n",
       "      <td>1967-10-06</td>\n",
       "      <td>Medical Oncology Consult Note  Video Consult  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>143</td>\n",
       "      <td>Female</td>\n",
       "      <td>Native Hawaiian or Other Pacific Islander</td>\n",
       "      <td>1943-12-23</td>\n",
       "      <td>This is an independent visit      SUBJECTIVE  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>144</td>\n",
       "      <td>Female</td>\n",
       "      <td>Native Hawaiian or Other Pacific Islander</td>\n",
       "      <td>1987-06-23</td>\n",
       "      <td>***** ***** Note  Patient Name: ***** *****   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>145</td>\n",
       "      <td>Female</td>\n",
       "      <td>Native American or Alaska Native</td>\n",
       "      <td>1985-02-26</td>\n",
       "      <td>HPI:  ***** ***** is a 34 y.o. female with E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>146</td>\n",
       "      <td>Female</td>\n",
       "      <td>Native Hawaiian or Other Pacific Islander</td>\n",
       "      <td>1964-03-15</td>\n",
       "      <td>*****  ***** with MBC ***** 2008    CC  2nd op...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>147</td>\n",
       "      <td>Female</td>\n",
       "      <td>Native Hawaiian or Other Pacific Islander</td>\n",
       "      <td>1990-03-26</td>\n",
       "      <td>ID: ***** ***** ***** is a 29 y.o. premenopaus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>148</td>\n",
       "      <td>Female</td>\n",
       "      <td>Native American or Alaska Native</td>\n",
       "      <td>1957-05-23</td>\n",
       "      <td>***** ***** Note  Patient Name: ***** ***** **...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>149</td>\n",
       "      <td>Female</td>\n",
       "      <td>Native American or Alaska Native</td>\n",
       "      <td>1954-08-11</td>\n",
       "      <td>***** ***** Note  Patient Name: ***** *****   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    coral_idx     Sex                 UCSFDerivedRaceEthnicity_X   BirthDate  \\\n",
       "44        184  Female                                     Latinx  1983-10-04   \n",
       "45        185  Female                       Multi-Race/Ethnicity  1973-03-08   \n",
       "53        193  Female                       Multi-Race/Ethnicity  1979-06-20   \n",
       "70        210  Female          Southwest Asian and North African  1974-04-05   \n",
       "83        223  Female                           Unknown/Declined  1960-10-12   \n",
       "0         140  Female  Native Hawaiian or Other Pacific Islander  1964-03-25   \n",
       "1         141  Female  Native Hawaiian or Other Pacific Islander  1975-03-29   \n",
       "2         142  Female  Native Hawaiian or Other Pacific Islander  1967-10-06   \n",
       "3         143  Female  Native Hawaiian or Other Pacific Islander  1943-12-23   \n",
       "4         144  Female  Native Hawaiian or Other Pacific Islander  1987-06-23   \n",
       "5         145  Female           Native American or Alaska Native  1985-02-26   \n",
       "6         146  Female  Native Hawaiian or Other Pacific Islander  1964-03-15   \n",
       "7         147  Female  Native Hawaiian or Other Pacific Islander  1990-03-26   \n",
       "8         148  Female           Native American or Alaska Native  1957-05-23   \n",
       "9         149  Female           Native American or Alaska Native  1954-08-11   \n",
       "\n",
       "                                            note_text  \n",
       "44  Medical Oncology Consult Note  Video Consult  ...  \n",
       "45  This is a shared service.  Physician Statement...  \n",
       "53  ID: ***** ***** is a 39 y.o. premenopausal pat...  \n",
       "70  Patient Name: ***** *****  ***** *****: 08/22/...  \n",
       "83  We performed this consultation using real-time...  \n",
       "0   Medical Oncology Consult Note    Patient Name:...  \n",
       "1   This is a shared visit for services provided b...  \n",
       "2   Medical Oncology Consult Note  Video Consult  ...  \n",
       "3   This is an independent visit      SUBJECTIVE  ...  \n",
       "4   ***** ***** Note  Patient Name: ***** *****   ...  \n",
       "5     HPI:  ***** ***** is a 34 y.o. female with E...  \n",
       "6   *****  ***** with MBC ***** 2008    CC  2nd op...  \n",
       "7   ID: ***** ***** ***** is a 29 y.o. premenopaus...  \n",
       "8   ***** ***** Note  Patient Name: ***** ***** **...  \n",
       "9   ***** ***** Note  Patient Name: ***** *****   ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_file_path = 'data/formaldef.txt'\n",
    "formaldic = txt_to_dict(txt_file_path)\n",
    "len(formaldic)\n",
    "\n",
    "meddict={}\n",
    "for k,v in formaldic.items():\n",
    "    meddict[k.split('Listen to pronunciation')[0].split('(')[0]]=v\n",
    "filename= 'data/filtered_medical_dictionary.csv'\n",
    "eighth_grade_words=set()\n",
    "with open(filename, 'r', encoding='utf-8') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    next(reader)  # Skip the header row\n",
    "    for row in reader:\n",
    "        if row:  # Make sure row is not empty\n",
    "            eighth_grade_words.add(row[0])  # Add the word (first column)\n",
    "filtered_meddict = {word: explanation for word, explanation in meddict.items() \n",
    "                   if word in eighth_grade_words}\n",
    "meddict=filtered_meddict\n",
    "# load data\n",
    "df = pd.read_csv('data/CORAL/coral-expert-curated-medical-oncology-reports-to-advance-language-model-inference-1.0/coral/unannotated/data/breastca_unannotated.csv')\n",
    "# df = df.sample(1, random_state=42)\n",
    "df=df.iloc[[44,45,53,70,83]+list(range(0,10))] \n",
    "# df=df.iloc[[44,45,53,70,83]] \n",
    "# df=df.iloc[[70,]] \n",
    "# test_note=df.iloc[2]['note_text']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfa3a9d",
   "metadata": {},
   "source": [
    "# keypoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03c3bce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "extraction_prompts = {\n",
    "\n",
    "\"Reason_for_Visit\": \"\"\"\n",
    "TASK: Extract 'Reason for Visit'.\n",
    "Respond *only* with a JSON object using this exact schema:\n",
    "{\n",
    "\"Patient type\": \"either New patient or follow up\"\n",
    "\"second opinion\": \"whether the visit is consultation/second opinion or not\",\n",
    "\"in-person\":\" either Televisit or in-person. (note, video consult, televisit, telehealth are the same thing\",\n",
    "\"summary\": \"A brief summary of the reason for visit.\"\n",
    "}\n",
    "\"\"\",\n",
    "\n",
    "\n",
    "\"What_We_Found\": \"\"\"\n",
    "TASK: Extract 'What We Found'. \n",
    "Respond *only* with a JSON object using this exact schema:\n",
    "{\n",
    "\"Type_of_Cancer\": \"list the type of cancer\"\n",
    "\"Stage_of_Cancer\": \"list the stage if it is written in the note\",\n",
    "\"Distant Metastasis\": \"if there is distant metastasis (met).\" \"Yes, to where; No, local, not sure, need more evidence such as imaging.\",\n",
    "\"Metastasis\": \"if there is met, Yes (to where), No, or Not sure\",\n",
    "\"lab_summary\": \"Summary of key lab results.\",\n",
    "\"findings\": \"Summary of new findings or disease status.\"\n",
    "}\n",
    "\"\"\",\n",
    "\n",
    "\n",
    "\"Treatment_Summary\": \"\"\"\n",
    "TASK: Extract 'Treatment Summary'. \n",
    "Respond *only* with a JSON object using this exact schema:\n",
    "{\n",
    "\"current_meds\": \"List of current oncologic medications or regimens.\",\n",
    "\"recent_changes\": \"Any holds, dose reductions, or switches.\",\n",
    "\"supportive_meds\": \"List of supportive medications.\"\n",
    "}\n",
    "\"\"\",\n",
    "\n",
    "\n",
    "\"Goals of care\": \"\"\"\n",
    "TASK: Extract 'What We Discussed / Decided'.\n",
    "Respond *only* with a JSON object using this exact schema:\n",
    "{\n",
    "\"goals_of_treatment\": \"eg, cancer is not curable, but it's treatable and the goal is to extend the duration and maintain the quality of life\",\n",
    "\"response_assessment\": \"How the cancer is responding to the treatment, not responding or responding.\",\n",
    "}\n",
    "\"\"\",\n",
    "}\n",
    "\n",
    "plan_extraction_prompts = {\n",
    "\n",
    "\"Medication_Plan_chatgpt\": \"\"\"\n",
    "TASK: Extract the 'Medication Plan' from the given 'Assessment/Plan' section.\n",
    "Include all current and future medication plans for both cancer therapy and supportive treatment. Cancer treatment could be one or many in chemotherapy, hormonal therapy, bone therapy, radiotherapy (eg. rad onc, xrt). Supportive treatment could be one or many in bowel regimen, pain medication, psychiatry medication, neuropathy or any blood transfusion plan. \n",
    "\n",
    "Include whether a medication is being started now (e.g,“will start”, “Rx sent”, “starting today” ), plan or discuss in the future after certain condition (e.g., “plan to start after radiation”, “discussed addition of…”), continue or maintained(“continue”), stop or change.\n",
    "\n",
    "Include an alterative, second-line or clinical trials options if dicussed. \n",
    "Include an 'other treatment' section for any other medications that only briefly mentioned in the plan section.\n",
    "Respond *only* with a JSON object using this exact schema:\n",
    "{\n",
    "    \"The medication/treatment, one of chemotherapy, hormonal therapy, bone therapy, radiotherapy\": \n",
    "{\"summary\":\"the summary of this type of medication, including start/stop/cotinue if applicable\",\n",
    "\"Short term side_effects_discussed\": \"short term Side effects of this particular medications.\",\n",
    "\"Long term side_effects_discussed\": \"long term Side effects of this particular medications..\"}\n",
    "}\n",
    "\"\"\",\n",
    "\n",
    "\n",
    "\"Medication_Plan\": \"\"\"\n",
    "TASK: Extract the 'Medication Plan' from the given 'Assessment/Plan' section.\n",
    "Include future medication plans, changes to current meds (start, stop, continue), supportive meds, bowel regimen, and any blood transfusion plan. Do not include past medications.\n",
    "Respond *only* with a JSON object using this exact schema:\n",
    "{\n",
    "    \"medication_plan\": \"A summary of the complete medication plan.\"\n",
    "}\n",
    "\"\"\",\n",
    "\n",
    "\n",
    "\"Therapy plan\": \"\"\"\n",
    "TASK: Extract the 'therapy Plan' from the given 'Assessment/Plan' section.\n",
    "Include chemotherapy, radiotherapy, hormonal therapy, and bone-therapy plans. Include future plans, changes to current therapy (start, stop, continue). Do not include past therapies.\n",
    "Respond *only* with a JSON object using this exact schema:\n",
    "{\n",
    "    \"therapy_plan\": \"A summary of the complete therapy plan.\"\n",
    "}\n",
    "\"\"\",\n",
    "\n",
    "\"radiotherapy plan\": \"\"\"\n",
    "TASK: Extract the 'radiotherapy Plan' from the given 'Assessment/Plan' section.\n",
    "For radiotherapy, include ANY statement that indicates \n",
    "it is being considered, recommended, or may be used, even if no explicit\n",
    "start/continue/plan keywords are present.\n",
    "\"\"\",\n",
    "\n",
    "\n",
    "\"Procedure_Plan\": \"\"\"\n",
    "TASK: Extract the 'Procedure Plan' from the given 'Assessment/Plan' section.\n",
    "Include future procedures including surgery, radiation therapy, or interventional procedures such as biopsy, lumbar puncture, or Chemo Port Insertion. Do not include past procedures.\n",
    "Do not include imaging plan, lab plan, or medication plan.\n",
    "Respond *only* with a JSON object using this exact schema:\n",
    "{\n",
    "    \"procedure_plan\": \"A summary of any planned procedures.\"\n",
    "}\n",
    "\"\"\",\n",
    "\n",
    "\n",
    "\"Imaging Plan\": \"\"\"\n",
    "TASK: Extract the 'Imaging Plan' from the given 'Assessment/Plan' section.\n",
    "Include all future imaging like CT, MRI, PET/CT, ultrasound,  DEXA scans, including timing and rationale if mentioned.\n",
    "Do not include any procedure plan, lab plan, or medication plan. Do not include past Imaging.\n",
    "Respond *only* with a JSON object using this exact schema:\n",
    "{\n",
    "    \"imaging_plan\": \"A summary of any planned procedures.\"\n",
    "}\n",
    "\"\"\",\n",
    "\n",
    "\n",
    "\"Lab Plan\": \"\"\"\n",
    "TASK: Extract the 'lab Plan' from the given 'Assessment/Plan' section.\n",
    "Include future labs like CBC, CMP, tumor markers, coagulation profile. Specify frequency and rationale if mentioned.\n",
    "Do not include any procedure plan, medication plan, or imagining plan. Do not include past labs.\n",
    "Respond *only* with a JSON object using this exact schema:\n",
    "{\n",
    "    \"lab_plan\": \"A summary of the future lab plan.\"\n",
    "}\n",
    "\"\"\",\n",
    "\n",
    "\n",
    "\"Genetic_Testing_Plan\": \"\"\"\n",
    "TASK: Extract the 'Genetic Testing Plan' from the given 'Assessment/Plan' section.\n",
    "\n",
    "1. Scope of section:\n",
    "- First, find the 'Assessment/Plan' section of the note (usually the final section).\n",
    "- Only look for plans that are clearly about FUTURE genetic or molecular TESTING.\n",
    "\n",
    "2. What COUNTS as 'genetic or molecular testing':\n",
    "- Diagnostic, prognostic, or predictive laboratory assays, such as:\n",
    "  - Tumor genomic sequencing / NGS / panel testing\n",
    "  - Germline genetic panels (e.g., hereditary cancer panel, BRCA testing)\n",
    "  - Liquid biopsy / circulating tumor DNA (ctDNA) tests\n",
    "  - Specific biomarker tests (e.g., PD-L1 testing, MSI testing, EGFR mutation testing)\n",
    "  - Any plan to \"send\", \"order\", \"check\", or \"obtain\" a genetic, genomic, or molecular TEST\n",
    "\n",
    "3. What MUST be EXCLUDED:\n",
    "- DO NOT include any medications, systemic therapies, or treatment plans:\n",
    "  - Chemotherapy, immunotherapy, targeted therapies (e.g., FGFR inhibitor, PARP inhibitor, CDK4/6 inhibitor, TKIs)\n",
    "  - Hormonal therapy, radiation therapy, surgery\n",
    "  - Clinical trial options, even if the trial involves targeted drugs or inhibitors\n",
    "- DO NOT include tests that are already completed, historic, or only mentioned in past oncology history.\n",
    "- DO NOT include imaging (CT, PET, MRI, X-ray, ultrasound) or routine labs (CBC, CMP).\n",
    "\n",
    "4. If there is NO new genetic or molecular test planned in the Assessment/Plan:\n",
    "- Set the value to a clear negative statement, for example:\n",
    "  \"No new genetic or molecular tests were planned during this visit.\"\n",
    "\n",
    "5. Output format:\n",
    "Respond *only* with a JSON object using this exact schema:\n",
    "{\n",
    "    \"genetic_testing_plan\": \"A summary of any future planned genetic or molecular tests, or a clear statement that none are planned.\"\n",
    "}\n",
    "\"\"\",\n",
    "\n",
    "\n",
    "\"Referral\": \"\"\"\n",
    "TASK: Extract 'Referral' from the given 'Assessment/Plan' section.\n",
    "\n",
    "Respond *only* with a JSON object using this exact schema:\n",
    "{\n",
    "\"Nutrition\": \"any nutration referrals such as diet optimization, appetite, weight maintenance), \"Genetics\": \"eg, germline testing, family counseling\",\n",
    "\"Specialty\": \"eg, Palliative care (symptom or pain management, goals of care), Radiation oncology, surgical oncology, Psychology, psychiatry for coping and mood support\",\n",
    "\"Others\": \"Physical or occupational therapy, Social work, financial counseling\",\n",
    "\"follow up\": \"follow up mentioned in the provided text. f/up[, f/p and fup all mean follow up.\"\n",
    "}\n",
    "\"\"\",\n",
    "\n",
    "\n",
    "\"follow up/next visit\": \"\"\"\n",
    "TASK: Extract the 'follow up/next visit' from the given 'Assessment/Plan' section.\n",
    "\n",
    "Respond *only* with a JSON object using this exact schema:\n",
    "{\n",
    "    \"Next clinic visit\": \"(in-person or telehealth): timing and purpose\"\n",
    "}\n",
    "\"\"\",\n",
    "\n",
    "\n",
    "\"Advance care planning\": \"\"\"\n",
    "TASK: Extract the 'Advance care planning' from the given 'Assessment/Plan' section.\n",
    "Include Advance directives, health-care proxy, code status (if appropriate)、\n",
    "Respond *only* with a JSON object using this exact schema:\n",
    "{\n",
    "    \"Advance care\": \"A summary of any planned Advance care.\"\n",
    "}\n",
    "\"\"\",\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2442743",
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_prompts = {\n",
    "\n",
    "\"Why you came today\": \"\"\"\n",
    "TASK: Briefly state the purpose of today's visit using only information from the key points. \n",
    "\"\"\",\n",
    "\n",
    "\n",
    "\"What you told us\": \"\"\"\n",
    "TASK: List the patient concerns, symptoms, or questions mentioned in the NOTE section.\n",
    "\"\"\",\n",
    "\n",
    "\n",
    "\"What did we find\": \"\"\"\n",
    "TASK: Explain the result of today's visit (exam, blood work, scans) in plain language. You need to include all 'key points' from KEYPOINTS section.\n",
    "\n",
    "\"\"\",\n",
    "\n",
    "\n",
    "\"What is the plan\": \"\"\"\n",
    "TASK: List the next steps (treatment, monitoring, follow-up) in plain language. Include all related information from KEYPOINTS section if the following are mentioned, including:\n",
    "    1, Medication plan\n",
    "    2, Procedure plan\n",
    "    3, Imaging plan\n",
    "    4, Lab plan\n",
    "    5, Genetic Testing Plan\n",
    "    6, Referral\n",
    "\"\"\",\n",
    "\n",
    "\n",
    "\"Closing with Support\": \"\"\"\n",
    "TASK: write an ending sentence to show your support. Limit to one sentence maximum.\n",
    "\"\"\",\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63edc7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create save file and write prompts\n",
    "with open('results.txt', 'w') as f:\n",
    "    f.write(\"extraction_prompts\\n\")\n",
    "    original_text = (str(extraction_prompts)\n",
    "                     .replace('\\\\n', '\\n')\n",
    "                     .replace(\"\\\\'\", \"'\")\n",
    "                     .replace('\\\\\"', '\"'))\n",
    "    f.write(original_text + \"\\n\")\n",
    "    f.write(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "    f.write(\"plan_extraction_prompts\\n\")\n",
    "    original_text = (str(plan_extraction_prompts)\n",
    "                     .replace('\\\\n', '\\n')\n",
    "                     .replace(\"\\\\'\", \"'\")\n",
    "                     .replace('\\\\\"', '\"'))\n",
    "    f.write(original_text + \"\\n\")\n",
    "    f.write(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "    f.write(\"explain_prompts\\n\")\n",
    "    original_text = (str(explain_prompts)\n",
    "                     .replace('\\\\n', '\\n')\n",
    "                     .replace(\"\\\\'\", \"'\")\n",
    "                     .replace('\\\\\"', '\"'))\n",
    "    f.write(original_text + \"\\n\")\n",
    "    f.write(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "with open('results.txt', 'a') as f:\n",
    "    f.write(\"\\n\"*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a96ba026",
   "metadata": {},
   "outputs": [],
   "source": [
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "EXPLANATION_GENERATION_CONFIG = {\n",
    "    \"max_new_tokens\": 666,\n",
    "    \"temperature\": 0.3,\n",
    "    \"top_p\": 0.85,\n",
    "    \"repetition_penalty\": 1.2,     \n",
    "    \"no_repeat_ngram_size\": 3,     \n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "    \"eos_token_id\": terminators,\n",
    "    \"early_stopping\": True,\n",
    "}\n",
    "\n",
    "CLEANING_CONFIG = {\n",
    "    \"max_new_tokens\": 666,\n",
    "    \"temperature\": 0.1,\n",
    "    \"top_p\": 0.9,\n",
    "    \"repetition_penalty\": 1.2,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "    \"eos_token_id\": terminators, \n",
    "    \"early_stopping\": True,\n",
    "}\n",
    "\n",
    "KEYPOINT_CONFIG = {\n",
    "    \"max_new_tokens\": 512, \n",
    "    \"eos_token_id\": tokenizer.eos_token_id,\n",
    "    \"do_sample\": False\n",
    "    }\n",
    "\n",
    "assesment_and_plan_CONFIG= {\n",
    "    \"max_new_tokens\": 2048, \n",
    "    \"eos_token_id\": tokenizer.eos_token_id,\n",
    "    \"do_sample\": False\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01a7421",
   "metadata": {},
   "source": [
    "# run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096fe7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing row 44/15...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got assesment_and_plan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keypoints from extraction_prompts\n",
      "keypoints from plan_extraction_prompts\n",
      "\n",
      "Processing row 45/15...\n",
      "got assesment_and_plan\n",
      "keypoints from extraction_prompts\n",
      "keypoints from plan_extraction_prompts\n",
      "\n",
      "Processing row 53/15...\n",
      "got assesment_and_plan\n",
      "keypoints from extraction_prompts\n",
      "keypoints from plan_extraction_prompts\n",
      "\n",
      "Processing row 70/15...\n",
      "got assesment_and_plan\n",
      "keypoints from extraction_prompts\n",
      "keypoints from plan_extraction_prompts\n",
      "\n",
      "Processing row 83/15...\n",
      "got assesment_and_plan\n",
      "keypoints from extraction_prompts\n",
      "keypoints from plan_extraction_prompts\n",
      "\n",
      "Processing row 0/15...\n",
      "got assesment_and_plan\n",
      "keypoints from extraction_prompts\n",
      "keypoints from plan_extraction_prompts\n",
      "\n",
      "Processing row 1/15...\n",
      "got assesment_and_plan\n",
      "keypoints from extraction_prompts\n",
      "keypoints from plan_extraction_prompts\n",
      "\n",
      "Processing row 2/15...\n",
      "got assesment_and_plan\n",
      "keypoints from extraction_prompts\n",
      "keypoints from plan_extraction_prompts\n",
      "\n",
      "Processing row 3/15...\n",
      "got assesment_and_plan\n",
      "keypoints from extraction_prompts\n",
      "keypoints from plan_extraction_prompts\n",
      "\n",
      "Processing row 4/15...\n",
      "got assesment_and_plan\n",
      "keypoints from extraction_prompts\n",
      "keypoints from plan_extraction_prompts\n",
      "\n",
      "Processing row 5/15...\n",
      "got assesment_and_plan\n",
      "keypoints from extraction_prompts\n",
      "keypoints from plan_extraction_prompts\n",
      "\n",
      "Processing row 6/15...\n",
      "got assesment_and_plan\n",
      "keypoints from extraction_prompts\n"
     ]
    }
   ],
   "source": [
    "all_results = {}\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    print(f\"\\nProcessing row {index}/{len(df)}...\")\n",
    "\n",
    "    note_text = row['note_text']\n",
    "    assesment_and_plan,_=run_model(\n",
    "    'here is a medical note\\n\\n'+note_text+'\\n\\n now, return me all the orignal text after the words like \"Assessment and Plan\" or \"Assessment/Plan\". ignore anything before that. ingore the line breaking characters.',\n",
    "    model,\n",
    "    tokenizer,\n",
    "    assesment_and_plan_CONFIG)\n",
    "    print('got assesment_and_plan')\n",
    "\n",
    "\n",
    "    # extract keypoints from extraction_prompts\n",
    "    keypoints_base_prompt = (\n",
    "        f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n\"\n",
    "        f\"You are a medical data extraction expert. You will be given a long medical note. \"\n",
    "        f\"Your task is to answer a series of questions about it, one by one. \"\n",
    "        f\"Respond *only* with the valid JSON object requested. Do not add markdown backticks or any other text.\"\n",
    "        f\"<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "        f\"Here is the medical note:\\n\\n\"\n",
    "        f\"--- BEGIN NOTE ---\\n{note_text}\\n--- END NOTE ---\"\n",
    "        f\"\\n\\nI will now ask you to extract specific sections. \"\n",
    "        f\"Please wait for my first extraction task.\"\n",
    "        f\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "        f\"{{\\\"status\\\": \\\"Understood. I have read the note and am ready.\\\"}}\"\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(keypoints_base_prompt, return_tensors=\"pt\").to(model.device)\n",
    "        outputs = model(\n",
    "            input_ids=inputs[\"input_ids\"], \n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            use_cache=True\n",
    "        )\n",
    "        base_cache = outputs.past_key_values\n",
    "        del inputs, outputs\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    keypoints = {}\n",
    "    run_model_function = run_model_with_cache_manual \n",
    "    for key, task in extraction_prompts.items():\n",
    "        task_prompt = (\n",
    "            f\"<|start_header_id|>user<|end_header_id|>\\n\\n\" \n",
    "            f\"{task}\"\n",
    "            f\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "        )\n",
    "        answer, returned_cache = run_model_function(\n",
    "            task_prompt, \n",
    "            model, \n",
    "            tokenizer, \n",
    "            KEYPOINT_CONFIG, \n",
    "            kv_cache=base_cache\n",
    "        )\n",
    "        del returned_cache\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        # try:\n",
    "        #     clean_answer = answer.strip().strip(\"```json\").strip(\"```\").strip()\n",
    "        #     keypoints[key] = json.loads(clean_answer)\n",
    "        # except json.JSONDecodeError:\n",
    "        #     keypoints[key] = answer\n",
    "\n",
    "        # check\n",
    "        verification_prompt = (\n",
    "            f\"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "            f\"CONTEXT: You previously extracted the following information (Initial Answer):\\n\"\n",
    "            f\"--- BEGIN INITIAL ANSWER ---\\n{answer}\\n--- END INITIAL ANSWER ---\\n\\n\"\n",
    "            f\"**CRITICAL TASK:** You must now act as a verifier. Review the Initial Answer against the original full medical note (which is stored in your memory/context).\\n\"\n",
    "            f\"1. **Faithfulness Check:** Check if every statement in the Initial Answer is strictly supported by the original medical note.\\n\"\n",
    "            f\"2. **Revision:** Generate a **Final Answer** by removing *any* part of the Initial Answer that is not supported by the original medical note. If the Initial Answer is fully supported, the Final Answer should be the same.\\n\"\n",
    "            f\"Return the result strictly as a JSON object with the key 'final_answer'.\\n\"\n",
    "            f\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "        )\n",
    "\n",
    "        final_answer_raw, _ = run_model_function(\n",
    "            verification_prompt, model, tokenizer, KEYPOINT_CONFIG, kv_cache=base_cache\n",
    "        )\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        try:\n",
    "            final_result = json.loads(final_answer_raw.strip().strip(\"```json\").strip(\"```\").strip())\n",
    "            keypoints[key] = final_result.get('final_answer', answer)\n",
    "        except json.JSONDecodeError:\n",
    "            keypoints[key] = answer\n",
    "    print('keypoints from extraction_prompts')\n",
    "\n",
    "\n",
    "    # extract keypoints from plan_extraction_prompts\n",
    "    keypoints_base_prompt = (\n",
    "    f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n\"\n",
    "    f\"You are a medical data extraction expert. You will be given a long medical note. \"\n",
    "    f\"Your task is to answer a series of questions about it, one by one. \"\n",
    "    f\"Respond *only* with the valid JSON object requested. Do not add markdown backticks or any other text.\"\n",
    "    f\"<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "    f\"Here is the medical note:\\n\\n\"\n",
    "    f\"--- BEGIN NOTE ---\\n{assesment_and_plan}\\n--- END NOTE ---\"\n",
    "    f\"\\n\\nI will now ask you to extract specific sections. \"\n",
    "    f\"Please wait for my first extraction task.\"\n",
    "    f\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "    f\"{{\\\"status\\\": \\\"Understood. I have read the note and am ready.\\\"}}\"\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(keypoints_base_prompt, return_tensors=\"pt\").to(model.device)\n",
    "        outputs = model(\n",
    "            input_ids=inputs[\"input_ids\"], \n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            use_cache=True\n",
    "        )\n",
    "        base_cache = outputs.past_key_values\n",
    "        del inputs, outputs\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    run_model_function = run_model_with_cache_manual \n",
    "    for key, task in plan_extraction_prompts.items():\n",
    "        task_prompt = (\n",
    "            f\"<|start_header_id|>user<|end_header_id|>\\n\\n\" \n",
    "            f\"{task}\"\n",
    "            f\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "        )\n",
    "        answer, returned_cache = run_model_function(\n",
    "            task_prompt, \n",
    "            model, \n",
    "            tokenizer, \n",
    "            KEYPOINT_CONFIG, \n",
    "            kv_cache=base_cache\n",
    "        )\n",
    "        del returned_cache\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        # try:\n",
    "        #     clean_answer = answer.strip().strip(\"```json\").strip(\"```\").strip()\n",
    "        #     keypoints[key] = json.loads(clean_answer)\n",
    "        # except json.JSONDecodeError:\n",
    "        #     keypoints[key] = answer\n",
    "                # check\n",
    "        verification_prompt = (\n",
    "            f\"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "            f\"CONTEXT: You previously extracted the following information (Initial Answer):\\n\"\n",
    "            f\"--- BEGIN INITIAL ANSWER ---\\n{answer}\\n--- END INITIAL ANSWER ---\\n\\n\"\n",
    "            f\"**CRITICAL TASK:** You must now act as a verifier. Review the Initial Answer against the original full medical note (which is stored in your memory/context).\\n\"\n",
    "            f\"1. **Faithfulness Check:** Check if every statement in the Initial Answer is strictly supported by the original medical note.\\n\"\n",
    "            f\"2. **Revision:** Generate a **Final Answer** by removing *any* part of the Initial Answer that is not supported by the original medical note. If the Initial Answer is fully supported, the Final Answer should be the same.\\n\"\n",
    "            f\"Return the result strictly as a JSON object with the key 'final_answer'.\\n\"\n",
    "            f\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "        )\n",
    "\n",
    "        final_answer_raw, _ = run_model_function(\n",
    "            verification_prompt, model, tokenizer, KEYPOINT_CONFIG, kv_cache=base_cache\n",
    "        )\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        try:\n",
    "            final_result = json.loads(final_answer_raw.strip().strip(\"```json\").strip(\"```\").strip())\n",
    "            keypoints[key] = final_result.get('final_answer', answer)\n",
    "        except json.JSONDecodeError:\n",
    "            keypoints[key] = answer\n",
    "\n",
    "    print('keypoints from plan_extraction_prompts')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # # explain\n",
    "    # explain_base_prompt = (\n",
    "    #     f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n\"\n",
    "    #     f\"You are an experienced and compassionate Oncologist and medical educator. Your primary role is to translate complex medical information into clear, 8th-grade level English. \"\n",
    "    #     f\"Your task is to answer a series of questions about it, one by one. \"\n",
    "    #     f'''### STRICT NEGATIVE CONSTRAINTS:\n",
    "    #     * Do not say anything not present in the medical note.\n",
    "    #     * If cancer has spread (metastasis), DO NOT list the specific organs affected. Say \"the cancer has spread to other parts of the body.\"\n",
    "    #     * Do not use fatalistic language. The focus MUST be on quality of life.\n",
    "    #     *NO ADDING SIDE EFFECTS, Unless they appear in the keypoints exactly.\n",
    "    #     '''\n",
    "    #     f\"<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "    #     f\"Here is the medical note:\\n\\n\"\n",
    "    #     f\"--- BEGIN NOTE ---\\n{note_text}\\n--- END NOTE ---\"\n",
    "    #     f\"Here is the keypoints extracted from the medical note:\\n\\n\"\n",
    "    #     f\"--- BEGIN KEYPOINTS ---\\n{keypoints}\\n--- END KEYPOINTS ---\"\n",
    "    #     f\"\\n\\nI will now ask you to extract specific sections. \"\n",
    "    #     f\"Please wait for my first extraction task.\"\n",
    "    #     f\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "    #     f\"{{\\\"status\\\": \\\"Understood. I have read the note and am ready.\\\"}}\"\n",
    "    # )\n",
    "\n",
    "    # with torch.no_grad():\n",
    "    #     inputs = tokenizer(explain_base_prompt, return_tensors=\"pt\").to(model.device)\n",
    "    #     outputs = model(\n",
    "    #         input_ids=inputs[\"input_ids\"], \n",
    "    #         attention_mask=inputs[\"attention_mask\"],\n",
    "    #         use_cache=True\n",
    "    #     )\n",
    "    #     base_cache = outputs.past_key_values\n",
    "    #     del inputs, outputs\n",
    "    #     torch.cuda.empty_cache()\n",
    "    #     gc.collect()\n",
    "\n",
    "    # explain = {}\n",
    "    # run_model_function = run_model_with_cache_manual \n",
    "    # for key, task in explain_prompts.items():\n",
    "    #     task_prompt = (\n",
    "    #         f\"<|start_header_id|>user<|end_header_id|>\\n\\n\" \n",
    "    #         f\"{task}\"\n",
    "    #         f\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "    #     )\n",
    "    #     answer, returned_cache = run_model_function(\n",
    "    #         task_prompt, \n",
    "    #         model, \n",
    "    #         tokenizer, \n",
    "    #         KEYPOINT_CONFIG, \n",
    "    #         kv_cache=base_cache\n",
    "    #     )\n",
    "    #     del returned_cache\n",
    "    #     torch.cuda.empty_cache()\n",
    "    #     gc.collect()\n",
    "        \n",
    "    #     try:\n",
    "    #         clean_answer = answer.strip().strip(\"```json\").strip(\"```\").strip()\n",
    "    #         explain[key] = json.loads(clean_answer)\n",
    "    #     except json.JSONDecodeError:\n",
    "    #         explain[key] = answer\n",
    "\n",
    "    # print('explain')\n",
    "\n",
    "    \n",
    "\n",
    "    # print('explain')\n",
    "    # explanation_prompt= create_explanation_prompt(note_text, keypoints)\n",
    "    # explanation,_  = run_model_function(explanation_prompt, model, tokenizer, EXPLANATION_GENERATION_CONFIG)\n",
    "\n",
    "    # # clean up\n",
    "    # print('clean')\n",
    "    # cleaning_prompt = create_cleaning_prompt(explanation)\n",
    "    # final_result,_  = run_model_function(cleaning_prompt, model, tokenizer, CLEANING_CONFIG)\n",
    "    \n",
    "    row_result = {\n",
    "        'coral_idx': row['coral_idx'],\n",
    "        'note_text': note_text,\n",
    "        'assesment_and_plan': assesment_and_plan,\n",
    "        'keypoints': keypoints,\n",
    "        # 'explain': explain,\n",
    "        # 'raw_result': explanation,\n",
    "        # 'clean_result': final_result\n",
    "    }\n",
    "    \n",
    "    all_results[index]=(row_result)\n",
    "\n",
    "    # # print here\n",
    "    # print('\\n'*5)\n",
    "    # print(f\"\\n{'='*60}\")\n",
    "    # print(f\"RESULTS FOR ROW {index + 1}\")\n",
    "    # print(f\"{'='*60}\")\n",
    "    \n",
    "    # for col in ['assesment_and_plan','keypoints', ]:\n",
    "    #     print(f\"\\n--- Column: {col} ---\")\n",
    "    #     original_text = row_result[col]\n",
    "    #     try:\n",
    "    #         (print_json((original_text)))\n",
    "    #     except:\n",
    "    #         print(original_text)\n",
    "\n",
    "\n",
    "    # write to txt file\n",
    "    with open('results.txt', 'a') as f:  # 'a' to append, 'w' to overwrite\n",
    "        f.write('\\n' * 5)\n",
    "        f.write(f\"\\n{'='*60}\\n\")\n",
    "        f.write(f\"RESULTS FOR ROW {index + 1}\\n\")\n",
    "        f.write(f\"{'='*60}\\n\")\n",
    "        \n",
    "        for col in row_result.keys():\n",
    "            f.write(f\"\\n--- Column: {col} ---\\n\")\n",
    "            original_text = row_result[col]\n",
    "            try:\n",
    "                import json\n",
    "                f.write(json.dumps(original_text, indent=2) + '\\n')\n",
    "            except:\n",
    "                f.write(str(original_text) + '\\n')\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
