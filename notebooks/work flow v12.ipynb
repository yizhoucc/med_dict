{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b2c708e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yc/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "with open(\"hf.token\", \"r\") as f:\n",
    "    hftoken = f.read().strip()  \n",
    "\n",
    "import os\n",
    "cache_dir = \"/mnt/c/Users/yc/.cache/huggingface\"\n",
    "os.environ['HF_HOME'] = cache_dir\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(token=hftoken)  # Move token to environment variable\n",
    "\n",
    "from ult import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dd4555-473d-48cd-bc86-60a4c41da0a8",
   "metadata": {},
   "source": [
    "## load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d74a59c-b625-4944-8a79-aea69bc6cd8a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 19/19 [03:28<00:00, 10.96s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_name = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",          # Use NF4 (a highly-performant 4-bit format)\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, # Use bfloat16 for faster computation\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "device_map = {\"\": 0}\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=device_map,\n",
    "    quantization_config=quantization_config,  # Pass the config here\n",
    "    # We are NOT using load_in_8bit=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f063ce1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running model with prompt: 'What is the difference between chemotherapy and immunotherapy for cancer treatment?'\n",
      "\n",
      "--- Model Response ---\n",
      "Chemotherapy and immunotherapy are both common types of cancer treatments, but they work in fundamentally different ways:\n",
      "\n",
      "1. Chemotherapy: This is a type of cancer treatment that uses drugs to kill cancer cells or stop them from dividing. Chemotherapy can be administered intravenously, orally, or topically. It is often used to treat advanced-stage cancers, reduce the size of tumors before surgery, or kill any remaining cancer cells after other treatments. However, chemotherapy can also damage healthy cells, leading to side effects such as hair loss, nausea, and fatigue.\n",
      "\n",
      "2. Immunotherapy: This is a relatively newer form of cancer treatment that uses the body's own immune system to fight cancer cells. Immunotherapy works by boosting, directing, or restoring the natural defenses of the immune system. There are several types of immunotherapy, including checkpoint inhibitors, monoclonal antibodies, and CAR T-cell therapy. Immunotherapy is often used to treat cancers that have spread to other parts of the body or when other treatments have not been successful. Unlike chemotherapy, immunotherapy tends\n",
      "--- End Response ---\n",
      "\n",
      "Generation took 38.46 seconds.\n",
      "Approximate speed: 6.53 tokens/second.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from typing import Dict, List, Optional # Added List, Optional\n",
    "\n",
    "def run_model(\n",
    "    prompt: str,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    generation_config: Optional[Dict] = None, # Make config optional\n",
    "    chat_history: Optional[List[Dict]] = None # Optional chat history\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Runs the loaded Hugging Face model to generate text based on a prompt,\n",
    "    handling attention masks and pad tokens correctly.\n",
    "\n",
    "    Args:\n",
    "        prompt: The latest user input text prompt for the model.\n",
    "        model: The loaded Hugging Face CausalLM model.\n",
    "        tokenizer: The loaded Hugging Face tokenizer.\n",
    "        generation_config: A dictionary containing parameters for model.generate()\n",
    "                           (e.g., max_new_tokens, temperature, do_sample).\n",
    "        chat_history: An optional list of previous chat messages in the format\n",
    "                      [{\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]\n",
    "\n",
    "    Returns:\n",
    "        The generated text string from the assistant.\n",
    "    \"\"\"\n",
    "    # --- Configuration Handling ---\n",
    "    if generation_config is None:\n",
    "        generation_config = {}\n",
    "\n",
    "    # Set default max_new_tokens if not provided, sensible default for testing\n",
    "    if \"max_new_tokens\" not in generation_config:\n",
    "        generation_config[\"max_new_tokens\"] = 250 # Increased default slightly\n",
    "\n",
    "    # --- Pad Token Handling ---\n",
    "    # Ensure pad_token_id is set. Using eos_token_id is common for autoregressive models.\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "        # Also update the model config if possible (might not be needed for inference only)\n",
    "        # model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    # Add pad_token_id to generation_config if not already present\n",
    "    if \"pad_token_id\" not in generation_config:\n",
    "         generation_config[\"pad_token_id\"] = tokenizer.pad_token_id\n",
    "\n",
    "    # --- Chat History and Input Formatting ---\n",
    "    if chat_history is None:\n",
    "        chat_history = []\n",
    "\n",
    "    # Add the current user prompt to the history\n",
    "    messages = chat_history + [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "    # Apply the chat template to format the input correctly for Mixtral Instruct\n",
    "    # return_tensors=\"pt\" ensures PyTorch tensors are returned\n",
    "    # add_generation_prompt=True adds the necessary tokens to signal assistant response start\n",
    "    # Explicitly handle padding and truncation if needed, though usually handled by generate\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        return_tensors=\"pt\",\n",
    "        add_generation_prompt=True\n",
    "        # padding=True, # Consider adding if batching > 1, may not be needed here\n",
    "        # truncation=True, # Consider if prompts can exceed model max length\n",
    "        # max_length=model.config.max_position_embeddings # Or a specific length\n",
    "    ).to(model.device)\n",
    "\n",
    "    # Get the attention mask from the tokenizer output\n",
    "    # `apply_chat_template` usually creates this when return_tensors='pt'\n",
    "    attention_mask = inputs.attention_mask if hasattr(inputs, 'attention_mask') else torch.ones_like(inputs)\n",
    "\n",
    "\n",
    "    # Get the length of the input tokens *before* generation\n",
    "    input_length = inputs.shape[1]\n",
    "\n",
    "    # --- Generation ---\n",
    "    with torch.no_grad(): # Disable gradient calculations for inference\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs,\n",
    "            attention_mask=attention_mask, # Explicitly pass the attention mask\n",
    "            # pad_token_id=tokenizer.pad_token_id, # REMOVED: Pass via **generation_config\n",
    "            **generation_config # Pass generation parameters including pad_token_id\n",
    "        )\n",
    "\n",
    "    # --- Decoding ---\n",
    "    # Extract only the newly generated tokens (after the input)\n",
    "    response_tokens = outputs[0][input_length:]\n",
    "\n",
    "    # Decode the response tokens into a string, skipping special tokens like <eos>\n",
    "    raw_output = tokenizer.decode(response_tokens, skip_special_tokens=True)\n",
    "\n",
    "    return raw_output.strip() # Added strip() to remove leading/trailing whitespace\n",
    "\n",
    "\n",
    "# --- Example Usage (requires model and tokenizer to be loaded) ---\n",
    "# Assuming 'model' and 'tokenizer' are loaded from your 4-bit Mixtral script:\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "\n",
    "    test_prompt = \"What is the difference between chemotherapy and immunotherapy for cancer treatment?\"\n",
    "\n",
    "    # Define generation parameters\n",
    "    gen_config = {\n",
    "        \"max_new_tokens\": 250,\n",
    "        \"temperature\": 0.6, # Slightly lower temp for more focused answer\n",
    "        \"do_sample\": True,\n",
    "        \"top_p\": 0.9,\n",
    "        \"eos_token_id\": tokenizer.eos_token_id,\n",
    "        \"pad_token_id\": tokenizer.eos_token_id # Ensure consistency\n",
    "    }\n",
    "\n",
    "    # --- Optional Chat History Example ---\n",
    "    history = [\n",
    "        # {\"role\": \"user\", \"content\": \"What is cancer?\"},\n",
    "        # {\"role\": \"assistant\", \"content\": \"Cancer is a disease where cells grow uncontrollably...\"}\n",
    "    ]\n",
    "    # ------------------------------------\n",
    "\n",
    "    print(f\"\\nRunning model with prompt: '{test_prompt}'\")\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        response = run_model(test_prompt, model, tokenizer, gen_config, chat_history=history)\n",
    "        end_time = time.time()\n",
    "        duration = end_time - start_time\n",
    "        num_tokens = len(tokenizer.encode(response)) # Rough token count\n",
    "\n",
    "        print(\"\\n--- Model Response ---\")\n",
    "        print(response)\n",
    "        print(\"--- End Response ---\")\n",
    "        print(f\"\\nGeneration took {duration:.2f} seconds.\")\n",
    "        if duration > 0:\n",
    "            print(f\"Approximate speed: {num_tokens / duration:.2f} tokens/second.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during generation: {e}\")\n",
    "\n",
    "        import traceback\n",
    "        traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e1c388-57f9-41b0-87d1-25a9ad884430",
   "metadata": {},
   "source": [
    "## data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf6281f4-7ebb-48fd-a08b-4271349ededf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def txt_to_dict(file_path):\n",
    "    data_dict = {}\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        for i in range(0, len(lines) - 1, 2):\n",
    "            key = lines[i].strip()    # Odd line are key\n",
    "            value = lines[i + 1].strip()  # Even line are value\n",
    "            data_dict[key] = value\n",
    "\n",
    "    return data_dict\n",
    "\n",
    "txt_file_path = 'data/formaldef.txt'\n",
    "formaldic = txt_to_dict(txt_file_path)\n",
    "len(formaldic)\n",
    "\n",
    "meddict={}\n",
    "for k,v in formaldic.items():\n",
    "    meddict[k.split('Listen to pronunciation')[0].split('(')[0]]=v\n",
    "filename= 'data/filtered_medical_dictionary.csv'\n",
    "eighth_grade_words=set()\n",
    "with open(filename, 'r', encoding='utf-8') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    next(reader)  # Skip the header row\n",
    "    for row in reader:\n",
    "        if row:  # Make sure row is not empty\n",
    "            eighth_grade_words.add(row[0])  # Add the word (first column)\n",
    "filtered_meddict = {word: explanation for word, explanation in meddict.items() \n",
    "                   if word in eighth_grade_words}\n",
    "meddict=filtered_meddict\n",
    "# load data\n",
    "df = pd.read_csv('data/CORAL/coral-expert-curated-medical-oncology-reports-to-advance-language-model-inference-1.0/coral/unannotated/data/breastca_unannotated.csv')\n",
    "df = df.sample(1, random_state=42) \n",
    "# text=df.iloc[0]['note_text']\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a445b4-0a2b-45d1-9892-42f775ef20b8",
   "metadata": {},
   "source": [
    "# prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "516413a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing row 84/1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "/home/yc/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:638: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 323\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;66;03m# --- Step 4: Generate Initial Explanation ---\u001b[39;00m\n\u001b[1;32m    322\u001b[0m explanation_prompt \u001b[38;5;241m=\u001b[39m create_explanation_prompt(annotated_text, keysummary)\n\u001b[0;32m--> 323\u001b[0m explanation \u001b[38;5;241m=\u001b[39m \u001b[43mrun_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexplanation_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEXPLANATION_GENERATION_CONFIG\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;66;03m# --- Step 5: Clean and Finalize the Explanation ---\u001b[39;00m\n\u001b[1;32m    326\u001b[0m cleaning_prompt \u001b[38;5;241m=\u001b[39m create_cleaning_prompt(explanation)\n",
      "Cell \u001b[0;32mIn[5], line 292\u001b[0m, in \u001b[0;36mrun_model\u001b[0;34m(prompt, model, tokenizer, generation_config)\u001b[0m\n\u001b[1;32m    289\u001b[0m input_length \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 292\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    297\u001b[0m response_tokens \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m][input_length:]\n\u001b[1;32m    298\u001b[0m raw_output \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(response_tokens, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/generation/utils.py:2215\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2207\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2208\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2209\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2210\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2211\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2212\u001b[0m     )\n\u001b[1;32m   2214\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2215\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2216\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2220\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2222\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2223\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2225\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2226\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2227\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2228\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2229\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2234\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2235\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/generation/utils.py:3206\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3203\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   3205\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 3206\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3208\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3209\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3210\u001b[0m     outputs,\n\u001b[1;32m   3211\u001b[0m     model_kwargs,\n\u001b[1;32m   3212\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3213\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/mixtral/modeling_mixtral.py:1287\u001b[0m, in \u001b[0;36mMixtralForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, output_router_logits, return_dict, cache_position, num_logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[1;32m   1284\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1286\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1287\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1288\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1289\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1292\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1293\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1294\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1295\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1296\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_router_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_router_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1299\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1301\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1302\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/mixtral/modeling_mixtral.py:1002\u001b[0m, in \u001b[0;36mMixtralModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, output_router_logits, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    990\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    991\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    992\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    999\u001b[0m         cache_position,\n\u001b[1;32m   1000\u001b[0m     )\n\u001b[1;32m   1001\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1002\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1003\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1004\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1005\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_router_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_router_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1013\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/mixtral/modeling_mixtral.py:742\u001b[0m, in \u001b[0;36mMixtralDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, output_router_logits, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    740\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    741\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 742\u001b[0m hidden_states, router_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock_sparse_moe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    743\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    745\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/mixtral/modeling_mixtral.py:663\u001b[0m, in \u001b[0;36mMixtralSparseMoeBlock.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    661\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m expert_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_experts):\n\u001b[1;32m    662\u001b[0m     expert_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperts[expert_idx]\n\u001b[0;32m--> 663\u001b[0m     idx, top_x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpert_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mexpert_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    665\u001b[0m     \u001b[38;5;66;03m# Index the correct hidden states and compute the expert hidden state for\u001b[39;00m\n\u001b[1;32m    666\u001b[0m     \u001b[38;5;66;03m# the current expert. We need to make sure to multiply the output hidden\u001b[39;00m\n\u001b[1;32m    667\u001b[0m     \u001b[38;5;66;03m# states by `routing_weights` on the corresponding tokens (top-1 and top-2)\u001b[39;00m\n\u001b[1;32m    668\u001b[0m     current_state \u001b[38;5;241m=\u001b[39m hidden_states[\u001b[38;5;28;01mNone\u001b[39;00m, top_x]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, hidden_dim)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "        \n",
    "def clean_model_output(text: str, fix_incomplete=True) -> str:\n",
    "    \"\"\"\n",
    "    Clean up model-generated text with common fixes, not currently using\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Basic cleanup\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Normalize all whitespace\n",
    "    \n",
    "    # Fix paragraph spacing\n",
    "    text = re.sub(r'\\n\\s*\\n\\s*\\n+', '\\n\\n', text)\n",
    "    \n",
    "    # Fix punctuation spacing\n",
    "    text = re.sub(r'\\s+([,.!?;:])', r'\\1', text)\n",
    "    text = re.sub(r'([.!?])\\s*([A-Z])', r'\\1 \\2', text)\n",
    "    \n",
    "    # Remove repetitive patterns (simple version)\n",
    "    lines = text.split('\\n')\n",
    "    cleaned_lines = []\n",
    "    for line in lines:\n",
    "        if not cleaned_lines or line.strip() != cleaned_lines[-1].strip():\n",
    "            cleaned_lines.append(line)\n",
    "    text = '\\n'.join(cleaned_lines)\n",
    "    \n",
    "    # Handle incomplete sentences\n",
    "    if fix_incomplete and text and not text.endswith(('.', '!', '?')):\n",
    "        sentences = re.split(r'[.!?]+', text)\n",
    "        if len(sentences) > 1 and len(sentences[-1].strip()) < 10:\n",
    "            # Remove likely incomplete last sentence\n",
    "            text = '.'.join(sentences[:-1]) + '.'\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def extract_medical_terms(text: str, meddict: Dict[str, str]) -> Dict[str, str]:\n",
    "\n",
    "    found_terms = {}\n",
    "    \n",
    "    # Strategy 1: Single words\n",
    "    words = re.findall(r'\\b[A-Za-z]+(?:[-\\'][A-Za-z]+)*\\b', text)\n",
    "    for word in words:\n",
    "        definition = find_term_in_dict(word, meddict)\n",
    "        if definition:\n",
    "            found_terms[word] = definition\n",
    "    \n",
    "    # Strategy 2: Multi-word terms\n",
    "    for n in range(2, 6):\n",
    "        n_grams = get_n_grams(text, n)\n",
    "        for phrase in n_grams:\n",
    "            definition = find_term_in_dict(phrase, meddict)\n",
    "            if definition:\n",
    "                found_terms[phrase] = definition\n",
    "    \n",
    "    # Strategy 3: Medical abbreviations\n",
    "    abbreviations = re.findall(r'\\b[A-Z]{2,8}\\b', text)\n",
    "    for abbrev in abbreviations:\n",
    "        definition = find_term_in_dict(abbrev, meddict)\n",
    "        if definition:\n",
    "            found_terms[abbrev] = definition\n",
    "    \n",
    "    # Strategy 4: Medical procedures and conditions with specific patterns\n",
    "    medical_patterns = [\n",
    "        r'\\b\\w+oscopy\\b',          # bronchoscopy, endoscopy, etc.\n",
    "        r'\\b\\w+ectomy\\b',          # appendectomy, etc.\n",
    "        r'\\b\\w+itis\\b',            # bronchitis, arthritis, etc.\n",
    "        r'\\b\\w+osis\\b',            # fibrosis, stenosis, etc.\n",
    "        r'\\b\\w+emia\\b',            # anemia, septicemia, etc.\n",
    "        r'\\b\\w+pathy\\b',           # myopathy, neuropathy, etc.\n",
    "        r'\\b\\w+malacia\\b',         # tracheomalacia, etc.\n",
    "    ]\n",
    "    \n",
    "    for pattern in medical_patterns:\n",
    "        matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "        for match in matches:\n",
    "            definition = find_term_in_dict(match, meddict)\n",
    "            if definition:\n",
    "                found_terms[match] = definition\n",
    "    \n",
    "    # Strategy 5: Medication names\n",
    "    medication_patterns = [\n",
    "        r'\\b\\w+cillin\\b',          # penicillin, amoxicillin, etc.\n",
    "        r'\\b\\w+mycin\\b',           # streptomycin, etc.\n",
    "        r'\\b\\w+floxacin\\b',        # levofloxacin, ciprofloxacin, etc.\n",
    "        r'\\b\\w+sone\\b',            # prednisone, cortisone, etc.\n",
    "        r'\\b\\w+pam\\b',             # lorazepam, etc.\n",
    "    ]\n",
    "    \n",
    "    for pattern in medication_patterns:\n",
    "        matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "        for match in matches:\n",
    "            definition = find_term_in_dict(match, meddict)\n",
    "            if definition:\n",
    "                found_terms[match] = definition\n",
    "    \n",
    "    return found_terms\n",
    "\n",
    "\n",
    "def get_n_grams(text: str, n: int) -> List[str]:\n",
    "    \"\"\"Generate n-grams from text.\"\"\"\n",
    "    words = re.findall(r'\\b[A-Za-z]+\\b', text.lower())\n",
    "    n_grams = []\n",
    "    for i in range(len(words) - n + 1):\n",
    "        phrase = ' '.join(words[i:i+n])\n",
    "        n_grams.append(phrase)\n",
    "    return n_grams\n",
    "\n",
    "\n",
    "def find_term_in_dict(term: str, meddict: Dict[str, str]) -> Optional[str]:\n",
    "    \"\"\"Find term in medical dictionary.\"\"\"\n",
    "    search_formats = [\n",
    "        term, term.lower(), term.upper(), term.title(), term.capitalize()\n",
    "    ]\n",
    "    \n",
    "    for search_term in search_formats:\n",
    "        if search_term in meddict:\n",
    "            return meddict[search_term]\n",
    "    \n",
    "    # Partial matching\n",
    "    for key in meddict.keys():\n",
    "        if key.lower() == term.lower():\n",
    "            return meddict[key]\n",
    "    \n",
    "    return None\n",
    "\n",
    "def create_annotated_text(text: str, meddict: Dict[str, str]) -> str:\n",
    "\n",
    "    # 1. Use the existing function to find all unique terms and their definitions.\n",
    "    found_terms = extract_medical_terms(text, meddict)\n",
    "    \n",
    "    # 2. Sort terms by length in descending order to handle overlaps.\n",
    "    # This is critical for terms like \"cerebral palsy\" and \"palsy\".\n",
    "    sorted_terms = sorted(found_terms.keys(), key=len, reverse=True)\n",
    "    \n",
    "    annotated_text = text\n",
    "    \n",
    "    # 3. Iterate and replace.\n",
    "    for term in sorted_terms:\n",
    "        definition = found_terms[term]\n",
    "        annotation = f\"{term} [DEFINITION: {definition}]\"\n",
    "        pattern = r'\\b' + re.escape(term) + r'\\b'\n",
    "        annotated_text = re.sub(pattern, annotation, annotated_text, count=1, flags=re.IGNORECASE)\n",
    "        \n",
    "    return annotated_text\n",
    "\n",
    "\n",
    "def create_key_summary_prompt(original_text: str) -> str:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"\"\"You are an expert medical information extractor with exceptional attention to detail. Your task is to carefully read the medical text and extract ONLY the key factual details that are explicitly mentioned. You must be extremely precise and never infer, assume, or add any information not directly stated in the text.\n",
    "CRITICAL RULES:\n",
    "1. Extract ONLY facts explicitly stated in the text\n",
    "2. Use the EXACT wording from the original text when possible\n",
    "3. Do not interpret, infer, or elaborate beyond what is written\n",
    "4. If multiple items exist in a category, separate them with semicolons\n",
    "5. Keep each section concise but complete\n",
    "6. If a category is not mentioned or unclear, write \"Not mentioned\" for that section.\"\"\"},\n",
    "        {\"role\": \"user\", \"content\": f\"\"\"**Medical Text:**\\n\"{original_text}\"\\n\\n**Format your response EXACTLY as follows:**\\n\\n**TREATMENTS RECEIVED:**\\n[List treatments]\\n\\n**MEDICAL CONDITIONS:**\\n[List conditions]\\n\\n**CANCER STAGE:**\\n[List stage]\\n\\n**REFERRALS:**\\n[List referrals]\\n\\n**CURRENT STATUS:**\\n[List status]\\n\\n**NEXT STEPS/PLAN:**\\n[List plan]\"\"\"}\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "\n",
    "def create_explanation_prompt(annotated_text: str, keysummary: str) -> str:\n",
    "\n",
    "    audience_instruction = \"The determined audience for this letter is the **patient**. You must address them directly as 'you' throughout the entire letter.\"\n",
    "    \n",
    "    system_prompt = f\"\"\"### Persona\n",
    "You are an experienced and compassionate Oncologist and medical educator. Your primary role is to translate complex medical information into clear, 8th-grade level English. Your tone should be professional, empathetic, and honest.\n",
    "\n",
    "### Golden Rule: Radical Simplicity - Translate, Don't Transfer\n",
    "Your single most important task is to convert medical terminology into simple explanations. Do not just define a term; replace it with an easy-to-understand explanation.\n",
    "\n",
    "### Your Task:\n",
    "Write a complete letter explaining the information from the medical text. Imagine you are sitting with the recipient and explaining this to them in person.\n",
    "\n",
    "{audience_instruction}\n",
    "\n",
    "### Letter Structure and Flow:\n",
    "Organize your letter logically using the following question-based headers:\n",
    "* **Empathetic Opening:** Start with a warm and supportive salutation.\n",
    "* **Why you came today:** Briefly explain the purpose of the visit in plain words.\n",
    "* **What you told us:** Summarize what the patient and family shared (symptoms, worries, questions).\n",
    "* **What did we find** Explain what the team learned from today's visit (exam, blood work, scans) in simple terms.\n",
    "* **What is the plan** List the next steps (treatment, monitoring, follow-up) in plain language.\n",
    "* **Closing with Support:** End by reinforcing your support. Limit to one sentence maximum.\n",
    "\n",
    "### STRICT NEGATIVE CONSTRAINTS:\n",
    "* Do not say anything not present in the medical note.\n",
    "* If cancer has spread (metastasis), DO NOT list the specific organs affected. Say \"the cancer has spread to other parts of the body.\"\n",
    "* Do not use fatalistic language. The focus MUST be on quality of life.\n",
    "\n",
    "### Strict Output Formatting:\n",
    "Provide ONLY the letter. Your output must start directly with \"Dear [Patient Name],\" and end with \"Sincerely,\\nYour Care Team at [Institution]\". Use the question-based headers exactly as specified.\"\"\"\n",
    "    \n",
    "    user_prompt = f\"\"\"### Original Medical Text:\n",
    "\"{annotated_text}\"\n",
    "\n",
    "### Internal Fact-Checking Reference (Translate these facts, do not copy them):\n",
    "\"{keysummary}\"\n",
    "\n",
    "Please provide the patient-focused explanation now. Your output must start directly with the salutation.\"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "\n",
    "\n",
    "def create_cleaning_prompt(raw_response: str) -> str:\n",
    "    \"\"\"\n",
    "    Creates the correctly formatted prompt for Llama 3 to clean/revise a response.\n",
    "    \"\"\"\n",
    "    system_prompt = f\"\"\"### Persona\n",
    "You are an expert medical writer and editor. Your unique skill is communicating complex clinical information with absolute precision and clarity. Your primary directive is to preserve the original meaning without fail.\n",
    "\n",
    "### Target Audience: patient\n",
    "\n",
    "### Your Task\n",
    "Revise the provided medical text to improve its quality in the following areas:\n",
    "1.  **Clean Up Language:** Improve sentence structure and use professional language.\n",
    "2.  **Reduce Repetition:** Eliminate redundant words and phrases.\n",
    "3.  **Improve Flow:** Enhance the logical flow and transitions.\n",
    "4.  **Define Medical Terms** For any complex medical term, replace it with a simple explanation. Example: \"you had tachycardia.\"--> \"you had a fast heart rate\".\n",
    "5.  **Remove formatting subtitles:** remove any section headers or subtitles from the text to make it read as a continuous letter.\n",
    "### The Golden Rule: Preserve Clinical Meaning at All Costs\n",
    "The revised text MUST be factually identical to the original.\n",
    "* **DO NOT** alter any clinical facts, diagnoses, measurements, or timelines.\n",
    "* **DO NOT** change the certainty of a statement.\n",
    "\n",
    "### Instructions for Output\n",
    "Provide your Revised Medical Text: The complete, revised version of the text.\"\"\"\n",
    "    user_prompt = f\"\"\"### Medical Text to Revise:\n",
    "{raw_response}\"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "    \n",
    "    return tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "\n",
    "KEY_SUMMARY_CONFIG = {\n",
    "    \"max_new_tokens\": 500,\n",
    "    \"do_sample\": False, \n",
    "    \"repetition_penalty\": 1.1,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "    \"eos_token_id\": terminators, \n",
    "}\n",
    "\n",
    "EXPLANATION_GENERATION_CONFIG = {\n",
    "    \"max_new_tokens\": 1024,\n",
    "    \"temperature\": 0.3,\n",
    "    \"top_p\": 0.85,\n",
    "    \"repetition_penalty\": 1.15,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "    \"eos_token_id\": terminators, \n",
    "    \"early_stopping\": True,\n",
    "}\n",
    "\n",
    "CLEANING_CONFIG = {\n",
    "    \"max_new_tokens\": 1500,\n",
    "    \"temperature\": 0.1,\n",
    "    \"top_p\": 0.9,\n",
    "    \"repetition_penalty\": 1.1,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "    \"eos_token_id\": terminators, \n",
    "    \"early_stopping\": True,\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def run_model(prompt: str, model, tokenizer, generation_config: Dict) -> str:\n",
    "\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(model.device)\n",
    "        input_length = inputs[\"input_ids\"].shape[1]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                **generation_config\n",
    "            )\n",
    "        response_tokens = outputs[0][input_length:]\n",
    "        raw_output = tokenizer.decode(response_tokens, skip_special_tokens=True)\n",
    "        return raw_output\n",
    "        \n",
    "\n",
    "\n",
    "all_results = {}\n",
    "LINE_WIDTH = 100 \n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    text = row['note_text'] \n",
    "    \n",
    "    print(f\"\\nProcessing row {index + 1}/{len(df)}...\")\n",
    "    \n",
    "    # --- Step 1: Determine Audience ---\n",
    "    audience = 'patient' \n",
    "    \n",
    "    # --- Step 2: Annotate Text (Non-LLM step) ---\n",
    "    annotated_text = create_annotated_text(text, meddict)\n",
    "    \n",
    "    # --- Step 3: Extract Key Summary ---\n",
    "    summary_prompt = create_key_summary_prompt(text)\n",
    "    keysummary = run_model(summary_prompt, model, tokenizer, KEY_SUMMARY_CONFIG)\n",
    "    \n",
    "    # --- Step 4: Generate Initial Explanation ---\n",
    "    explanation_prompt = create_explanation_prompt(annotated_text, keysummary)\n",
    "    explanation = run_model(explanation_prompt, model, tokenizer, EXPLANATION_GENERATION_CONFIG)\n",
    "\n",
    "    # --- Step 5: Clean and Finalize the Explanation ---\n",
    "    cleaning_prompt = create_cleaning_prompt(explanation)\n",
    "    final_result = run_model(cleaning_prompt, model, tokenizer, CLEANING_CONFIG)\n",
    "    \n",
    "    # --- Store and Print Results ---\n",
    "    row_result = {\n",
    "        'original_text': text,\n",
    "        # 'annotated_text': annotated_text,\n",
    "        'keysummary': keysummary,\n",
    "        'raw_explanation': explanation,\n",
    "        'final_letter': final_result\n",
    "    }\n",
    "    \n",
    "    all_results[index]=(row_result)\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"RESULTS FOR ROW {index + 1}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    for col in ['keysummary', 'raw_explanation', 'final_letter']:\n",
    "        print(f\"\\n--- Column: {col} ---\")\n",
    "        original_text = row_result[col]\n",
    "        wrapped_text = textwrap.fill(original_text, width=LINE_WIDTH)\n",
    "        print(wrapped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fc6234e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved to 'output/output_8.csv'\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame(all_results).T\n",
    "# results_df.drop(columns=['annotated_text'], inplace=True)\n",
    "output_format = 'csv'\n",
    "\n",
    "# --- File Saving Logic ---\n",
    "base_filename = 'output/output'\n",
    "extension = ''\n",
    "\n",
    "if output_format == 'csv':\n",
    "    extension = '.csv'\n",
    "elif output_format == 'json':\n",
    "    extension = '.json'\n",
    "else:\n",
    "    print(f\"Error: Unsupported format '{output_format}'. Please choose 'csv' or 'json'.\")\n",
    "    exit() # Exits the script if the format is invalid\n",
    "\n",
    "\n",
    "output_filename = base_filename + extension\n",
    "counter = 1\n",
    "\n",
    "output_dir = os.path.dirname(base_filename)\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "while os.path.exists(output_filename):\n",
    "    output_filename = f\"{base_filename}_{counter}{extension}\"\n",
    "    counter += 1\n",
    "\n",
    "if output_format == 'csv':\n",
    "    results_df.to_csv(output_filename, index=True)\n",
    "elif output_format == 'json':\n",
    "    results_df.to_json(output_filename, orient='index', indent=2)\n",
    "\n",
    "print(f\"DataFrame saved to '{output_filename}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
