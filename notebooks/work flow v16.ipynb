{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4dd4555-473d-48cd-bc86-60a4c41da0a8",
   "metadata": {},
   "source": [
    "## load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "611f29b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b2c708e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yc/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "with open(\"hf.token\", \"r\") as f:\n",
    "    hftoken = f.read().strip()  \n",
    "\n",
    "import os\n",
    "cache_dir = \"/mnt/c/Users/yc/.cache/huggingface\"\n",
    "os.environ['HF_HOME'] = cache_dir\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(token=hftoken)  # Move token to environment variable\n",
    "\n",
    "from ult import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d74a59c-b625-4944-8a79-aea69bc6cd8a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [01:34<00:00, 23.58s/it]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "device_map = {\"\": 0}\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=device_map,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e1c388-57f9-41b0-87d1-25a9ad884430",
   "metadata": {},
   "source": [
    "## data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cf6281f4-7ebb-48fd-a08b-4271349ededf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coral_idx</th>\n",
       "      <th>Sex</th>\n",
       "      <th>UCSFDerivedRaceEthnicity_X</th>\n",
       "      <th>BirthDate</th>\n",
       "      <th>note_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>184</td>\n",
       "      <td>Female</td>\n",
       "      <td>Latinx</td>\n",
       "      <td>1983-10-04</td>\n",
       "      <td>Medical Oncology Consult Note  Video Consult  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>185</td>\n",
       "      <td>Female</td>\n",
       "      <td>Multi-Race/Ethnicity</td>\n",
       "      <td>1973-03-08</td>\n",
       "      <td>This is a shared service.  Physician Statement...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>193</td>\n",
       "      <td>Female</td>\n",
       "      <td>Multi-Race/Ethnicity</td>\n",
       "      <td>1979-06-20</td>\n",
       "      <td>ID: ***** ***** is a 39 y.o. premenopausal pat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>210</td>\n",
       "      <td>Female</td>\n",
       "      <td>Southwest Asian and North African</td>\n",
       "      <td>1974-04-05</td>\n",
       "      <td>Patient Name: ***** *****  ***** *****: 08/22/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>223</td>\n",
       "      <td>Female</td>\n",
       "      <td>Unknown/Declined</td>\n",
       "      <td>1960-10-12</td>\n",
       "      <td>We performed this consultation using real-time...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    coral_idx     Sex         UCSFDerivedRaceEthnicity_X   BirthDate  \\\n",
       "44        184  Female                             Latinx  1983-10-04   \n",
       "45        185  Female               Multi-Race/Ethnicity  1973-03-08   \n",
       "53        193  Female               Multi-Race/Ethnicity  1979-06-20   \n",
       "70        210  Female  Southwest Asian and North African  1974-04-05   \n",
       "83        223  Female                   Unknown/Declined  1960-10-12   \n",
       "\n",
       "                                            note_text  \n",
       "44  Medical Oncology Consult Note  Video Consult  ...  \n",
       "45  This is a shared service.  Physician Statement...  \n",
       "53  ID: ***** ***** is a 39 y.o. premenopausal pat...  \n",
       "70  Patient Name: ***** *****  ***** *****: 08/22/...  \n",
       "83  We performed this consultation using real-time...  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_file_path = 'data/formaldef.txt'\n",
    "formaldic = txt_to_dict(txt_file_path)\n",
    "len(formaldic)\n",
    "\n",
    "meddict={}\n",
    "for k,v in formaldic.items():\n",
    "    meddict[k.split('Listen to pronunciation')[0].split('(')[0]]=v\n",
    "filename= 'data/filtered_medical_dictionary.csv'\n",
    "eighth_grade_words=set()\n",
    "with open(filename, 'r', encoding='utf-8') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    next(reader)  # Skip the header row\n",
    "    for row in reader:\n",
    "        if row:  # Make sure row is not empty\n",
    "            eighth_grade_words.add(row[0])  # Add the word (first column)\n",
    "filtered_meddict = {word: explanation for word, explanation in meddict.items() \n",
    "                   if word in eighth_grade_words}\n",
    "meddict=filtered_meddict\n",
    "# load data\n",
    "df = pd.read_csv('data/CORAL/coral-expert-curated-medical-oncology-reports-to-advance-language-model-inference-1.0/coral/unannotated/data/breastca_unannotated.csv')\n",
    "# df = df.sample(1, random_state=42)\n",
    "# df=df.iloc[[44,45,53,70,83, 0,1,2,3,4]] \n",
    "df=df.iloc[[44,45,53,70,83]] \n",
    "test_note=df.iloc[0]['note_text']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfa3a9d",
   "metadata": {},
   "source": [
    "# save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "03c3bce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction_prompts = {\n",
    "\n",
    "\"Medication_Plan\": \"\"\"\n",
    "TASK: Extract the 'Medication Plan'. Find the 'Assessment/Plan' section of the note, usually the final section.\n",
    "Include all current and future medication plans for both cancer therapy and supportive treatment. Cancer treatment could be one or many in chemotherapy, hormone therapy, bone therapy, radiotherapy (eg. rad onc, xrt). Supportive treatment could be one or many in bowel regimen, pain medication, psychiatry medication, neuropathy or any blood transfusion plan. \n",
    "\n",
    "Include whether a medication is being started now (e.g,“will start”, “Rx sent”, “starting today” ), plan or discuss in the future after certain condition (e.g., “plan to start after radiation”, “discussed addition of…”), continue or maintained(“continue”), stop or change.\n",
    "\n",
    "Do NOT include procedures, labs, imaging, or genetic testing.\n",
    "For your information, a completed, finished, status post medication, s/p, means past treatments.\n",
    "Include an alterative, second-line or clinical trials options if dicussed.\n",
    "\n",
    "Respond *only* with a JSON object using this exact schema:\n",
    "{\n",
    "    \"The medication/treatment, one of chemotherapy, hormone therapy, bone therapy, radiotherapy\": \n",
    "        {\n",
    "        \"summary\":\"the summary of this type of medication, including start/stop/cotinue if applicable\",\n",
    "        \"Short term side_effects_discussed\": \"short term Side effects of this particular medications.\",\n",
    "        \"Long term side_effects_discussed\": \"long term Side effects of this particular medications.\"\n",
    "        },\n",
    "}\n",
    "\"\"\",\n",
    "\n",
    "\"Medication_Plan old\": \"\"\"\n",
    "TASK: Extract the 'Medication Plan'. Find the 'Assessment/Plan' section of the note, usually the final section.\n",
    "Include future medication plans, changes to current meds (start, stop, continue), supportive meds, bowel regimen, and any blood transfusion plan.\n",
    "Respond *only* with a JSON object using this exact schema:\n",
    "{\n",
    "    \"medication_plan\": \"A summary of the complete medication plan.\"\n",
    "}\n",
    "\"\"\",\n",
    "\n",
    "\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "15fe5d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "res=[]\n",
    "\n",
    "for row in df.itertuples():\n",
    "    test_note=row.note_text\n",
    "    gen_config = {\n",
    "    \"max_new_tokens\": 512,  # <-- Increased from 150 to 512\n",
    "    \"eos_token_id\": tokenizer.eos_token_id,\n",
    "    \"do_sample\": False\n",
    "    }\n",
    "\n",
    "    # myprint(\"\\n--- 1. Creating Base KV Cache from long text... ---\")\n",
    "\n",
    "    base_prompt = (\n",
    "        f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n\"\n",
    "        f\"You are a medical data extraction expert. You will be given a long medical note. \"\n",
    "        f\"Your task is to answer a series of questions about it, one by one. \"\n",
    "        # FIX 2: Stricter instruction in system prompt\n",
    "        f\"Respond *only* with the valid JSON object requested. Do not add markdown backticks or any other text.\"\n",
    "        f\"<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "        f\"Here is the medical note:\\n\\n\"\n",
    "        f\"--- BEGIN NOTE ---\\n{test_note}\\n--- END NOTE ---\"\n",
    "        f\"\\n\\nI will now ask you to extract specific sections. \"\n",
    "        f\"Please wait for my first extraction task.\"\n",
    "        f\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "        f\"{{\\\"status\\\": \\\"Understood. I have read the note and am ready.\\\"}}\"\n",
    "    )\n",
    "\n",
    "    # myprint(\"  Tokenizing and running forward pass to get base cache...\")\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(base_prompt, return_tensors=\"pt\").to(model.device)\n",
    "        \n",
    "        # This logic to create the base_cache is correct\n",
    "        outputs = model(\n",
    "            input_ids=inputs[\"input_ids\"], \n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            use_cache=True\n",
    "        )\n",
    "        \n",
    "        base_cache = outputs.past_key_values\n",
    "        \n",
    "        # Clean up\n",
    "        del inputs, outputs\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    # myprint(\"Base KV Cache created. Ready for 'branching' extractions.\")\n",
    "\n",
    "    # --- FIX 3: Define STRICT SCHEMAS for each extraction task ---\n",
    "    # This is the most important \"prompt engineering\" fix.\n",
    "    # We are telling the model *exactly* what keys to use.\n",
    "\n",
    "\n",
    "    extracted_data = {}\n",
    "\n",
    "    # myprint(\"\\n--- 2. Running EFFICIENT 'Branching' Extractions ---\")\n",
    "\n",
    "    # (Assuming `run_model_with_cache_manual` is your function `run_model_with_cache`)\n",
    "    # If not, please rename this call to `run_model_with_cache`\n",
    "    run_model_function = run_model_with_cache_manual \n",
    "\n",
    "    for key, task in extraction_prompts.items():\n",
    "        # myprint(f\"\\nExtracting: {key}...\")\n",
    "        \n",
    "        # --- FIX 4: Remove the BOS token from the loop ---\n",
    "        # The base_cache *already* contains the BOS token.\n",
    "        # Adding it again can cause errors.\n",
    "        task_prompt = (\n",
    "            f\"<|start_header_id|>user<|end_header_id|>\\n\\n\" # <-- Removed <|begin_of_text|>\n",
    "            f\"{task}\" # <-- Use the new, detailed prompt\n",
    "            f\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "        )\n",
    "\n",
    "        # Use the SAME base_cache for each extraction\n",
    "        answer, returned_cache = run_model_function(\n",
    "            task_prompt, \n",
    "            model, \n",
    "            tokenizer, \n",
    "            gen_config, \n",
    "            kv_cache=base_cache\n",
    "        )\n",
    "        \n",
    "        # Your memory management here is good.\n",
    "        del returned_cache\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        # myprint(f\"  Raw Output: {answer}\")\n",
    "        \n",
    "        try:\n",
    "            # We are now *only* expecting a JSON object\n",
    "            clean_answer = answer.strip().strip(\"```json\").strip(\"```\").strip()\n",
    "            extracted_data[key] = json.loads(clean_answer)\n",
    "        except json.JSONDecodeError:\n",
    "            extracted_data[key] = {\"error\": \"Failed to parse JSON\", \"raw\": answer}\n",
    "\n",
    "    # myprint(\"\\n--- 3. All extractions complete. ---\")\n",
    "    # myprint(\"\\n--- FINAL EXTRACTED DATA ---\")\n",
    "    res.append(json.dumps(extracted_data, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e2c87f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resdf=[json.loads(r) for r in res]\n",
    "# for i in range(1):\n",
    "#     test_note=df.iloc[i]['note_text']\n",
    "#     resdf[i]['note']=test_note\n",
    "#     resdf[i]['coral_idx']=df.iloc[i].coral_idx\n",
    "# resdf=pd.DataFrame(resdf)\n",
    "# mysave(resdf, base_filename='output/keysummary', extension='.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9e63340d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "\n",
      "coral_idx: 184\n",
      "\n",
      "\n",
      "Parsed JSON:\n",
      "{\n",
      "  \"chemotherapy\": {\n",
      "    \"summary\": \"Gemzar and carboplatin, with possible second line treatment of ***** or clinical trials/eribulin/doxil\",\n",
      "    \"short_term_side_effects\": \"nausea, vomiting, fatigue, hair loss, anemia, neutropenia, thrombocytopenia\",\n",
      "    \"long_term_side_effects\": \"kidney damage, hearing loss, infertility, secondary cancers\"\n",
      "  }\n",
      "}\n",
      "==============================\n",
      "\n",
      "coral_idx: 185\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[90], line 40\u001b[0m\n\u001b[1;32m     17\u001b[0m task_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mGiven this medication plan, organize them into the following structured JSON schema.  \u001b[39m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124mCombine all related medications of the same type into a concise summary.\u001b[39m\n\u001b[1;32m     19\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124mReturn ONLY the JSON object, nothing else.\u001b[39m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Run the model\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m answer, returned_cache \u001b[38;5;241m=\u001b[39m \u001b[43mrun_model_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgen_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# More aggressive cleaning\u001b[39;00m\n\u001b[1;32m     48\u001b[0m clean_answer \u001b[38;5;241m=\u001b[39m answer\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[0;32m~/repo/med_dict/ult.py:273\u001b[0m, in \u001b[0;36mrun_model_with_cache_manual\u001b[0;34m(prompt_text, model, tokenizer, generation_config, kv_cache)\u001b[0m\n\u001b[1;32m    270\u001b[0m     next_token \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(next_token_logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    272\u001b[0m \u001b[38;5;66;03m# Store the token value\u001b[39;00m\n\u001b[0;32m--> 273\u001b[0m token_id \u001b[38;5;241m=\u001b[39m \u001b[43mnext_token\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m generated_tokens\u001b[38;5;241m.\u001b[39mappend(token_id)\n\u001b[1;32m    276\u001b[0m \u001b[38;5;66;03m# Check for EOS\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "for i, r in enumerate(res):\n",
    "    coral_idx = df.iloc[i].coral_idx\n",
    "    print(\"=\" * 30)\n",
    "    print(f\"\\ncoral_idx: {coral_idx}\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Parse the old medication plan\n",
    "        old_out_str = json.loads(r)['Medication_Plan old']['medication_plan']\n",
    "    except (json.JSONDecodeError, KeyError) as e:\n",
    "        print(f\"Error parsing input for coral_idx {coral_idx}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    # Construct the prompt\n",
    "    task_prompt = \"\"\"Given this medication plan, organize them into the following structured JSON schema.  \n",
    "    Combine all related medications of the same type into a concise summary.\n",
    "\n",
    "    CRITICAL: Respond ONLY with a valid JSON object. Do not include ANY text before or after the JSON.\n",
    "    Do not include explanations, markdown, or any other text.\n",
    "\n",
    "    Use this exact schema:\n",
    "    {\n",
    "        \"chemotherapy\": {\n",
    "            \"summary\": \"the summary\",\n",
    "            \"short_term_side_effects (only if mentioned in the Medication plan)\": \"effects\",\n",
    "            \"long_term_side_effects (only if mentioned in the Medication plan)\": \"effects\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    <begin Medication plan>\n",
    "    \"\"\" + old_out_str + \"\"\"\n",
    "    <end of Medication plan>\n",
    "    \n",
    "    Return ONLY the JSON object, nothing else.\n",
    "    \"\"\"\n",
    "\n",
    "    # Run the model\n",
    "    answer, returned_cache = run_model_function(\n",
    "        task_prompt, \n",
    "        model, \n",
    "        tokenizer, \n",
    "        gen_config, \n",
    "    )\n",
    "\n",
    "    # More aggressive cleaning\n",
    "    clean_answer = answer.strip()\n",
    "    \n",
    "    # Remove markdown code blocks\n",
    "    clean_answer = re.sub(r'^```(?:json)?\\s*', '', clean_answer)\n",
    "    clean_answer = re.sub(r'\\s*```$', '', clean_answer)\n",
    "    \n",
    "    # Try to extract JSON object using regex (finds first complete JSON object)\n",
    "    json_match = re.search(r'\\{(?:[^{}]|(?:\\{(?:[^{}]|(?:\\{[^{}]*\\}))*\\}))*\\}', clean_answer, re.DOTALL)\n",
    "    \n",
    "    if json_match:\n",
    "        clean_answer = json_match.group(0)\n",
    "    \n",
    "    # # Debug: print what we're trying to parse\n",
    "    # print(\"Cleaned answer:\")\n",
    "    # print(clean_answer[:500])  # Print first 500 chars\n",
    "    # print(\"...\")\n",
    "    \n",
    "    try:\n",
    "        thisout = json.loads(clean_answer)\n",
    "        print(\"\\nParsed JSON:\")\n",
    "        print(json.dumps(thisout, indent=2))\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSON Parse Error for coral_idx {coral_idx}: {e}\")\n",
    "        print(f\"Attempted to parse: {clean_answer[:200]}...\")\n",
    "        \n",
    "        # Try alternative: find just the first complete JSON object\n",
    "        try:\n",
    "            # More lenient: decode until first valid JSON\n",
    "            decoder = json.JSONDecoder()\n",
    "            thisout = decoder.raw_decode(clean_answer)[0]\n",
    "            print(\"\\nParsed JSON (with raw_decode):\")\n",
    "            print(json.dumps(thisout, indent=2))\n",
    "        except:\n",
    "            print(\"Could not parse JSON even with raw_decode\")\n",
    "            continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
