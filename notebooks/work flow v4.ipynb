{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "503c81c2-012a-4e15-a255-a4a3e4c576d7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install sentencepiece protobuf datasets transformers trl textstat peft bitsandbytes nltk --quiet\n",
    "!pip install -U bitsandbytes accelerate --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b2c708e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"hf.token\", \"r\") as f:\n",
    "    hftoken = f.read().strip()  \n",
    "\n",
    "import os\n",
    "cache_dir = \"/mnt/c/Users/yc/.cache/huggingface\"\n",
    "os.environ['HF_HOME'] = cache_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53651bb",
   "metadata": {},
   "source": [
    "v4 is about oncology notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43c967d-ea35-4d4e-8ddc-f39328032b1a",
   "metadata": {},
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e3b7919-277b-47b9-95b9-eab518373862",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yc/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import csv\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "\n",
    "# Third-party data and ML libraries\n",
    "import pandas as pd\n",
    "import torch\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Hugging Face ecosystem\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "\n",
    "# Fine-tuning and optimization\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# Text analysis and readability metrics\n",
    "from textstat import (\n",
    "    flesch_kincaid_grade, \n",
    "    flesch_reading_ease,\n",
    "    smog_index, \n",
    "    gunning_fog, \n",
    "    dale_chall_readability_score,\n",
    "    text_standard, \n",
    "    syllable_count\n",
    ")\n",
    "\n",
    "# Optional: Uncomment if needed\n",
    "from huggingface_hub import login\n",
    "login(token=hftoken)  # Move token to environment variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dd4555-473d-48cd-bc86-60a4c41da0a8",
   "metadata": {},
   "source": [
    "## load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d74a59c-b625-4944-8a79-aea69bc6cd8a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:32<00:00,  8.09s/it]\n"
     ]
    }
   ],
   "source": [
    "# Model Configuration\n",
    "# model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "# Recommended upgrade - Llama 3.1 8B\n",
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "# # Or try Mistral 7B\n",
    "# model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "device_map = {\"\": 0}\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name,\n",
    "                                        #   cache_dir=cache_dir\n",
    "                                          )\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "# Load model with quantization\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     device_map=device_map,\n",
    "#     quantization_config=bnb_config\n",
    "# )\n",
    "\n",
    "# # full precision\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     device_map=device_map,\n",
    "#     torch_dtype=torch.float16,  # Use half precision instead\n",
    "#     # cache_dir=cache_dir\n",
    "# )\n",
    "\n",
    "# 8 precision\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=device_map,\n",
    "    load_in_8bit=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e1c388-57f9-41b0-87d1-25a9ad884430",
   "metadata": {},
   "source": [
    "## data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf6281f4-7ebb-48fd-a08b-4271349ededf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def calculate_readability_scores(text, term=None):\n",
    "    \"\"\"Calculate readability scores for a piece of text, optionally removing a term\"\"\"\n",
    "    scoring_text = text\n",
    "    \n",
    "    # Remove the term and its variations before scoring if provided\n",
    "    if term:\n",
    "        # Create variations of the term to remove (capitalized, lowercase)\n",
    "        term_variations = [term, term.lower(), term.capitalize()]\n",
    "        \n",
    "        for variation in term_variations:\n",
    "            scoring_text = scoring_text.replace(variation, \"\")\n",
    "        \n",
    "        # Clean up any double spaces created\n",
    "        scoring_text = \" \".join(scoring_text.split())\n",
    "    \n",
    "    fk_grade = flesch_kincaid_grade(scoring_text)\n",
    "    readability = flesch_reading_ease(scoring_text)\n",
    "    return {\n",
    "        \"fk_grade\": fk_grade,\n",
    "        \"readability\": readability,\n",
    "    } \n",
    "def txt_to_dict(file_path):\n",
    "    data_dict = {}\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        for i in range(0, len(lines) - 1, 2):\n",
    "            key = lines[i].strip()    # Odd line are key\n",
    "            value = lines[i + 1].strip()  # Even line are value\n",
    "            data_dict[key] = value\n",
    "\n",
    "    return data_dict\n",
    "\n",
    "txt_file_path = 'formaldef.txt' \n",
    "formaldic = txt_to_dict(txt_file_path)\n",
    "len(formaldic)\n",
    "\n",
    "meddict={}\n",
    "for k,v in formaldic.items():\n",
    "    meddict[k.split('Listen to pronunciation')[0].split('(')[0]]=v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c6d4475-de2d-4b19-b300-0db9acd700c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df = pd.read_csv('/mnt/c/Users/yc/Downloads/coral/unannotated/data/breastca_unannotated.csv')\n",
    "df=df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a445b4-0a2b-45d1-9892-42f775ef20b8",
   "metadata": {},
   "source": [
    "# prompt, paragraph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d5214f6-5ac9-4799-bc18-cfeb96d6deac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modular Medical Text Explanation System\n",
    "# Returns clean dict with configurable steps: extract, generate, clean\n",
    "\n",
    "import re\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "\n",
    "class MedicalTextExplainer:\n",
    "    def __init__(self, model, tokenizer, meddict):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.meddict = meddict\n",
    "        \n",
    "        # Improved generation config for better outputs\n",
    "        self.generation_config = {\n",
    "            \"max_new_tokens\": 999,              # Increased for fuller explanations\n",
    "            \"temperature\": 0.3,\n",
    "            \"top_p\": 0.85,\n",
    "            \"repetition_penalty\": 1.15,\n",
    "            \"do_sample\": True,\n",
    "            \"pad_token_id\": tokenizer.eos_token_id,\n",
    "            \"eos_token_id\": tokenizer.eos_token_id,  # Ensure proper stopping\n",
    "            \"early_stopping\": True,              # Stop when EOS token is generated\n",
    "        }\n",
    "        \n",
    "        # Define available steps\n",
    "        self.available_steps = ['determine_audience', 'extract', 'generate', 'clean'] # 增加新步骤\n",
    "        self.step_functions = {\n",
    "            'determine_audience': self._step_determine_audience, # 增加新映射\n",
    "            'extract': self._step_extract_terms,\n",
    "            'generate': self._step_generate_explanation,\n",
    "            'clean': self._step_clean_response\n",
    "        }\n",
    "    def create_audience_prompt(self, original_text):\n",
    "            \"\"\"\n",
    "            Creates a simple, direct prompt to classify the audience.\n",
    "            \"\"\"\n",
    "            prompt = f\"\"\"\n",
    "        [INST] You are an expert medical text classifier. Read the following medical text and determine the appropriate audience for a summary letter.\n",
    "\n",
    "        **Medical Text:**\n",
    "        \"{original_text}\"\n",
    "\n",
    "        **Instructions:**\n",
    "        Based on the text, who is the audience for the explanation letter?\n",
    "        - If the text describes a patient recovering, Discharge Condition says much improved, or having a positive or follow ongoing treatment plan, the audience is the **patient**.\n",
    "        - If the text mentions \"died\", \"passed away,\" \"deceased,\" or describes a fatal outcome such \"comfort care\" \"hospice care\" \"pallliative care\", \"palliative extubate\", the audience is the **patient's family**.\n",
    "\n",
    "        Respond with a single word ONLY: **patient** or **family**.\n",
    "        [/INST]\n",
    "        \"\"\"\n",
    "            return prompt\n",
    "    \n",
    "\n",
    "    def _step_determine_audience(self, result):\n",
    "        \"\"\"Step 0: Determine the correct audience (patient or family).\"\"\"\n",
    "        text = result['original_text']\n",
    "        \n",
    "        # 1. 创建分类任务的Prompt\n",
    "        audience_prompt = self.create_audience_prompt(text)\n",
    "        \n",
    "        # 2. 调用模型进行分类 (使用一个轻量的生成配置)\n",
    "        try:\n",
    "            inputs = self.tokenizer(audience_prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "            input_length = inputs[\"input_ids\"].shape[1]\n",
    "            \n",
    "            # 使用一个非常短的生成配置，因为我们只需要一个词\n",
    "            audience_config = {\n",
    "                \"max_new_tokens\": 5,\n",
    "                \"temperature\": 0.01, # 温度极低，追求最确定的答案\n",
    "                \"do_sample\": False, # 不进行采样\n",
    "                \"pad_token_id\": self.tokenizer.eos_token_id\n",
    "            }\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(inputs[\"input_ids\"], **audience_config)\n",
    "            \n",
    "            response_tokens = outputs[0][input_length:]\n",
    "            raw_label = self.tokenizer.decode(response_tokens, skip_special_tokens=True).lower().strip()\n",
    "            \n",
    "            # 3. 清理和验证标签\n",
    "            if \"family\" in raw_label:\n",
    "                audience = \"family\"\n",
    "            else:\n",
    "                audience = \"patient\" # 默认为 patient\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ⚠️ Audience determination failed: {e}. Defaulting to 'patient'.\")\n",
    "            audience = \"patient\"\n",
    "\n",
    "        result['determined_audience'] = audience\n",
    "        print(f\"   🎯 Determined audience: '{audience}'\")\n",
    "        return result\n",
    "\n",
    "\n",
    "    def explain_medical_text(self, text, steps=['extract', 'generate', 'clean']):\n",
    "        \n",
    "        # Validate steps\n",
    "        invalid_steps = [step for step in steps if step not in self.available_steps]\n",
    "        if invalid_steps:\n",
    "            raise ValueError(f\"Invalid steps: {invalid_steps}. Available steps: {self.available_steps}\")\n",
    "        \n",
    "        # Initialize result with original text\n",
    "        result = {\n",
    "            'original_text': text,\n",
    "            'steps_run': steps.copy(),\n",
    "            'available_steps': self.available_steps.copy()\n",
    "        }\n",
    "        \n",
    "        # Run each step in sequence, passing result between steps\n",
    "        for step in steps:\n",
    "            print(f\"🔄 Running step: {step}\")\n",
    "            result = self.step_functions[step](result)\n",
    "        \n",
    "        print(f\"✅ Completed {len(steps)} steps: {steps}\")\n",
    "        return result\n",
    "    \n",
    "    def _step_extract_terms(self, result):\n",
    "        \"\"\"Step 1: Extract medical terms from text.\"\"\"\n",
    "        text = result['original_text']\n",
    "        found_terms = self.extract_medical_terms(text)\n",
    "        \n",
    "        result.update({\n",
    "            'found_terms': found_terms,\n",
    "            'terms_count': len(found_terms)\n",
    "        })\n",
    "        \n",
    "        print(f\"   📋 Found {len(found_terms)} medical terms\")\n",
    "        return result\n",
    "    \n",
    "    \n",
    "    def _step_generate_explanation(self, result):\n",
    "        \"\"\"Step 2: Generate explanation using model.\"\"\"\n",
    "        text = result['original_text']\n",
    "        if 'determined_audience' not in result:\n",
    "            raise ValueError(\"Audience not determined. Please run 'determine_audience' step before 'generate'.\")\n",
    "        \n",
    "        audience = result['determined_audience']\n",
    "        # Use found terms if extract step was run, otherwise extract them now\n",
    "        if 'found_terms' in result:\n",
    "            found_terms = result['found_terms']\n",
    "        else:\n",
    "            print(\"   ⚠️  Terms not extracted yet, extracting now...\")\n",
    "            found_terms = self.extract_medical_terms(text)\n",
    "            result['found_terms'] = found_terms\n",
    "        \n",
    "        # Create prompt and generate\n",
    "        prompt = self.create_prompt(text, found_terms, audience)\n",
    "        raw_output = self._generate_raw_response(prompt)\n",
    "        \n",
    "        result.update({\n",
    "            'prompt': prompt,\n",
    "            'raw_output': raw_output,\n",
    "            'prompt_length': len(prompt),\n",
    "        })\n",
    "        \n",
    "        print(f\"   🤖 Generated {len(raw_output)} character response\")\n",
    "        return result\n",
    "    \n",
    "    \n",
    "    def _step_clean_response(self, result):\n",
    "        \"\"\"Step 3: Clean the raw model output.\"\"\"\n",
    "        \n",
    "        # Check if we have raw output to clean\n",
    "        if 'raw_output' not in result:\n",
    "            raise ValueError(\"No raw_output found. Must run 'generate' step before 'clean' step.\")\n",
    "        \n",
    "        raw_output = result['raw_output']\n",
    "        \n",
    "        cleaned_output = self.model_clean_response(raw_output, result['determined_audience'])\n",
    "        \n",
    "        result.update({\n",
    "            'cleaned_output': cleaned_output,\n",
    "            'cleaned_length': len(cleaned_output)\n",
    "        })\n",
    "        \n",
    "        print(f\"   🧹 Cleaned to {len(cleaned_output)} characters\")\n",
    "        return result\n",
    "    \n",
    "    def extract_medical_terms(self, text):\n",
    "        \"\"\"\n",
    "        Enhanced medical term extraction to catch more terms from complex texts.\n",
    "        \"\"\"\n",
    "        found_terms = {}\n",
    "        \n",
    "        # Strategy 1: Single words (improved regex for medical terms)\n",
    "        words = re.findall(r'\\b[A-Za-z]+(?:[-\\'][A-Za-z]+)*\\b', text)\n",
    "        for word in words:\n",
    "            definition = self.find_term_in_dict(word)\n",
    "            if definition:\n",
    "                found_terms[word] = definition\n",
    "        \n",
    "        # Strategy 2: Multi-word terms (expanded range for complex medical phrases)\n",
    "        for n in range(2, 6):  # Increased from 5 to 6 for longer medical phrases\n",
    "            n_grams = self.get_n_grams(text, n)\n",
    "            for phrase in n_grams:\n",
    "                definition = self.find_term_in_dict(phrase)\n",
    "                if definition:\n",
    "                    found_terms[phrase] = definition\n",
    "        \n",
    "        # Strategy 3: Medical abbreviations (enhanced pattern)\n",
    "        abbreviations = re.findall(r'\\b[A-Z]{2,8}\\b', text)  # Increased from 6 to 8\n",
    "        for abbrev in abbreviations:\n",
    "            definition = self.find_term_in_dict(abbrev)\n",
    "            if definition:\n",
    "                found_terms[abbrev] = definition\n",
    "        \n",
    "        # Strategy 4: Medical procedures and conditions with specific patterns\n",
    "        medical_patterns = [\n",
    "            r'\\b\\w+oscopy\\b',          # bronchoscopy, endoscopy, etc.\n",
    "            r'\\b\\w+ectomy\\b',          # appendectomy, etc.\n",
    "            r'\\b\\w+itis\\b',            # bronchitis, arthritis, etc.\n",
    "            r'\\b\\w+osis\\b',            # fibrosis, stenosis, etc.\n",
    "            r'\\b\\w+emia\\b',            # anemia, septicemia, etc.\n",
    "            r'\\b\\w+pathy\\b',           # myopathy, neuropathy, etc.\n",
    "            r'\\b\\w+malacia\\b',         # tracheomalacia, etc.\n",
    "        ]\n",
    "        \n",
    "        for pattern in medical_patterns:\n",
    "            matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "            for match in matches:\n",
    "                definition = self.find_term_in_dict(match)\n",
    "                if definition:\n",
    "                    found_terms[match] = definition\n",
    "        \n",
    "        # Strategy 5: Medication names (often end in specific suffixes)\n",
    "        medication_patterns = [\n",
    "            r'\\b\\w+cillin\\b',          # penicillin, amoxicillin, etc.\n",
    "            r'\\b\\w+mycin\\b',           # streptomycin, etc.\n",
    "            r'\\b\\w+floxacin\\b',        # levofloxacin, ciprofloxacin, etc.\n",
    "            r'\\b\\w+sone\\b',            # prednisone, cortisone, etc.\n",
    "            r'\\b\\w+pam\\b',             # lorazepam, etc.\n",
    "        ]\n",
    "        \n",
    "        for pattern in medication_patterns:\n",
    "            matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "            for match in matches:\n",
    "                definition = self.find_term_in_dict(match)\n",
    "                if definition:\n",
    "                    found_terms[match] = definition\n",
    "        \n",
    "        return found_terms\n",
    "    \n",
    "    \n",
    "    def get_n_grams(self, text, n):\n",
    "        \"\"\"Generate n-grams from text.\"\"\"\n",
    "        words = re.findall(r'\\b[A-Za-z]+\\b', text.lower())\n",
    "        n_grams = []\n",
    "        for i in range(len(words) - n + 1):\n",
    "            phrase = ' '.join(words[i:i+n])\n",
    "            n_grams.append(phrase)\n",
    "        return n_grams\n",
    "    \n",
    "    def find_term_in_dict(self, term):\n",
    "        \"\"\"Find term in medical dictionary.\"\"\"\n",
    "        search_formats = [\n",
    "            term, term.lower(), term.upper(), term.title(), term.capitalize()\n",
    "        ]\n",
    "        \n",
    "        for search_term in search_formats:\n",
    "            if search_term in self.meddict:\n",
    "                return self.meddict[search_term]\n",
    "        \n",
    "        # Partial matching\n",
    "        for key in self.meddict.keys():\n",
    "            if key.lower() == term.lower():\n",
    "                return self.meddict[key]\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def create_prompt(self, original_text, found_terms, determined_audience):\n",
    "\n",
    "        # Format found terms for inclusion in prompt\n",
    "        terms_section = \"\"\n",
    "        if found_terms:\n",
    "            terms_section = \"Found medical terms and their definitions:\\n\"\n",
    "            for term, definition in found_terms.items():\n",
    "                terms_section += f\"- {term}: {definition}\\n\"\n",
    "        else:\n",
    "            terms_section = \"No medical terms found in dictionary.\\n\"\n",
    "        \n",
    "        if determined_audience == 'family':\n",
    "            audience_instruction = \"The determined audience for this letter is the **patient's family**. You must address them directly as 'you' and refer to the patient in the third person (e.g., 'your loved one,' 'he/she').\"\n",
    "        else: # patient\n",
    "            audience_instruction = \"The determined audience for this letter is the **patient**. You must address them directly as 'you' throughout the entire letter.\"\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "<s>[INST] \n",
    "### Persona\n",
    "You are an experienced and compassionate Oncologist (cancer specialist) and a skilled medical educator. Your primary role is to translate complex medical information into clear, understandable, and supportive explanations for patients and their families. Your tone should always be professional, empathetic, and honest, balancing realism with hope.\n",
    "\n",
    "### Original Medical Text:\n",
    "\"{original_text}\"\n",
    "\n",
    "### Your Task:\n",
    "Your goal is to write a single, complete, and polished letter to the patient that explains the information from the medical text above. Imagine you are sitting with the patient and explaining this to them in person, then putting it in writing.\n",
    "STRICT NEGATIVE CONSTRAINT: Under NO circumstances should you say something not exist in the note.\n",
    "\n",
    "**1. Letter Structure and Flow:** Organize your letter logically to guide the patient through the information without overwhelming them. Follow this structure:\n",
    "    * **Empathetic Opening:** Start with a short warm and supportive salutation. Acknowledge the reason for their recent visit and the stress they might be feeling. \n",
    "    STRICT NEGATIVE CONSTRAINT: AVOID LENGTHY OPENINGS. 1 sentence maxiumm.\n",
    "    * **Summary of Past History:** Briefly recap their initial diagnosis and treatment. **Crucial rule: When discussing past treatment decisions, use neutral, non-judgmental language. Avoid direct phrases like \"you chose not to\" or \"you refused.\" Instead, use objective phrasing like \"At that time, the treatment plan did not include...\" or \"The decision was made not to proceed with...\" to describe past events.**\n",
    "    * **Explanation of Current Findings:** Describe what the recent tests (like the CT scan) have shown. Use simple, everyday language. Use analogies if helpful. **After providing a simple explanation (e.g., 'the cancer has spread'), avoid immediately following it with blunt, technical classifications. However, we want to highight the cancer stage if the note mentioned, and explain the stage meaning immediately after mentioning. Prioritize the compassionate explanation over the clinical label in the letter's flow.**\n",
    "    * **The Main Diagnosis (Assessment):** Clearly and gently explain the main conclusion. Explain what terms like \"metastatic\" or \"recurrence\" mean.\n",
    "    STRICT NEGATIVE CONSTRAINT: If the cancer has spread (metastasis), absolutely DO NOT list the specific organs affected. Instead, just say \"the cancer has spread to other parts of your body.\"\n",
    "    * **The Go-Forward Plan:** Detail the next steps you have planned. For each step (like a biopsy, MRI, or bone scan), explain **WHAT** it is, and more importantly, **WHY** you are doing it.\n",
    "    * **Treatment Goals and Philosophy:** This is the most important section. If the medical note mentions the term \"palliative,\" explain this concept with extreme care. define it as an active and positive treatment approach focused on controlling the cancer, managing symptoms, and improving quality of life. The entire focus must be on the quality and extension of life.    \n",
    "    * **STRICT NEGATIVE CONSTRAINT: Under NO circumstances use fatalistic or terminal language. Absolutely AVOID phrases like \"until the end of your life,\" \"preparing for the end,\" \"however many that may be\", or any language that focuses on dying. The entire focus of this paragraph MUST be on the quality and extension of LIFE.**\n",
    "    STRICT NEGATIVE CONSTRAINT: AVOID LENGTHY comforting sentences and too much sympathy. The letter should be concise and focused on key information.\n",
    "    * **Closing with Support:** End the letter by reinforcing that your team is there to support them through every step.\n",
    "    For the \"Explanation of Current Findings & Diagnosis\" section:\n",
    "\n",
    "\n",
    "**2. Language and Tone Directives:**\n",
    "    * **Define Key Terms:** Do not assume the patient knows any medical jargon. Clearly define necessary terms like \"metastatic\" or \"biopsy\" as they appear.\n",
    "    * **Maintain Your Persona:** Use \"we\" to refer to the medical team. Write with empathy and clarity. The goal is to inform, not to scare.\n",
    "    * **Audience:** The letter is strictly for the **patient**.\n",
    "\n",
    "### Strict Output Formatting:\n",
    "Provide ONLY the extracted letter text. Your output must start directly with the salutation (e.g., \"Dear [Patient],\") and end immediately after the signature. There should be absolutely no other text before or after the letter. You can use this format:\n",
    "Dear [patient name]\n",
    "We hope you are doing well. We're writing this letter to help you understand what happened during your recent visit on [Data] with [Physician name]\n",
    "Why did I come to the clinic?\n",
    "Why was discussed?\n",
    "What tests were done or ordered?\n",
    "What treatment or medication were changed?\n",
    "What is the plan?\n",
    "Thank you for choosing [Institution] for your care. We are committed to supporting you every step of the way on your journey to better health and well-being.\n",
    "Please feel free to contact us if you have any questions.\n",
    "Sincerely\n",
    "[Institution]\n",
    "Please provide the patient-focused explanation now: Your output must start directly with the salutation.\n",
    "[/INST]\n",
    "    \"\"\"\n",
    "        return prompt\n",
    "    \n",
    "    def _generate_raw_response(self, prompt):\n",
    "        \"\"\"\n",
    "        Generate raw response from model (separated from cleaning).\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Tokenize input\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(self.model.device)\n",
    "            input_length = inputs[\"input_ids\"].shape[1]\n",
    "            \n",
    "            # Generate response\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    inputs[\"input_ids\"],\n",
    "                    attention_mask=inputs[\"attention_mask\"],\n",
    "                    **self.generation_config\n",
    "                )\n",
    "            \n",
    "            # Extract only the NEW tokens (response), not the input prompt\n",
    "            response_tokens = outputs[0][input_length:]  # Skip input tokens\n",
    "            raw_output = self.tokenizer.decode(response_tokens, skip_special_tokens=False)\n",
    "            \n",
    "            return raw_output\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error generating response: {str(e)}\"\n",
    "    \n",
    "    def model_clean_response(self, raw_response, aduience):\n",
    "        \"\"\"\n",
    "        Enhanced cleaning with better instructions for patient-focused, structured output.\n",
    "        \"\"\"\n",
    "        \n",
    "        # If response is very short or error, return as-is\n",
    "        if len(raw_response.strip()) < 10 or raw_response.startswith(\"❌\"):\n",
    "            return raw_response\n",
    "        \n",
    "        cleaning_prompt = f\"\"\"\n",
    " \n",
    "[INST]\n",
    "### Persona\n",
    "You are a specialized text-formatting AI. Your sole purpose is to extract a clean, complete letter from a raw text block, removing all surrounding artifacts, preambles, and post-scripts. You do not edit or change the content of the letter itself.\n",
    "\n",
    "### Raw Text Block:\n",
    "\"{raw_response}\"\n",
    "\n",
    "### Your Task:\n",
    "Your task is to analyze the provided Raw Text Block and isolate ONLY the formal letter contained within it. You must follow these rules precisely:\n",
    "\n",
    "1.  **Identify the Letter's Boundaries:**\n",
    "    * The letter starts with a salutation (e.g., \"Dear [Patient],\" or similar).\n",
    "    * The letter ends with the final line of the signature (e.g., \"Oncologist\" or the doctor's name).\n",
    "\n",
    "2.  **Strip All Prefixes:**\n",
    "    * You MUST remove any and all text that appears **before** the letter's starting salutation.\n",
    "    * **Examples of prefixes to remove:** \"The final answer is:\", \"Here is the rewritten letter:\", \"Certainly, here is the polished version:\", conversational filler, blank lines, etc.\n",
    "\n",
    "3.  **Strip All Suffixes:**\n",
    "    * You MUST remove any and all text that appears **after** the final line of the signature.\n",
    "    * **Examples of suffixes to remove:** \"---\", \"Note:\", \"<|eot_id|>\", explanations about the edits, meta-commentary, etc.\n",
    "\n",
    "4.  **Strict No-Edit Rule:**\n",
    "    * **Do not change the wording, tone, or content of the letter itself.** Your only job is to surgically extract the letter from the surrounding text. You are pruning, not rewriting.\n",
    "\n",
    "\n",
    "\n",
    "Please provide the cleaned letter now:\n",
    "[/INST]\n",
    "\n",
    "\"\"\"\n",
    "        \n",
    "        try:\n",
    "            inputs = self.tokenizer(cleaning_prompt, return_tensors=\"pt\", truncation=True).to(self.model.device)\n",
    "            input_length = inputs[\"input_ids\"].shape[1]\n",
    "            \n",
    "            # Use enhanced generation config for cleaning\n",
    "            cleaning_config = {\n",
    "                \"max_new_tokens\": 600,  # Increased for more complete cleaning\n",
    "                \"temperature\": 0.1,\n",
    "                \"top_p\": 0.9,\n",
    "                \"repetition_penalty\": 1.1,\n",
    "                \"do_sample\": True,\n",
    "                \"pad_token_id\": self.tokenizer.eos_token_id,\n",
    "                \"eos_token_id\": self.tokenizer.eos_token_id,\n",
    "                \"early_stopping\": True,\n",
    "            }\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    inputs[\"input_ids\"],\n",
    "                    attention_mask=inputs[\"attention_mask\"],\n",
    "                    **cleaning_config\n",
    "                )\n",
    "            \n",
    "            # Extract only the cleaned response\n",
    "            response_tokens = outputs[0][input_length:]\n",
    "            cleaned = self.tokenizer.decode(response_tokens, skip_special_tokens=True)\n",
    "            \n",
    "\n",
    "            \n",
    "            return cleaned\n",
    "            \n",
    "        except Exception as e:\n",
    "     \n",
    "            return raw_response\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "330cd4de-e256-45db-94ae-348effe10813",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = MedicalTextExplainer(model, tokenizer, meddict)\n",
    "allres=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bebc6fb-06a4-4ec8-b5ae-81f3690ed848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['coral_idx', 'Sex', 'UCSFDerivedRaceEthnicity_X', 'BirthDate',\n",
       "       'note_text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55cb93ac-50ff-41b2-8495-cfede50d5853",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yc/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.01` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/yc/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Running step: determine_audience\n",
      "   🎯 Determined audience: 'patient'\n",
      "🔄 Running step: extract\n",
      "   📋 Found 143 medical terms\n",
      "🔄 Running step: generate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yc/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:638: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   🤖 Generated 2041 character response\n",
      "🔄 Running step: clean\n",
      "   🧹 Cleaned to 2045 characters\n",
      "✅ Completed 4 steps: ['determine_audience', 'extract', 'generate', 'clean']\n",
      "🔄 Running step: determine_audience\n",
      "   🎯 Determined audience: 'family'\n",
      "🔄 Running step: extract\n",
      "   📋 Found 265 medical terms\n",
      "🔄 Running step: generate\n",
      "   🤖 Generated 2941 character response\n",
      "🔄 Running step: clean\n",
      "   🧹 Cleaned to 3086 characters\n",
      "✅ Completed 4 steps: ['determine_audience', 'extract', 'generate', 'clean']\n",
      "🔄 Running step: determine_audience\n",
      "   🎯 Determined audience: 'patient'\n",
      "🔄 Running step: extract\n",
      "   📋 Found 87 medical terms\n",
      "🔄 Running step: generate\n",
      "   🤖 Generated 3090 character response\n",
      "🔄 Running step: clean\n",
      "   🧹 Cleaned to 2139 characters\n",
      "✅ Completed 4 steps: ['determine_audience', 'extract', 'generate', 'clean']\n",
      "🔄 Running step: determine_audience\n",
      "   🎯 Determined audience: 'patient'\n",
      "🔄 Running step: extract\n",
      "   📋 Found 141 medical terms\n",
      "🔄 Running step: generate\n",
      "   🤖 Generated 4015 character response\n",
      "🔄 Running step: clean\n",
      "   🧹 Cleaned to 2995 characters\n",
      "✅ Completed 4 steps: ['determine_audience', 'extract', 'generate', 'clean']\n",
      "🔄 Running step: determine_audience\n",
      "   🎯 Determined audience: 'family'\n",
      "🔄 Running step: extract\n",
      "   📋 Found 139 medical terms\n",
      "🔄 Running step: generate\n",
      "   🤖 Generated 2785 character response\n",
      "🔄 Running step: clean\n",
      "   🧹 Cleaned to 2419 characters\n",
      "✅ Completed 4 steps: ['determine_audience', 'extract', 'generate', 'clean']\n",
      "🔄 Running step: determine_audience\n",
      "   🎯 Determined audience: 'patient'\n",
      "🔄 Running step: extract\n",
      "   📋 Found 140 medical terms\n",
      "🔄 Running step: generate\n",
      "   🤖 Generated 3004 character response\n",
      "🔄 Running step: clean\n",
      "   🧹 Cleaned to 2421 characters\n",
      "✅ Completed 4 steps: ['determine_audience', 'extract', 'generate', 'clean']\n",
      "🔄 Running step: determine_audience\n",
      "   🎯 Determined audience: 'patient'\n",
      "🔄 Running step: extract\n",
      "   📋 Found 82 medical terms\n",
      "🔄 Running step: generate\n",
      "   🤖 Generated 2170 character response\n",
      "🔄 Running step: clean\n",
      "   🧹 Cleaned to 2139 characters\n",
      "✅ Completed 4 steps: ['determine_audience', 'extract', 'generate', 'clean']\n",
      "🔄 Running step: determine_audience\n",
      "   🎯 Determined audience: 'patient'\n",
      "🔄 Running step: extract\n",
      "   📋 Found 174 medical terms\n",
      "🔄 Running step: generate\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m      2\u001b[0m     text\u001b[38;5;241m=\u001b[39mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnote_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \n\u001b[0;32m----> 3\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mexplainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplain_medical_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdetermine_audience\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mextract\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgenerate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclean\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     allres\u001b[38;5;241m.\u001b[39mappend(result)\n",
      "Cell \u001b[0;32mIn[7], line 113\u001b[0m, in \u001b[0;36mMedicalTextExplainer.explain_medical_text\u001b[0;34m(self, text, steps)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m steps:\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🔄 Running step: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 113\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_functions\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Completed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(steps)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m steps: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msteps\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "Cell \u001b[0;32mIn[7], line 149\u001b[0m, in \u001b[0;36mMedicalTextExplainer._step_generate_explanation\u001b[0;34m(self, result)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;66;03m# Create prompt and generate\u001b[39;00m\n\u001b[1;32m    148\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_prompt(text, found_terms, audience)\n\u001b[0;32m--> 149\u001b[0m raw_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_raw_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m result\u001b[38;5;241m.\u001b[39mupdate({\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m'\u001b[39m: prompt,\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraw_output\u001b[39m\u001b[38;5;124m'\u001b[39m: raw_output,\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt_length\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mlen\u001b[39m(prompt),\n\u001b[1;32m    155\u001b[0m })\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   🤖 Generated \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(raw_output)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m character response\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 348\u001b[0m, in \u001b[0;36mMedicalTextExplainer._generate_raw_response\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# Generate response\u001b[39;00m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 348\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;66;03m# Extract only the NEW tokens (response), not the input prompt\u001b[39;00m\n\u001b[1;32m    355\u001b[0m response_tokens \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m][input_length:]  \u001b[38;5;66;03m# Skip input tokens\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/generation/utils.py:2215\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2207\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2208\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2209\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2210\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2211\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2212\u001b[0m     )\n\u001b[1;32m   2214\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2215\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2216\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2220\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2222\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2223\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2225\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2226\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2227\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2228\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2229\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2234\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2235\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/generation/utils.py:3206\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3203\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   3205\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 3206\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3208\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3209\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3210\u001b[0m     outputs,\n\u001b[1;32m   3211\u001b[0m     model_kwargs,\n\u001b[1;32m   3212\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3213\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1190\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[1;32m   1187\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1190\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1203\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:945\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    933\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    934\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    935\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    942\u001b[0m         position_embeddings,\n\u001b[1;32m    943\u001b[0m     )\n\u001b[1;32m    944\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 945\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    956\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:676\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    673\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    675\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 676\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    687\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    689\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:559\u001b[0m, in \u001b[0;36mLlamaSdpaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    546\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mforward(\n\u001b[1;32m    547\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m    548\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    554\u001b[0m         position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[1;32m    555\u001b[0m     )\n\u001b[1;32m    557\u001b[0m bsz, q_len, _ \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m--> 559\u001b[0m query_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    560\u001b[0m key_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj(hidden_states)\n\u001b[1;32m    561\u001b[0m value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj(hidden_states)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:1082\u001b[0m, in \u001b[0;36mLinear8bitLt.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1079\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m x\u001b[38;5;241m.\u001b[39mdtype:\n\u001b[1;32m   1080\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m-> 1082\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mbnb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1084\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mhas_fp16_weights \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mCB \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1085\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mCB\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:431\u001b[0m, in \u001b[0;36mmatmul\u001b[0;34m(A, B, out, state, threshold, bias)\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (A\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m ipex_cpu) \u001b[38;5;129;01mor\u001b[39;00m (A\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m ipex_xpu):\n\u001b[1;32m    430\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m MatMul8bitFp\u001b[38;5;241m.\u001b[39mapply(A, B, out, bias, state)\n\u001b[0;32m--> 431\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMatMul8bitLt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/function.py:575\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    574\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 575\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    583\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:197\u001b[0m, in \u001b[0;36mMatMul8bitLt.forward\u001b[0;34m(ctx, A, B, out, bias, state)\u001b[0m\n\u001b[1;32m    194\u001b[0m     CA, CAt, SCA, SCAt, outlier_cols \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mint8_double_quant(A\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat16), threshold\u001b[38;5;241m=\u001b[39mstate\u001b[38;5;241m.\u001b[39mthreshold)\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;66;03m# Fast path\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m     CA, SCA, outlier_cols \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint8_vectorwise_quant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthreshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m     CAt \u001b[38;5;241m=\u001b[39m SCAt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    200\u001b[0m has_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/bitsandbytes/functional.py:2088\u001b[0m, in \u001b[0;36mint8_vectorwise_quant\u001b[0;34m(A, threshold)\u001b[0m\n\u001b[1;32m   2070\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mint8_vectorwise_quant\u001b[39m(A: torch\u001b[38;5;241m.\u001b[39mTensor, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m):\n\u001b[1;32m   2071\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Quantizes a tensor with dtype `torch.float16` to `torch.int8` in accordance to the `LLM.int8()` algorithm.\u001b[39;00m\n\u001b[1;32m   2072\u001b[0m \n\u001b[1;32m   2073\u001b[0m \u001b[38;5;124;03m    For more information, see the [LLM.int8() paper](https://arxiv.org/abs/2208.07339).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2086\u001b[0m \u001b[38;5;124;03m        - `torch.Tensor` with dtype `torch.int32`, *optional*: A list of column indices which contain outlier features.\u001b[39;00m\n\u001b[1;32m   2087\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2088\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbitsandbytes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint8_vectorwise_quant\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_ops.py:716\u001b[0m, in \u001b[0;36mOpOverload.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m/\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 716\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_compile.py:32\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m     disable_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)\n\u001b[1;32m     30\u001b[0m     fn\u001b[38;5;241m.\u001b[39m__dynamo_disable \u001b[38;5;241m=\u001b[39m disable_fn\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m prior \u001b[38;5;241m=\u001b[39m _maybe_set_eval_frame(callback)\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 632\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    634\u001b[0m     _maybe_set_eval_frame(prior)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/library.py:566\u001b[0m, in \u001b[0;36m_impl.<locals>.register.<locals>.func_no_dynamo\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39m_disable_dynamo\n\u001b[1;32m    565\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc_no_dynamo\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 566\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/bitsandbytes/backends/cuda/ops.py:144\u001b[0m, in \u001b[0;36m_\u001b[0;34m(A, threshold)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m threshold \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;66;03m# TODO we could improve perf of this\u001b[39;00m\n\u001b[1;32m    142\u001b[0m     outliers \u001b[38;5;241m=\u001b[39m A\u001b[38;5;241m.\u001b[39mabs() \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m threshold\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m outliers\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m    145\u001b[0m         outlier_cols \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margwhere(outliers\u001b[38;5;241m.\u001b[39many(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    147\u001b[0m         \u001b[38;5;66;03m# Needed for torch.compile support.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for index, row in df.iterrows():\n",
    "    text=row['note_text'] \n",
    "    result = explainer.explain_medical_text(text, steps=['determine_audience','extract', 'generate','clean'])\n",
    "    allres.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8f08ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Dear [Patient],\\nWe hope you are doing well. We're writing this letter to help you understand what happened during your recent visit on December 30, 2019, with Dr. [Physician name].\\n\\nFirst, let's talk about your original diagnosis. In May 2013, you were diagnosed with a type of breast cancer called Stage IIA. At that time, the treatment plan included a mastectomy with sentinel node removal and implant reconstruction. Unfortunately, the cancer came back, and we've recently learned that it has spread to other parts of your body.\\n\\nThe recent tests showed that the cancer has spread to several areas, including your lungs, liver, and other places. This means that your cancer is considered metastatic. Don't worry; this doesn't mean that the cancer is untreatable. It simply means that we'll need to take a different approach to manage it.\\n\\nTo do this, we'll first confirm the type of cancer cells present using a biopsy. This involves taking a small sample of tissue from one of the affected areas. Once we know more about the cancer, we can discuss possible treatment options with you.\\n\\nOur goal is to control the cancer, alleviate any uncomfortable symptoms, and ensure that you live comfortably for as long as possible. We call this approach palliative care. Palliative care isn't just about treating the disease itself; it's also about making sure you receive the best possible care to enhance your quality of life.\\n\\nNext steps include scheduling a biopsy appointment with Dr. [Physician name]. After completing some additional testing, such as a bone scan and MRI, we'll review all the findings together and create a personalized plan tailored to your needs.\\n\\nIf you have any questions or concerns, please don't hesitate to reach out to our team. We're here to support you throughout this process.\\n\\nThank you for trusting us with your care. Sincerely,\\n[Institution] [/s]<|eot_id|>\""
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allres[0].keys()\n",
    "# allres[0]['terms_count']\n",
    "allres[0]['raw_output']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6469dbf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"### Cleaned Letter\\n\\nDear [Patient],\\n\\nWe hope you are doing well. We're writing this letter to help you understand what happened during your recent visit on December 30, 2019, with our oncologist.\\n\\nFirst, let's review your original diagnosis. In May 2013, you were diagnosed with a type of breast cancer called invasive ductal carcinoma. At that time, the treatment plan included a mastectomy with sentinel node removal and implant reconstruction. Unfortunately, the cancer came back, and we've been monitoring its progression ever since.\\n\\nRecently, you underwent some additional testing, including a CT scan of your chest, abdomen, and pelvis. These scans showed that the cancer has spread to other parts of your body. Specifically, the cancer has spread to several areas, including your lungs, liver, and ovaries. It's also caused a tumor to grow in your right armpit.\\n\\nThis means that your cancer is considered advanced, and we'll need to adjust our treatment plan accordingly. Our goal remains to control the cancer, manage any related symptoms, and improve your overall quality of life.\\n\\nTo achieve these goals, we've scheduled a biopsy procedure for you to take place soon. During this test, we'll remove a small sample of the tumor in your right armpit and examine it under a microscope to determine the best course of action moving forward. We'll also order additional tests, such as an MRI of your brain and bones, to gather more information about the extent of the disease.\\n\\nOur treatment philosophy is centered around making sure you receive the best possible care while maintaining your comfort and dignity throughout this process. We believe that even though the cancer has progressed, there are still ways to make your remaining days as comfortable and fulfilling as possible.\\n\\nIf you have any questions or concerns regarding your condition or upcoming procedures, please don't hesitate to reach out to us. We're committed to supporting you every step of the way.\\n\\nThank you for trusting us with your care.\\n\\nSincerely,\\n[Institution]\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "allres[0]['cleaned_output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5227c8a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12584"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allres[0]['prompt_length']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b73718",
   "metadata": {},
   "source": [
    "# gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d1e92aa-12c6-4c24-82c1-2398837a9848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['original_text', 'steps_run', 'available_steps', 'determined_audience',\n",
       "       'found_terms', 'terms_count', 'prompt', 'raw_output', 'prompt_length',\n",
       "       'cleaned_output', 'cleaned_length'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.DataFrame(allres)\n",
    "results_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d09b893-bbd1-4072-a282-06f7929ea49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved to 'output_12.csv'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "base_filename = 'output'\n",
    "extension = '.csv'\n",
    "output_filename = base_filename + extension\n",
    "counter = 1\n",
    "\n",
    "# Check if the file exists and find a new name if it does\n",
    "while os.path.exists(output_filename):\n",
    "    output_filename = f\"{base_filename}_{counter}{extension}\"\n",
    "    counter += 1\n",
    "\n",
    "# Save the DataFrame to the new, unique filename\n",
    "results_df.to_csv(output_filename, index=False)\n",
    "\n",
    "print(f\"DataFrame saved to '{output_filename}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
