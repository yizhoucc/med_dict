{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4dd4555-473d-48cd-bc86-60a4c41da0a8",
   "metadata": {},
   "source": [
    "## load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "611f29b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b2c708e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yc/miniconda3/envs/medllm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "with open(\"hf.token\", \"r\") as f:\n",
    "    hftoken = f.read().strip()  \n",
    "\n",
    "import os\n",
    "cache_dir = \"/mnt/c/Users/yc/.cache/huggingface\"\n",
    "os.environ['HF_HOME'] = cache_dir\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(token=hftoken)  # Move token to environment variable\n",
    "\n",
    "from ult import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e1c388-57f9-41b0-87d1-25a9ad884430",
   "metadata": {},
   "source": [
    "## data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf6281f4-7ebb-48fd-a08b-4271349ededf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coral_idx</th>\n",
       "      <th>Sex</th>\n",
       "      <th>UCSFDerivedRaceEthnicity_X</th>\n",
       "      <th>BirthDate</th>\n",
       "      <th>note_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>140</td>\n",
       "      <td>Female</td>\n",
       "      <td>Native Hawaiian or Other Pacific Islander</td>\n",
       "      <td>1964-03-25</td>\n",
       "      <td>Medical Oncology Consult Note    Patient Name:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>141</td>\n",
       "      <td>Female</td>\n",
       "      <td>Native Hawaiian or Other Pacific Islander</td>\n",
       "      <td>1975-03-29</td>\n",
       "      <td>This is a shared visit for services provided b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   coral_idx     Sex                 UCSFDerivedRaceEthnicity_X   BirthDate  \\\n",
       "0        140  Female  Native Hawaiian or Other Pacific Islander  1964-03-25   \n",
       "1        141  Female  Native Hawaiian or Other Pacific Islander  1975-03-29   \n",
       "\n",
       "                                           note_text  \n",
       "0  Medical Oncology Consult Note    Patient Name:...  \n",
       "1  This is a shared visit for services provided b...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_file_path = 'data/formaldef.txt'\n",
    "formaldic = txt_to_dict(txt_file_path)\n",
    "len(formaldic)\n",
    "\n",
    "meddict={}\n",
    "for k,v in formaldic.items():\n",
    "    meddict[k.split('Listen to pronunciation')[0].split('(')[0]]=v\n",
    "filename= 'data/filtered_medical_dictionary.csv'\n",
    "eighth_grade_words=set()\n",
    "with open(filename, 'r', encoding='utf-8') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    next(reader)  # Skip the header row\n",
    "    for row in reader:\n",
    "        if row:  # Make sure row is not empty\n",
    "            eighth_grade_words.add(row[0])  # Add the word (first column)\n",
    "filtered_meddict = {word: explanation for word, explanation in meddict.items() \n",
    "                   if word in eighth_grade_words}\n",
    "meddict=filtered_meddict\n",
    "# load data\n",
    "df = pd.read_csv('data/CORAL/coral-expert-curated-medical-oncology-reports-to-advance-language-model-inference-1.0/coral/unannotated/data/breastca_unannotated.csv')\n",
    "df=df.iloc[list(range(0,2))] \n",
    "# df = df.sample(1, random_state=42)\n",
    "# df=df.iloc[[44,45,53,70,83]+list(range(0,10))] \n",
    "# df=df.iloc[[44,45,53,70,83]] \n",
    "# df=df.iloc[[70,]] \n",
    "# test_note=df.iloc[2]['note_text']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d74a59c-b625-4944-8a79-aea69bc6cd8a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading weights: 100%|██████████| 291/291 [01:29<00:00,  3.23it/s, Materializing param=model.norm.weight]                              \n"
     ]
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "device_map = {\"\": 0}\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=device_map,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfa3a9d",
   "metadata": {},
   "source": [
    "# keypoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03c3bce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "extraction_prompts = {\n",
    "\n",
    "\"Reason_for_Visit\": \"\"\"\n",
    "TASK: Extract 'Reason for Visit'.\n",
    "Respond *only* with a JSON object using this exact schema:\n",
    "{\n",
    "\"Patient type\": \"either New patient or follow up\",\n",
    "\"second opinion\": \"whether the visit is consultation/second opinion or not\",\n",
    "\"in-person\": \"either Televisit or in-person. (note, video consult, televisit, telehealth are the same thing)\",\n",
    "\"summary\": \"A brief summary of the reason for visit.\"\n",
    "}\n",
    "\"\"\",\n",
    "\n",
    "\n",
    "\"What_We_Found\": \"\"\"\n",
    "TASK: Extract 'What We Found'. \n",
    "Respond *only* with a JSON object using this exact schema:\n",
    "{\n",
    "\"Type_of_Cancer\": \"list the type of cancer\",\n",
    "\"Stage_of_Cancer\": \"list the stage if it is written in the note\",\n",
    "\"Distant Metastasis\": \"if there is distant metastasis (met). Yes, to where; No, local, not sure, need more evidence such as imaging.\",\n",
    "\"Metastasis\": \"if there is met, Yes (to where), No, or Not sure\",\n",
    "\"lab_summary\": \"Summary of key lab results.\",\n",
    "\"findings\": \"Summary of new findings or disease status.\"\n",
    "}\n",
    "\"\"\",\n",
    "\n",
    "\n",
    "\"Treatment_Summary\": \"\"\"\n",
    "TASK: Extract 'Treatment Summary'. \n",
    "Respond *only* with a JSON object using this exact schema:\n",
    "{\n",
    "\"current_meds\": \"List of current oncologic medications or regimens.\",\n",
    "\"recent_changes\": \"Any holds, dose reductions, or switches.\",\n",
    "\"supportive_meds\": \"List of supportive medications.\"\n",
    "}\n",
    "\"\"\",\n",
    "\n",
    "\n",
    "\"Goals of care\": \"\"\"\n",
    "TASK: Extract 'What We Discussed / Decided'.\n",
    "Respond *only* with a JSON object using this exact schema:\n",
    "{\n",
    "\"goals_of_treatment\": \"eg, cancer is not curable, but it's treatable and the goal is to extend the duration and maintain the quality of life\",\n",
    "\"response_assessment\": \"How the cancer is responding to the treatment, not responding or responding.\"\n",
    "}\n",
    "\"\"\",\n",
    "}\n",
    "\n",
    "plan_extraction_prompts = {\n",
    "\n",
    "\"Medication_Plan_chatgpt\": \"\"\"\n",
    "TASK: Extract the 'Medication Plan' from the given 'Assessment/Plan' section.\n",
    "Include all current and future medication plans for both cancer therapy and supportive treatment. Cancer treatment could be one or many in chemotherapy, hormonal therapy, bone therapy, radiotherapy (eg. rad onc, xrt). Supportive treatment could be one or many in bowel regimen, pain medication, psychiatry medication, neuropathy or any blood transfusion plan. \n",
    "\n",
    "Include whether a medication is being started now (e.g,“will start”, “Rx sent”, “starting today” ), plan or discuss in the future after certain condition (e.g., “plan to start after radiation”, “discussed addition of…”), continue or maintained(“continue”), stop or change.\n",
    "\n",
    "Include an alterative, second-line or clinical trials options if dicussed. \n",
    "Include an 'other treatment' section for any other medications that only briefly mentioned in the plan section.\n",
    "Respond *only* with a JSON object using this exact schema:\n",
    "{\n",
    "    \"The medication/treatment, one of chemotherapy, hormonal therapy, bone therapy, radiotherapy\": \n",
    "{\"summary\":\"the summary of this type of medication, including start/stop/cotinue if applicable\",\n",
    "\"Short term side_effects_discussed\": \"short term Side effects of this particular medications.\",\n",
    "\"Long term side_effects_discussed\": \"long term Side effects of this particular medications..\"}\n",
    "}\n",
    "\"\"\",\n",
    "\n",
    "\n",
    "\"Medication_Plan\": \"\"\"\n",
    "TASK: Extract the 'Medication Plan' from the given 'Assessment/Plan' section.\n",
    "Include future medication plans, changes to current meds (start, stop, continue), supportive meds, bowel regimen, and any blood transfusion plan. Do not include past medications.\n",
    "Respond *only* with a JSON object using this exact schema:\n",
    "{\n",
    "    \"medication_plan\": \"A summary of the complete medication plan.\"\n",
    "}\n",
    "\"\"\",\n",
    "\n",
    "\n",
    "\"Therapy plan\": \"\"\"\n",
    "TASK: Extract the 'therapy Plan' from the given 'Assessment/Plan' section.\n",
    "Include chemotherapy, radiotherapy, hormonal therapy, and bone-therapy plans. Include future plans, changes to current therapy (start, stop, continue). Do not include past therapies.\n",
    "Respond *only* with a JSON object using this exact schema:\n",
    "{\n",
    "    \"therapy_plan\": \"A summary of the complete therapy plan.\"\n",
    "}\n",
    "\"\"\",\n",
    "\n",
    "\"radiotherapy plan\": \"\"\"\n",
    "TASK: Extract the 'radiotherapy Plan' from the given 'Assessment/Plan' section.\n",
    "For radiotherapy, include ANY statement that indicates \n",
    "it is being considered, recommended, or may be used, even if no explicit\n",
    "start/continue/plan keywords are present.\n",
    "\"\"\",\n",
    "\n",
    "\n",
    "\"Procedure_Plan\": \"\"\"\n",
    "TASK: Extract the 'Procedure Plan' from the given 'Assessment/Plan' section.\n",
    "Include future procedures including surgery, radiation therapy, or interventional procedures such as biopsy, lumbar puncture, or Chemo Port Insertion. Do not include past procedures.\n",
    "Do not include imaging plan, lab plan, or medication plan.\n",
    "Respond *only* with a JSON object using this exact schema:\n",
    "{\n",
    "    \"procedure_plan\": \"A summary of any planned procedures.\"\n",
    "}\n",
    "\"\"\",\n",
    "\n",
    "\n",
    "\"Imaging Plan\": \"\"\"\n",
    "TASK: Extract the 'Imaging Plan' from the given 'Assessment/Plan' section.\n",
    "Include all future imaging like CT, MRI, PET/CT, ultrasound,  DEXA scans, including timing and rationale if mentioned.\n",
    "Do not include any procedure plan, lab plan, or medication plan. Do not include past Imaging.\n",
    "Respond *only* with a JSON object using this exact schema:\n",
    "{\n",
    "    \"imaging_plan\": \"A summary of any planned procedures.\"\n",
    "}\n",
    "\"\"\",\n",
    "\n",
    "\n",
    "\"Lab Plan\": \"\"\"\n",
    "TASK: Extract the 'lab Plan' from the given 'Assessment/Plan' section.\n",
    "Include future labs like CBC, CMP, tumor markers, coagulation profile. Specify frequency and rationale if mentioned.\n",
    "Do not include any procedure plan, medication plan, or imagining plan. Do not include past labs.\n",
    "Respond *only* with a JSON object using this exact schema:\n",
    "{\n",
    "    \"lab_plan\": \"A summary of the future lab plan.\"\n",
    "}\n",
    "\"\"\",\n",
    "\n",
    "\n",
    "\"Genetic_Testing_Plan\": \"\"\"\n",
    "TASK: Extract the 'Genetic Testing Plan' from the given 'Assessment/Plan' section.\n",
    "\n",
    "1. Scope of section:\n",
    "- First, find the 'Assessment/Plan' section of the note (usually the final section).\n",
    "- Only look for plans that are clearly about FUTURE genetic or molecular TESTING.\n",
    "\n",
    "2. What COUNTS as 'genetic or molecular testing':\n",
    "- Diagnostic, prognostic, or predictive laboratory assays, such as:\n",
    "  - Tumor genomic sequencing / NGS / panel testing\n",
    "  - Germline genetic panels (e.g., hereditary cancer panel, BRCA testing)\n",
    "  - Liquid biopsy / circulating tumor DNA (ctDNA) tests\n",
    "  - Specific biomarker tests (e.g., PD-L1 testing, MSI testing, EGFR mutation testing)\n",
    "  - Any plan to \"send\", \"order\", \"check\", or \"obtain\" a genetic, genomic, or molecular TEST\n",
    "\n",
    "3. What MUST be EXCLUDED:\n",
    "- DO NOT include any medications, systemic therapies, or treatment plans:\n",
    "  - Chemotherapy, immunotherapy, targeted therapies (e.g., FGFR inhibitor, PARP inhibitor, CDK4/6 inhibitor, TKIs)\n",
    "  - Hormonal therapy, radiation therapy, surgery\n",
    "  - Clinical trial options, even if the trial involves targeted drugs or inhibitors\n",
    "- DO NOT include tests that are already completed, historic, or only mentioned in past oncology history.\n",
    "- DO NOT include imaging (CT, PET, MRI, X-ray, ultrasound) or routine labs (CBC, CMP).\n",
    "\n",
    "4. If there is NO new genetic or molecular test planned in the Assessment/Plan:\n",
    "- Set the value to a clear negative statement, for example:\n",
    "  \"No new genetic or molecular tests were planned during this visit.\"\n",
    "\n",
    "5. Output format:\n",
    "Respond *only* with a JSON object using this exact schema:\n",
    "{\n",
    "    \"genetic_testing_plan\": \"A summary of any future planned genetic or molecular tests, or a clear statement that none are planned.\"\n",
    "}\n",
    "\"\"\",\n",
    "\n",
    "\n",
    "\"Referral\": \"\"\"\n",
    "TASK: Extract 'Referral' from the given 'Assessment/Plan' section.\n",
    "do not mistake the doctor's signature at the end of the note as a referral.\n",
    "Respond *only* with a JSON object using this exact schema:\n",
    "{\n",
    "\"Nutrition\": \"any nutrition referrals such as diet optimization, appetite, weight maintenance\",\n",
    "\"Genetics\": \"eg, germline testing, family counseling\",\n",
    "\"Specialty\": \"eg, Palliative care (symptom or pain management, goals of care), Radiation oncology, surgical oncology, Psychology, psychiatry for coping and mood support\",\n",
    "\"Others\": \"Physical or occupational therapy, Social work, financial counseling\",\n",
    "\"follow up\": \"follow up mentioned in the provided text. f/up[, f/p and fup all mean follow up.\"\n",
    "}\n",
    "\"\"\",\n",
    "\n",
    "\n",
    "\"follow up/next visit\": \"\"\"\n",
    "TASK: Extract the 'follow up/next visit' from the given 'Assessment/Plan' section.\n",
    "\n",
    "Respond *only* with a JSON object using this exact schema:\n",
    "{\n",
    "    \"Next clinic visit\": \"(in-person or telehealth): timing and purpose\"\n",
    "}\n",
    "\"\"\",\n",
    "\n",
    "\n",
    "\"Advance care planning\": \"\"\"\n",
    "TASK: Extract the 'Advance care planning' from the given 'Assessment/Plan' section.\n",
    "Include Advance directives, health-care proxy, code status (if appropriate). \n",
    "Respond *only* with a JSON object using this exact schema:\n",
    "{\n",
    "    \"Advance care\": \"A summary of any planned Advance care.\"\n",
    "}\n",
    "\"\"\",\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2442743",
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_prompts = {\n",
    "\n",
    "\"Why you came today\": \"\"\"\n",
    "TASK: Briefly state the purpose of today's visit using only information from the key points. \n",
    "\"\"\",\n",
    "\n",
    "\n",
    "\"What you told us\": \"\"\"\n",
    "TASK: List the patient concerns, symptoms, or questions mentioned in the NOTE section.\n",
    "\"\"\",\n",
    "\n",
    "\n",
    "\"What did we find\": \"\"\"\n",
    "TASK: Explain the result of today's visit (exam, blood work, scans) in plain language. You need to include all 'key points' from KEYPOINTS section.\n",
    "\n",
    "\"\"\",\n",
    "\n",
    "\n",
    "\"What is the plan\": \"\"\"\n",
    "TASK: List the next steps (treatment, monitoring, follow-up) in plain language. Include all related information from KEYPOINTS section if the following are mentioned, including:\n",
    "    1, Medication plan\n",
    "    2, Procedure plan\n",
    "    3, Imaging plan\n",
    "    4, Lab plan\n",
    "    5, Genetic Testing Plan\n",
    "    6, Referral\n",
    "\"\"\",\n",
    "\n",
    "\n",
    "\"Closing with Support\": \"\"\"\n",
    "TASK: write an ending sentence to show your support. Limit to one sentence maximum.\n",
    "\"\"\",\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63edc7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create save file and write prompts\n",
    "from datetime import datetime\n",
    "import os\n",
    "_notebook_name = 'unknown'\n",
    "if '__vsc_ipynb_file__' in globals():\n",
    "    _notebook_name = os.path.basename(globals()['__vsc_ipynb_file__'])\n",
    "else:\n",
    "    try:\n",
    "        import ipynbname\n",
    "        _notebook_name = ipynbname.name() + '.ipynb'\n",
    "    except Exception:\n",
    "        _notebook_name = 'work flow keypoint v1 check v3.ipynb'\n",
    "_run_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "with open('results.txt', 'w') as f:\n",
    "    f.write(f\"Source: {_notebook_name} | Run started: {_run_time}\\n\")\n",
    "    f.write(\"=\" * 60 + \"\\n\")\n",
    "    f.write(\"extraction_prompts\\n\")\n",
    "    original_text = (str(extraction_prompts)\n",
    "                     .replace('\\\\n', '\\n')\n",
    "                     .replace(\"\\\\'\", \"'\")\n",
    "                     .replace('\\\\\"', '\"'))\n",
    "    f.write(original_text + \"\\n\")\n",
    "    f.write(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "    f.write(\"plan_extraction_prompts\\n\")\n",
    "    original_text = (str(plan_extraction_prompts)\n",
    "                     .replace('\\\\n', '\\n')\n",
    "                     .replace(\"\\\\'\", \"'\")\n",
    "                     .replace('\\\\\"', '\"'))\n",
    "    f.write(original_text + \"\\n\")\n",
    "    f.write(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "    f.write(\"explain_prompts\\n\")\n",
    "    original_text = (str(explain_prompts)\n",
    "                     .replace('\\\\n', '\\n')\n",
    "                     .replace(\"\\\\'\", \"'\")\n",
    "                     .replace('\\\\\"', '\"'))\n",
    "    f.write(original_text + \"\\n\")\n",
    "    f.write(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "with open('results.txt', 'a') as f:\n",
    "    f.write(\"\\n\"*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96ba026",
   "metadata": {},
   "outputs": [],
   "source": [
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "EXPLANATION_GENERATION_CONFIG = {\n",
    "    \"max_new_tokens\": 666,\n",
    "    \"temperature\": 0.3,\n",
    "    \"top_p\": 0.85,\n",
    "    \"repetition_penalty\": 1.2,     \n",
    "    \"no_repeat_ngram_size\": 3,     \n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "    \"eos_token_id\": terminators,\n",
    "    \"early_stopping\": True,\n",
    "}\n",
    "\n",
    "CLEANING_CONFIG = {\n",
    "    \"max_new_tokens\": 666,\n",
    "    \"temperature\": 0.1,\n",
    "    \"top_p\": 0.9,\n",
    "    \"repetition_penalty\": 1.2,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "    \"eos_token_id\": terminators, \n",
    "    \"early_stopping\": True,\n",
    "}\n",
    "\n",
    "KEYPOINT_CONFIG = {\n",
    "    \"max_new_tokens\": 512, \n",
    "    \"eos_token_id\": tokenizer.eos_token_id,\n",
    "    \"do_sample\": False\n",
    "    }\n",
    "\n",
    "assessment_and_plan_CONFIG= {\n",
    "    \"max_new_tokens\": 2048, \n",
    "    \"eos_token_id\": tokenizer.eos_token_id,\n",
    "    \"do_sample\": False\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01a7421",
   "metadata": {},
   "source": [
    "# run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096fe7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = {}\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    print(f\"\\nProcessing row {index}/{len(df)}...\")\n",
    "\n",
    "    # note_text = row['note_text']\n",
    "    # assessment_and_plan,_=run_model(\n",
    "    # 'here is a medical note \\n\\n'+note_text+'\\n\\n now, return me all the ORIGINAL TEXT (do not do any modifications, do no rephrase and summarize) after the words like \"Assessment and Plan\" or \"Assessment/Plan\". ignore anything before that. ignore the line breaking characters.',\n",
    "    # model,\n",
    "    # tokenizer,\n",
    "    # assessment_and_plan_CONFIG)\n",
    "\n",
    "\n",
    "    # sanity_prompt = (\n",
    "    #     \"You are a text validator. Compare the following two texts.\\n\\n\"\n",
    "    #     \"--- ORIGINAL SOURCE START ---\\n\" + note_text + \"\\n--- ORIGINAL SOURCE END ---\\n\\n\"\n",
    "    #     \"--- EXTRACTED SEGMENT START ---\\n\" + assessment_and_plan + \"\\n--- EXTRACTED SEGMENT END ---\\n\\n\"\n",
    "    #     \"Task: Verify if the 'EXTRACTED SEGMENT' appears word-for-word inside the 'ORIGINAL SOURCE'.\\n\"\n",
    "    #     \"If it is a summary, rephrased, or contains text not in the original, return FAIL.\\n\"\n",
    "    #     \"If it is an exact substring match, return PASS.\\n\"\n",
    "    #     \"Answer with one word: PASS or FAIL.\"\n",
    "    # )\n",
    "\n",
    "    # sanity_result, _ = run_model(\n",
    "    #     sanity_prompt,\n",
    "    #     model,\n",
    "    #     tokenizer,\n",
    "    #     assessment_and_plan_CONFIG\n",
    "    # )\n",
    "\n",
    "    # if \"FAIL\" in sanity_result.upper():\n",
    "    #     print(f\"Sanity Check Failed for row. Model output was: {sanity_result}\")\n",
    "    #     # Optional: Retry logic, or flag for manual review\n",
    "    #     # assessment_and_plan = None \n",
    "    # else:\n",
    "    #     print(\"got assessment_and_plan (verified)\")\n",
    "\n",
    "\n",
    "\n",
    "    # print('got assessment_and_plan')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    MAX_RETRIES = 3\n",
    "    retry_count = 0\n",
    "    extraction_success = False\n",
    "    final_assessment_plan = \"\"  # Default to empty\n",
    "\n",
    "    note_text = row['note_text']\n",
    "\n",
    "    while retry_count < MAX_RETRIES and not extraction_success:\n",
    "        \n",
    "        # --- STEP 1: GENERATION ---\n",
    "        \n",
    "        # 1. Dynamic Prompting & Config\n",
    "        if retry_count == 0:\n",
    "            # First try: Standard Prompt + Greedy Decoding\n",
    "            current_prompt = (\n",
    "                'here is a medical note \\n\\n' + note_text + '\\n\\n '\n",
    "                'now, return me all the ORIGINAL TEXT (do not do any modifications, do no rephrase and summarize) '\n",
    "                'after the words like \"Assessment and Plan\" or \"Assessment/Plan\". '\n",
    "                'ignore anything before that. ignore the line breaking characters.'\n",
    "            )\n",
    "            # Greedy decoding for precision\n",
    "            current_config = assessment_and_plan_CONFIG \n",
    "            \n",
    "        else:\n",
    "            # Retry: \"Angrier\" Prompt + Sampling (Temperature) to break loops\n",
    "            current_prompt = (\n",
    "                'The previous attempt failed because it was not an exact copy. '\n",
    "                'Your task is to COPY-PASTE the \"Assessment and Plan\" section.'\n",
    "                '\\n\\nSOURCE TEXT:\\n' + note_text + '\\n\\n'\n",
    "                'INSTRUCTION: Extract the Assessment and Plan section exactly as written. '\n",
    "                'Do not summarize. Do not fix grammar. Do not change punctuation.'\n",
    "            )\n",
    "            # Enable sampling to explore new paths\n",
    "            current_config = {\n",
    "                \"max_new_tokens\": 2048, \n",
    "                \"eos_token_id\": tokenizer.eos_token_id,\n",
    "                \"do_sample\": True, \n",
    "                \"temperature\": 0.6,\n",
    "                \"top_p\": 0.9\n",
    "            }\n",
    "\n",
    "        print(f\"Extraction Attempt {retry_count + 1}...\")\n",
    "        candidate_text, _ = run_model(\n",
    "            current_prompt,\n",
    "            model,\n",
    "            tokenizer,\n",
    "            current_config\n",
    "        )\n",
    "\n",
    "        # --- STEP 2: LLM SANITY CHECK ---\n",
    "        \n",
    "        # We ask the model to compare the Candidate against the Source.\n",
    "        # We explicitly ask it to allow whitespace differences but forbid rewrites.\n",
    "        \n",
    "        sanity_check_prompt = (\n",
    "            f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n\"\n",
    "            f\"You are a strict text auditor. Your job is to verify data fidelity.\"\n",
    "            f\"<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "            f\"I have a SOURCE text and an EXTRACTED snippet. \\n\"\n",
    "            f\"Task: Verify if the EXTRACTED snippet appears in the SOURCE text.\\n\"\n",
    "            f\"Rules:\\n\"\n",
    "            f\"1. Ignore differences in whitespace (newlines, tabs, spaces).\\n\"\n",
    "            f\"2. Ignore minor formatting (bullet points vs dashes).\\n\"\n",
    "            f\"3. STRICTLY FAIL if the text is summarized, reworded, or contains new words.\\n\\n\"\n",
    "            f\"--- SOURCE TEXT START ---\\n{note_text}\\n--- SOURCE TEXT END ---\\n\\n\"\n",
    "            f\"--- EXTRACTED SNIPPET START ---\\n{candidate_text}\\n--- EXTRACTED SNIPPET END ---\\n\\n\"\n",
    "            f\"Does the snippet match? Reply with exactly one JSON object: {{\\\"match\\\": true}} or {{\\\"match\\\": false}}.\"\n",
    "            f\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "        )\n",
    "\n",
    "        # Run the check (Use greedy decoding for the check itself to be deterministic)\n",
    "        print(\"Running LLM Sanity Check...\")\n",
    "        check_output, _ = run_model(\n",
    "            sanity_check_prompt,\n",
    "            model,\n",
    "            tokenizer,\n",
    "            {\"max_new_tokens\": 100, \"do_sample\": False} # Fast, greedy check\n",
    "        )\n",
    "\n",
    "        # --- STEP 3: EVALUATE RESULT ---\n",
    "        \n",
    "        is_pass = False\n",
    "        if \"true\" in check_output.lower():\n",
    "            is_pass = True\n",
    "        \n",
    "        # Optional: Keep your length check just to prevent empty string returns\n",
    "        if is_pass and len(candidate_text) > 20:\n",
    "            final_assessment_plan = candidate_text\n",
    "            extraction_success = True\n",
    "            print(f\"Success on attempt {retry_count + 1} (Verified by LLM)\")\n",
    "        else:\n",
    "            print(f\"Attempt {retry_count + 1} failed LLM sanity check. Retrying...\")\n",
    "            retry_count += 1\n",
    "\n",
    "    # 4. Final Fallback\n",
    "    if not extraction_success:\n",
    "        print(\"All attempts failed. Skipping plan-specific extraction.\")\n",
    "        final_assessment_plan = None\n",
    "\n",
    "        \n",
    "    assessment_and_plan = final_assessment_plan\n",
    "\n",
    "    # extract keypoints from extraction_prompts\n",
    "    base_cache = build_base_cache(note_text, model, tokenizer)\n",
    "    keypoints = extract_and_verify(extraction_prompts, model, tokenizer, KEYPOINT_CONFIG, base_cache)\n",
    "    print('keypoints from extraction_prompts')\n",
    "\n",
    "    # extract keypoints from plan_extraction_prompts\n",
    "    if assessment_and_plan is not None:\n",
    "        base_cache = build_base_cache(assessment_and_plan, model, tokenizer)\n",
    "        plan_keypoints = extract_and_verify(plan_extraction_prompts, model, tokenizer, KEYPOINT_CONFIG, base_cache)\n",
    "        keypoints.update(plan_keypoints)\n",
    "    print('keypoints from plan_extraction_prompts')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # # explain\n",
    "    # explain_base_prompt = (\n",
    "    #     f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n\"\n",
    "    #     f\"You are an experienced and compassionate Oncologist and medical educator. Your primary role is to translate complex medical information into clear, 8th-grade level English. \"\n",
    "    #     f\"Your task is to answer a series of questions about it, one by one. \"\n",
    "    #     f'''### STRICT NEGATIVE CONSTRAINTS:\n",
    "    #     * Do not say anything not present in the medical note.\n",
    "    #     * If cancer has spread (metastasis), DO NOT list the specific organs affected. Say \"the cancer has spread to other parts of the body.\"\n",
    "    #     * Do not use fatalistic language. The focus MUST be on quality of life.\n",
    "    #     *NO ADDING SIDE EFFECTS, Unless they appear in the keypoints exactly.\n",
    "    #     '''\n",
    "    #     f\"<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "    #     f\"Here is the medical note:\\n\\n\"\n",
    "    #     f\"--- BEGIN NOTE ---\\n{note_text}\\n--- END NOTE ---\"\n",
    "    #     f\"Here is the keypoints extracted from the medical note:\\n\\n\"\n",
    "    #     f\"--- BEGIN KEYPOINTS ---\\n{keypoints}\\n--- END KEYPOINTS ---\"\n",
    "    #     f\"\\n\\nI will now ask you to extract specific sections. \"\n",
    "    #     f\"Please wait for my first extraction task.\"\n",
    "    #     f\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "    #     f\"{{\\\"status\\\": \\\"Understood. I have read the note and am ready.\\\"}}\"\n",
    "    # )\n",
    "\n",
    "    # with torch.no_grad():\n",
    "    #     inputs = tokenizer(explain_base_prompt, return_tensors=\"pt\").to(model.device)\n",
    "    #     outputs = model(\n",
    "    #         input_ids=inputs[\"input_ids\"], \n",
    "    #         attention_mask=inputs[\"attention_mask\"],\n",
    "    #         use_cache=True\n",
    "    #     )\n",
    "    #     base_cache = outputs.past_key_values\n",
    "    #     del inputs, outputs\n",
    "    #     torch.cuda.empty_cache()\n",
    "    #     gc.collect()\n",
    "\n",
    "    # explain = {}\n",
    "    # run_model_function = run_model_with_cache_manual \n",
    "    # for key, task in explain_prompts.items():\n",
    "    #     task_prompt = (\n",
    "    #         f\"<|start_header_id|>user<|end_header_id|>\\n\\n\" \n",
    "    #         f\"{task}\"\n",
    "    #         f\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "    #     )\n",
    "    #     answer, returned_cache = run_model_function(\n",
    "    #         task_prompt, \n",
    "    #         model, \n",
    "    #         tokenizer, \n",
    "    #         KEYPOINT_CONFIG, \n",
    "    #         kv_cache=base_cache\n",
    "    #     )\n",
    "    #     del returned_cache\n",
    "    #     torch.cuda.empty_cache()\n",
    "    #     gc.collect()\n",
    "        \n",
    "    #     try:\n",
    "    #         clean_answer = answer.strip().strip(\"```json\").strip(\"```\").strip()\n",
    "    #         explain[key] = json.loads(clean_answer)\n",
    "    #     except json.JSONDecodeError:\n",
    "    #         explain[key] = answer\n",
    "\n",
    "    # print('explain')\n",
    "\n",
    "    \n",
    "\n",
    "    # print('explain')\n",
    "    # explanation_prompt= create_explanation_prompt(note_text, keypoints)\n",
    "    # explanation,_  = run_model_function(explanation_prompt, model, tokenizer, EXPLANATION_GENERATION_CONFIG)\n",
    "\n",
    "    # # clean up\n",
    "    # print('clean')\n",
    "    # cleaning_prompt = create_cleaning_prompt(explanation)\n",
    "    # final_result,_  = run_model_function(cleaning_prompt, model, tokenizer, CLEANING_CONFIG)\n",
    "    \n",
    "    row_result = {\n",
    "        'coral_idx': row['coral_idx'],\n",
    "        'note_text': note_text,\n",
    "        'assessment_and_plan': assessment_and_plan,\n",
    "        'keypoints': keypoints,\n",
    "        # 'explain': explain,\n",
    "        # 'raw_result': explanation,\n",
    "        # 'clean_result': final_result\n",
    "    }\n",
    "    \n",
    "    all_results[index]=(row_result)\n",
    "\n",
    "    # # print here\n",
    "    # print('\\n'*5)\n",
    "    # print(f\"\\n{'='*60}\")\n",
    "    # print(f\"RESULTS FOR ROW {index + 1}\")\n",
    "    # print(f\"{'='*60}\")\n",
    "    \n",
    "    # for col in ['assessment_and_plan','keypoints', ]:\n",
    "    #     print(f\"\\n--- Column: {col} ---\")\n",
    "    #     original_text = row_result[col]\n",
    "    #     try:\n",
    "    #         (print_json((original_text)))\n",
    "    #     except:\n",
    "    #         print(original_text)\n",
    "\n",
    "\n",
    "    # write to txt file\n",
    "    with open('results.txt', 'a') as f:  # 'a' to append, 'w' to overwrite\n",
    "        f.write('\\n' * 5)\n",
    "        f.write(f\"\\n{'='*60}\\n\")\n",
    "        f.write(f\"RESULTS FOR ROW {index + 1}\\n\")\n",
    "        f.write(f\"{'='*60}\\n\")\n",
    "        \n",
    "        for col in row_result.keys():\n",
    "            f.write(f\"\\n--- Column: {col} ---\\n\")\n",
    "            original_text = row_result[col]\n",
    "            try:\n",
    "                import json\n",
    "                f.write(json.dumps(original_text, indent=2) + '\\n')\n",
    "            except:\n",
    "                f.write(str(original_text) + '\\n')\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}